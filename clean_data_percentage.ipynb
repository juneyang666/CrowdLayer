{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yajingyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, merge, Reshape, Permute, Multiply, Dot,dot, Concatenate, Add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy, CrowdsClassificationSModel, \\\n",
    "    CrowdsClassificationCModelSingleWeight, CrowdsClassificationCModel, MaskedMultiCrossEntropyCosSim, \\\n",
    "    MaskedMultiCrossEntropyBaseChannel, MaskedMultiCrossEntropyBaseChannelConst, CrowdsClassificationSModelChannelMatrix, \\\n",
    "    MaskedMultiCrossEntropyCurriculumChannelMatrix\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = np.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets\n",
    "\n",
    "def get_data(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train_vgg16.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test_vgg16.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid_vgg16.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "    return x, y_gt, y_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def eval(model,x_test, y_test):\n",
    "    print('Test dataset results: ')\n",
    "    print(dict(zip(model.metrics_names,model.evaluate(x_test,y_test, verbose=False))))\n",
    "\n",
    "\n",
    "def get_trace(model):\n",
    "\n",
    "    channel_matrix = model.get_weights()[-1]\n",
    "    channel_matrix_trace = tf.trace(K.permute_dimensions(channel_matrix, [2, 0, 1]))\n",
    "    channel_matrix_trace_arr = K.eval(channel_matrix_trace)\n",
    "    return channel_matrix_trace_arr\n",
    "\n",
    "\n",
    "def print_single_loss(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['baseline_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['baseline_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_history(df, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Make a data frame\n",
    "    df['x'] = range(df.shape[0])\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    # multiple line plot\n",
    "    num = 0\n",
    "    for column in df.drop('x', axis=1):\n",
    "        num += 1\n",
    "        plt.plot(df['x'], df[column], marker='', color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=2)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(train_data_shape, N_CLASSES):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.4))\n",
    "    base_model.add(Dense(64, activation='relu'))\n",
    "    base_model.add(Dropout(0.4))\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation(\"softmax\"))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def majority_vote(x, y_gt, y_annot, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_annot['mv'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_gt(x, y_gt, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_gt['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    hidden_layers = Sequential()\n",
    "    hidden_layers.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    hidden_layers.add(Dense(128, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "    hidden_layers.add(Dense(64, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = hidden_layers(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "    \n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    hidden_layers = Sequential()\n",
    "    hidden_layers.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    hidden_layers.add(Dense(128, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "    hidden_layers.add(Dense(64, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = hidden_layers(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    \n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LabelMe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(10000, 4, 4, 512)\n",
      "(10000,)\n",
      "(10000,)\n",
      "\n",
      "Loading AMT data...\n",
      "(10000, 59)\n",
      "N_CLASSES: 8\n",
      "N_ANNOT: 59\n",
      "\n",
      "Loading test data...\n",
      "(1188, 4, 4, 512)\n",
      "(1188,)\n",
      "\n",
      "Loading validation data...\n",
      "(500, 4, 4, 512)\n",
      "(500,)\n",
      "(100, 4, 4, 512) (100, 8) (10000, 8, 59)\n",
      "\n",
      "Baseline model with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.710868588200322, 'acc': 0.7794612794612794}\n",
      "Test dataset results: \n",
      "{'loss': 0.8067635827594333, 'acc': 0.734006734006734}\n",
      "Test dataset results: \n",
      "{'loss': 0.7348099656478323, 'acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': 0.7108175661226716, 'acc': 0.7634680134680135}\n",
      "Test dataset results: \n",
      "{'loss': 0.6636118900896323, 'acc': 0.7718855218855218}\n",
      "Test dataset results: \n",
      "{'loss': 0.6636052964512347, 'acc': 0.7777777777777778}\n",
      "Test dataset results: \n",
      "{'loss': 0.7271905772212378, 'acc': 0.7643097643097643}\n",
      "Test dataset results: \n",
      "{'loss': 0.7929752127891437, 'acc': 0.7483164983164983}\n",
      "Test dataset results: \n",
      "{'loss': 0.6596275058258262, 'acc': 0.7954545454545454}\n",
      "Test dataset results: \n",
      "{'loss': 0.7363768972531713, 'acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': 0.7503938436106801, 'acc': 0.7483164983164983}\n",
      "Test dataset results: \n",
      "{'loss': 0.7238400978873475, 'acc': 0.7575757575757576}\n",
      "Test dataset results: \n",
      "{'loss': 0.6804185519214431, 'acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': 0.650971573184837, 'acc': 0.7794612794612794}\n",
      "Test dataset results: \n",
      "{'loss': 0.6136230202234956, 'acc': 0.7962962962962963}\n",
      "Test dataset results: \n",
      "{'loss': 0.6427505735797111, 'acc': 0.7996632996632996}\n",
      "Test dataset results: \n",
      "{'loss': 0.7167445136240436, 'acc': 0.7626262626262627}\n",
      "Test dataset results: \n",
      "{'loss': 0.7280061707552836, 'acc': 0.7617845117845118}\n",
      "Test dataset results: \n",
      "{'loss': 0.7234590073627254, 'acc': 0.7491582491582491}\n",
      "Test dataset results: \n",
      "{'loss': 0.6784338737377013, 'acc': 0.7937710437710438}\n",
      "\n",
      "Crowd noise adaptation model with 0.01 clean data\n",
      "Tensor(\"baseline_22/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_19/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_19/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2822932233714095, 'crowds_classification_19_loss': -7.2822932233714095, 'baseline_loss': 0.9853228525405981, 'crowds_classification_19_acc': 0.021464646464646464, 'baseline_acc': 0.8434343434343434}\n",
      "Tensor(\"baseline_23/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_20/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_20/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.278884773703938, 'crowds_classification_20_loss': -7.278884773703938, 'baseline_loss': 1.0265825732990548, 'crowds_classification_20_acc': 0.034301346801346805, 'baseline_acc': 0.8409090909090909}\n",
      "Tensor(\"baseline_24/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_21/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_21/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.275244642187048, 'crowds_classification_21_loss': -7.275244642187048, 'baseline_loss': 1.0740361584958806, 'crowds_classification_21_acc': 0.03282828282828283, 'baseline_acc': 0.8257575757575758}\n",
      "Tensor(\"baseline_25/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_22/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_22/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.308316720455182, 'crowds_classification_22_loss': -7.308316720455182, 'baseline_loss': 0.9468901298049625, 'crowds_classification_22_acc': 0.021780303030303032, 'baseline_acc': 0.8543771043771043}\n",
      "Tensor(\"baseline_26/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_23/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_23/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.22019266520285, 'crowds_classification_23_loss': -7.22019266520285, 'baseline_loss': 2.482883437765568, 'crowds_classification_23_acc': 0.0211489898989899, 'baseline_acc': 0.7668350168350169}\n",
      "Tensor(\"baseline_27/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_24/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_24/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.288526515768032, 'crowds_classification_24_loss': -7.288526515768032, 'baseline_loss': 1.038060392224095, 'crowds_classification_24_acc': 0.030513468013468013, 'baseline_acc': 0.8451178451178452}\n",
      "Tensor(\"baseline_28/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_25/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_25/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.265398314504912, 'crowds_classification_25_loss': -7.265398314504912, 'baseline_loss': 1.068150702550355, 'crowds_classification_25_acc': 0.02462121212121212, 'baseline_acc': 0.8434343434343434}\n",
      "Tensor(\"baseline_29/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_26/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_26/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.306846857873679, 'crowds_classification_26_loss': -7.306846857873679, 'baseline_loss': 0.9123143092149034, 'crowds_classification_26_acc': 0.02220117845117845, 'baseline_acc': 0.8417508417508418}\n",
      "Tensor(\"baseline_30/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_27/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_27/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.285478238706236, 'crowds_classification_27_loss': -7.285478238706236, 'baseline_loss': 0.9740293938603867, 'crowds_classification_27_acc': 0.021675084175084174, 'baseline_acc': 0.8249158249158249}\n",
      "Tensor(\"baseline_31/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_28/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_28/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.29288286151308, 'crowds_classification_28_loss': -7.29288286151308, 'baseline_loss': 0.9115946503161682, 'crowds_classification_28_acc': 0.021675084175084174, 'baseline_acc': 0.8417508417508418}\n",
      "Tensor(\"baseline_32/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_29/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_29/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.170993475801615, 'crowds_classification_29_loss': -7.170993475801615, 'baseline_loss': 2.8356005916190066, 'crowds_classification_29_acc': 0.02251683501683502, 'baseline_acc': 0.7407407407407407}\n",
      "Tensor(\"baseline_33/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_30/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_30/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.297841651672466, 'crowds_classification_30_loss': -7.297841651672466, 'baseline_loss': 1.1693251746854045, 'crowds_classification_30_acc': 0.02030723905723906, 'baseline_acc': 0.8493265993265994}\n",
      "Tensor(\"baseline_34/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_31/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_31/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.278727916756061, 'crowds_classification_31_loss': -7.278727916756061, 'baseline_loss': 0.9006452485166415, 'crowds_classification_31_acc': 0.02251683501683502, 'baseline_acc': 0.8383838383838383}\n",
      "Tensor(\"baseline_35/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_32/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_32/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.278589845907809, 'crowds_classification_32_loss': -7.278589845907809, 'baseline_loss': 0.9449352099613509, 'crowds_classification_32_acc': 0.02241161616161616, 'baseline_acc': 0.8409090909090909}\n",
      "Tensor(\"baseline_36/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_33/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_33/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.271381098814685, 'crowds_classification_33_loss': -7.271381098814685, 'baseline_loss': 1.1601700496793999, 'crowds_classification_33_acc': 0.021780303030303032, 'baseline_acc': 0.8409090909090909}\n",
      "Tensor(\"baseline_37/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_34/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_34/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.054001991194908, 'crowds_classification_34_loss': -7.054001991194908, 'baseline_loss': 4.809644415109146, 'crowds_classification_34_acc': 0.020938552188552187, 'baseline_acc': 0.6414141414141414}\n",
      "Tensor(\"baseline_38/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_35/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_35/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.309978709879147, 'crowds_classification_35_loss': -7.309978709879147, 'baseline_loss': 0.8981289927806918, 'crowds_classification_35_acc': 0.03167087542087542, 'baseline_acc': 0.8400673400673401}\n",
      "Tensor(\"baseline_39/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_36/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_36/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.193875852257315, 'crowds_classification_36_loss': -7.193875852257315, 'baseline_loss': 2.5542688499195405, 'crowds_classification_36_acc': 0.021254208754208755, 'baseline_acc': 0.7516835016835017}\n",
      "Tensor(\"baseline_40/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_37/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_37/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2745337213329995, 'crowds_classification_37_loss': -7.2745337213329995, 'baseline_loss': 0.9715543274175037, 'crowds_classification_37_acc': 0.025252525252525252, 'baseline_acc': 0.8324915824915825}\n",
      "Tensor(\"baseline_41/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_38/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_38/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.276537408732405, 'crowds_classification_38_loss': -7.276537408732405, 'baseline_loss': 1.0125721020730658, 'crowds_classification_38_acc': 0.03293350168350168, 'baseline_acc': 0.8425925925925926}\n",
      "(200, 4, 4, 512) (200, 8) (10000, 8, 59)\n",
      "\n",
      "Baseline model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.5278879528495197, 'acc': 0.8198653198653199}\n",
      "Test dataset results: \n",
      "{'loss': 0.469300569006891, 'acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': 0.47753517013607605, 'acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': 0.4779000258204913, 'acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': 0.46331710932832776, 'acc': 0.8459595959595959}\n",
      "Test dataset results: \n",
      "{'loss': 0.5114974965551485, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 0.5331737287718841, 'acc': 0.8198653198653199}\n",
      "Test dataset results: \n",
      "{'loss': 0.44814488096068605, 'acc': 0.8484848484848485}\n",
      "Test dataset results: \n",
      "{'loss': 0.5342716142184004, 'acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': 0.42946997952180277, 'acc': 0.8501683501683501}\n",
      "Test dataset results: \n",
      "{'loss': 0.4880120933356911, 'acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': 0.5767406408834939, 'acc': 0.8232323232323232}\n",
      "Test dataset results: \n",
      "{'loss': 0.4701804971755153, 'acc': 0.8409090909090909}\n",
      "Test dataset results: \n",
      "{'loss': 0.494655000992897, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 0.5360541957797427, 'acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': 0.45761841063948994, 'acc': 0.8468013468013468}\n",
      "Test dataset results: \n",
      "{'loss': 0.5678513816962338, 'acc': 0.8089225589225589}\n",
      "Test dataset results: \n",
      "{'loss': 0.5091288814239631, 'acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': 0.4919482298973032, 'acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': 0.5099080230049814, 'acc': 0.8383838383838383}\n",
      "\n",
      "Crowd noise adaptation model with 0.02 clean data\n",
      "Tensor(\"baseline_42/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_39/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_39/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.375283589668146, 'crowds_classification_39_loss': -7.375283589668146, 'baseline_loss': 0.8421689738528897, 'crowds_classification_39_acc': 0.0, 'baseline_acc': 0.8341750841750841}\n",
      "Tensor(\"baseline_43/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_40/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_40/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.285198484607016, 'crowds_classification_40_loss': -7.285198484607016, 'baseline_loss': 1.0126308005064826, 'crowds_classification_40_acc': 0.0, 'baseline_acc': 0.7878787878787878}\n",
      "Tensor(\"baseline_44/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_41/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_41/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.338080675915034, 'crowds_classification_41_loss': -7.338080675915034, 'baseline_loss': 0.93084997800414, 'crowds_classification_41_acc': 0.0, 'baseline_acc': 0.8543771043771043}\n",
      "Tensor(\"baseline_45/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_42/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_42/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.338086330529415, 'crowds_classification_42_loss': -7.338086330529415, 'baseline_loss': 0.9444671857236612, 'crowds_classification_42_acc': 0.02156986531986532, 'baseline_acc': 0.8451178451178452}\n",
      "Tensor(\"baseline_46/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_43/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_43/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.392648791624641, 'crowds_classification_43_loss': -7.392648791624641, 'baseline_loss': 0.8511205374965639, 'crowds_classification_43_acc': 0.018518518518518517, 'baseline_acc': 0.8636363636363636}\n",
      "Tensor(\"baseline_47/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_44/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_44/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3556236626724605, 'crowds_classification_44_loss': -7.3556236626724605, 'baseline_loss': 0.864933033483197, 'crowds_classification_44_acc': 0.022306397306397305, 'baseline_acc': 0.8518518518518519}\n",
      "Tensor(\"baseline_48/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_45/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_45/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.352319890802557, 'crowds_classification_45_loss': -7.352319890802557, 'baseline_loss': 0.877584117079022, 'crowds_classification_45_acc': 0.02220117845117845, 'baseline_acc': 0.8543771043771043}\n",
      "Tensor(\"baseline_49/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_46/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_46/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.364592542551985, 'crowds_classification_46_loss': -7.364592542551985, 'baseline_loss': 0.8551102845556406, 'crowds_classification_46_acc': 0.02220117845117845, 'baseline_acc': 0.8661616161616161}\n",
      "Tensor(\"baseline_50/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_47/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_47/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.39833929241707, 'crowds_classification_47_loss': -7.39833929241707, 'baseline_loss': 0.7308589501011653, 'crowds_classification_47_acc': 0.020412457912457913, 'baseline_acc': 0.8712121212121212}\n",
      "Tensor(\"baseline_51/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_48/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_48/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.37464823546233, 'crowds_classification_48_loss': -7.37464823546233, 'baseline_loss': 0.878589492466815, 'crowds_classification_48_acc': 0.021043771043771045, 'baseline_acc': 0.8695286195286195}\n",
      "Tensor(\"baseline_52/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_49/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_49/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.372348873703568, 'crowds_classification_49_loss': -7.372348873703568, 'baseline_loss': 0.8389451131977216, 'crowds_classification_49_acc': 0.02199074074074074, 'baseline_acc': 0.8636363636363636}\n",
      "Tensor(\"baseline_53/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_50/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_50/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.361280675688978, 'crowds_classification_50_loss': -7.361280675688978, 'baseline_loss': 0.8399681107388803, 'crowds_classification_50_acc': 0.0211489898989899, 'baseline_acc': 0.8577441077441077}\n",
      "Tensor(\"baseline_54/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_51/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_51/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.385576787621084, 'crowds_classification_51_loss': -7.385576787621084, 'baseline_loss': 0.7021935382475355, 'crowds_classification_51_acc': 0.020833333333333332, 'baseline_acc': 0.8653198653198653}\n",
      "Tensor(\"baseline_55/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_52/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_52/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.213423806007462, 'crowds_classification_52_loss': -7.213423806007462, 'baseline_loss': 2.774982283813785, 'crowds_classification_52_acc': 0.022937710437710437, 'baseline_acc': 0.7415824915824916}\n",
      "Tensor(\"baseline_56/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_53/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_53/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.362146459444605, 'crowds_classification_53_loss': -7.362146459444605, 'baseline_loss': 0.7975434490451307, 'crowds_classification_53_acc': 0.020833333333333332, 'baseline_acc': 0.8611111111111112}\n",
      "Tensor(\"baseline_57/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_54/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_54/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3597361567847255, 'crowds_classification_54_loss': -7.3597361567847255, 'baseline_loss': 0.9671378725456068, 'crowds_classification_54_acc': 0.02030723905723906, 'baseline_acc': 0.8518518518518519}\n",
      "Tensor(\"baseline_58/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_55/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_55/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.243295256938999, 'crowds_classification_55_loss': -7.243295256938999, 'baseline_loss': 2.382440455333151, 'crowds_classification_55_acc': 0.020622895622895623, 'baseline_acc': 0.7575757575757576}\n",
      "Tensor(\"baseline_59/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_56/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_56/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.357436819108648, 'crowds_classification_56_loss': -7.357436819108648, 'baseline_loss': 0.9509314999034509, 'crowds_classification_56_acc': 0.020517676767676768, 'baseline_acc': 0.8409090909090909}\n",
      "Tensor(\"baseline_60/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_57/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_57/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.345801836713798, 'crowds_classification_57_loss': -7.345801836713798, 'baseline_loss': 0.794853867003412, 'crowds_classification_57_acc': 0.020728114478114477, 'baseline_acc': 0.8207070707070707}\n",
      "Tensor(\"baseline_61/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_58/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_58/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2706302874016036, 'crowds_classification_58_loss': -7.2706302874016036, 'baseline_loss': 2.43485397752688, 'crowds_classification_58_acc': 0.021464646464646464, 'baseline_acc': 0.7634680134680135}\n",
      "(300, 4, 4, 512) (300, 8) (10000, 8, 59)\n",
      "\n",
      "Baseline model with 0.03 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.49557699732206484, 'acc': 0.8518518518518519}\n",
      "Test dataset results: \n",
      "{'loss': 0.4477361015574097, 'acc': 0.8577441077441077}\n",
      "Test dataset results: \n",
      "{'loss': 0.43931501127955086, 'acc': 0.867003367003367}\n",
      "Test dataset results: \n",
      "{'loss': 0.49564951041130106, 'acc': 0.8543771043771043}\n",
      "Test dataset results: \n",
      "{'loss': 0.4402836995945635, 'acc': 0.8611111111111112}\n",
      "Test dataset results: \n",
      "{'loss': 0.48923498226536644, 'acc': 0.8569023569023569}\n",
      "Test dataset results: \n",
      "{'loss': 0.494027922701354, 'acc': 0.8602693602693603}\n",
      "Test dataset results: \n",
      "{'loss': 0.42562007077464753, 'acc': 0.8712121212121212}\n",
      "Test dataset results: \n",
      "{'loss': 0.4666164497584606, 'acc': 0.8560606060606061}\n",
      "Test dataset results: \n",
      "{'loss': 0.49332012890866306, 'acc': 0.8493265993265994}\n",
      "Test dataset results: \n",
      "{'loss': 0.503462731185987, 'acc': 0.8560606060606061}\n",
      "Test dataset results: \n",
      "{'loss': 0.49223165848740824, 'acc': 0.8552188552188552}\n",
      "Test dataset results: \n",
      "{'loss': 0.45265992668140614, 'acc': 0.8611111111111112}\n",
      "Test dataset results: \n",
      "{'loss': 0.446504374997383, 'acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': 0.5517574114229543, 'acc': 0.8518518518518519}\n",
      "Test dataset results: \n",
      "{'loss': 0.5033789747190797, 'acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': 0.4326590233880663, 'acc': 0.8611111111111112}\n",
      "Test dataset results: \n",
      "{'loss': 0.5210919486553179, 'acc': 0.8493265993265994}\n",
      "Test dataset results: \n",
      "{'loss': 0.5328412481430003, 'acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': 0.4648751435857831, 'acc': 0.8636363636363636}\n",
      "\n",
      "Crowd noise adaptation model with 0.03 clean data\n",
      "Tensor(\"baseline_62/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_59/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_59/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.377634579886491, 'crowds_classification_59_loss': -7.377634579886491, 'baseline_loss': 0.8255755346055184, 'crowds_classification_59_acc': 0.00042087542087542086, 'baseline_acc': 0.8611111111111112}\n",
      "Tensor(\"baseline_63/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_60/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_60/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.196885319269867, 'crowds_classification_60_loss': -7.196885319269867, 'baseline_loss': 3.0505536702345517, 'crowds_classification_60_acc': 0.0, 'baseline_acc': 0.6877104377104377}\n",
      "Tensor(\"baseline_64/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_61/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_61/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.372950984171344, 'crowds_classification_61_loss': -7.372950984171344, 'baseline_loss': 0.8291678223947082, 'crowds_classification_61_acc': 0.0, 'baseline_acc': 0.8358585858585859}\n",
      "Tensor(\"baseline_65/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_62/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_62/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.338504561671504, 'crowds_classification_62_loss': -7.338504561671504, 'baseline_loss': 1.1561829141695492, 'crowds_classification_62_acc': 0.00010521885521885521, 'baseline_acc': 0.8417508417508418}\n",
      "Tensor(\"baseline_66/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_63/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_63/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.31179030415185, 'crowds_classification_63_loss': -7.31179030415185, 'baseline_loss': 1.6940734613724429, 'crowds_classification_63_acc': 0.02199074074074074, 'baseline_acc': 0.7760942760942761}\n",
      "Tensor(\"baseline_67/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_64/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_64/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.381607609565812, 'crowds_classification_64_loss': -7.381607609565812, 'baseline_loss': 0.8515809198922982, 'crowds_classification_64_acc': 0.00010521885521885521, 'baseline_acc': 0.8417508417508418}\n",
      "Tensor(\"baseline_68/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_65/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_65/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.352123053386958, 'crowds_classification_65_loss': -7.352123053386958, 'baseline_loss': 1.2406485287428706, 'crowds_classification_65_acc': 0.0, 'baseline_acc': 0.7887205387205387}\n",
      "Tensor(\"baseline_69/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_66/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_66/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.362230111452867, 'crowds_classification_66_loss': -7.362230111452867, 'baseline_loss': 0.8832817222132827, 'crowds_classification_66_acc': 0.02335858585858586, 'baseline_acc': 0.8451178451178452}\n",
      "Tensor(\"baseline_70/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_67/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_67/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2568837303906575, 'crowds_classification_67_loss': -7.2568837303906575, 'baseline_loss': 2.5032879242591988, 'crowds_classification_67_acc': 0.00010521885521885521, 'baseline_acc': 0.7693602693602694}\n",
      "Tensor(\"baseline_71/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_68/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_68/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.416501658532756, 'crowds_classification_68_loss': -7.416501658532756, 'baseline_loss': 0.8178328365238026, 'crowds_classification_68_acc': 0.0, 'baseline_acc': 0.8661616161616161}\n",
      "Tensor(\"baseline_72/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_69/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_69/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3883688939540875, 'crowds_classification_69_loss': -7.3883688939540875, 'baseline_loss': 0.8402155737082163, 'crowds_classification_69_acc': 0.0, 'baseline_acc': 0.8291245791245792}\n",
      "Tensor(\"baseline_73/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_70/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_70/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.338966111141423, 'crowds_classification_70_loss': -7.338966111141423, 'baseline_loss': 1.287032815342399, 'crowds_classification_70_acc': 0.0, 'baseline_acc': 0.7929292929292929}\n",
      "Tensor(\"baseline_74/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_71/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_71/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.423537533692639, 'crowds_classification_71_loss': -7.423537533692639, 'baseline_loss': 0.7535728050101085, 'crowds_classification_71_acc': 0.00021043771043771043, 'baseline_acc': 0.8762626262626263}\n",
      "Tensor(\"baseline_75/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_72/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_72/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.354861222533666, 'crowds_classification_72_loss': -7.354861222533666, 'baseline_loss': 1.1684603737259553, 'crowds_classification_72_acc': 0.00021043771043771043, 'baseline_acc': 0.7853535353535354}\n",
      "Tensor(\"baseline_76/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_73/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_73/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.411678803890241, 'crowds_classification_73_loss': -7.411678803890241, 'baseline_loss': 0.7074719174993961, 'crowds_classification_73_acc': 0.00021043771043771043, 'baseline_acc': 0.8720538720538721}\n",
      "Tensor(\"baseline_77/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_74/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_74/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.273127260432902, 'crowds_classification_74_loss': -7.273127260432902, 'baseline_loss': 2.41661558538575, 'crowds_classification_74_acc': 0.0, 'baseline_acc': 0.7407407407407407}\n",
      "Tensor(\"baseline_78/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_75/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_75/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.400387725444755, 'crowds_classification_75_loss': -7.400387725444755, 'baseline_loss': 0.8910493953059418, 'crowds_classification_75_acc': 0.020622895622895623, 'baseline_acc': 0.8425925925925926}\n",
      "Tensor(\"baseline_79/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_76/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_76/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.307567132442487, 'crowds_classification_76_loss': -7.307567132442487, 'baseline_loss': 2.5116050454101178, 'crowds_classification_76_acc': 0.00010521885521885521, 'baseline_acc': 0.7651515151515151}\n",
      "Tensor(\"baseline_80/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_77/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_77/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.293669851540717, 'crowds_classification_77_loss': -7.293669851540717, 'baseline_loss': 2.6305938347521858, 'crowds_classification_77_acc': 0.021464646464646464, 'baseline_acc': 0.7643097643097643}\n",
      "Tensor(\"baseline_81/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_78/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_78/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.414957521740435, 'crowds_classification_78_loss': -7.414957521740435, 'baseline_loss': 0.680633088379396, 'crowds_classification_78_acc': 0.020728114478114477, 'baseline_acc': 0.8636363636363636}\n",
      "(400, 4, 4, 512) (400, 8) (10000, 8, 59)\n",
      "\n",
      "Baseline model with 0.04 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.4253368493771613, 'acc': 0.8712121212121212}\n",
      "Test dataset results: \n",
      "{'loss': 0.41987885130876645, 'acc': 0.8737373737373737}\n",
      "Test dataset results: \n",
      "{'loss': 0.4638470105500735, 'acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': 0.4449079075219856, 'acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': 0.481005335320728, 'acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': 0.43985537843471423, 'acc': 0.8602693602693603}\n",
      "Test dataset results: \n",
      "{'loss': 0.4417877925515978, 'acc': 0.8661616161616161}\n",
      "Test dataset results: \n",
      "{'loss': 0.45826738426831837, 'acc': 0.867003367003367}\n",
      "Test dataset results: \n",
      "{'loss': 0.4438839841772009, 'acc': 0.8686868686868687}\n",
      "Test dataset results: \n",
      "{'loss': 0.48384892386117767, 'acc': 0.8577441077441077}\n",
      "Test dataset results: \n",
      "{'loss': 0.4631512348198329, 'acc': 0.867003367003367}\n",
      "Test dataset results: \n",
      "{'loss': 0.46902432547951184, 'acc': 0.8602693602693603}\n",
      "Test dataset results: \n",
      "{'loss': 0.41226212873503015, 'acc': 0.8779461279461279}\n",
      "Test dataset results: \n",
      "{'loss': 0.46020803765491003, 'acc': 0.8737373737373737}\n",
      "Test dataset results: \n",
      "{'loss': 0.41099683903094997, 'acc': 0.8762626262626263}\n",
      "Test dataset results: \n",
      "{'loss': 0.4571088824032332, 'acc': 0.8644781144781145}\n",
      "Test dataset results: \n",
      "{'loss': 0.45870364637988986, 'acc': 0.8644781144781145}\n",
      "Test dataset results: \n",
      "{'loss': 0.402376504859539, 'acc': 0.872895622895623}\n",
      "Test dataset results: \n",
      "{'loss': 0.44356626099107244, 'acc': 0.8695286195286195}\n",
      "Test dataset results: \n",
      "{'loss': 0.4891617439611994, 'acc': 0.8594276094276094}\n",
      "\n",
      "Crowd noise adaptation model with 0.04 clean data\n",
      "Tensor(\"baseline_82/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_79/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_79/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.301608064760664, 'crowds_classification_79_loss': -7.301608064760664, 'baseline_loss': 2.413249167428675, 'crowds_classification_79_acc': 0.00021043771043771043, 'baseline_acc': 0.7643097643097643}\n",
      "Tensor(\"baseline_83/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_80/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_80/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.455032101383916, 'crowds_classification_80_loss': -7.455032101383916, 'baseline_loss': 0.7004416526878497, 'crowds_classification_80_acc': 0.021254208754208755, 'baseline_acc': 0.8787878787878788}\n",
      "Tensor(\"baseline_84/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_81/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_81/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.45015137765544, 'crowds_classification_81_loss': -7.45015137765544, 'baseline_loss': 0.6690652806324389, 'crowds_classification_81_acc': 0.00021043771043771043, 'baseline_acc': 0.867003367003367}\n",
      "Tensor(\"baseline_85/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_82/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_82/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.428996710664896, 'crowds_classification_82_loss': -7.428996710664896, 'baseline_loss': 0.805460576129519, 'crowds_classification_82_acc': 0.00010521885521885521, 'baseline_acc': 0.8754208754208754}\n",
      "Tensor(\"baseline_86/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_83/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_83/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.4335981137824785, 'crowds_classification_83_loss': -7.4335981137824785, 'baseline_loss': 0.7901180556125512, 'crowds_classification_83_acc': 0.00021043771043771043, 'baseline_acc': 0.8695286195286195}\n",
      "Tensor(\"baseline_87/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_84/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_84/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.374980436832415, 'crowds_classification_84_loss': -7.374980436832415, 'baseline_loss': 0.8820430191879722, 'crowds_classification_84_acc': 0.0, 'baseline_acc': 0.7929292929292929}\n",
      "Tensor(\"baseline_88/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_85/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_85/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.338741432536732, 'crowds_classification_85_loss': -7.338741432536732, 'baseline_loss': 1.4157180796167264, 'crowds_classification_85_acc': 0.0, 'baseline_acc': 0.7794612794612794}\n",
      "Tensor(\"baseline_89/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_86/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_86/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.351350016866871, 'crowds_classification_86_loss': -7.351350016866871, 'baseline_loss': 2.38859614057573, 'crowds_classification_86_acc': 0.0, 'baseline_acc': 0.7836700336700336}\n",
      "Tensor(\"baseline_90/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_87/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_87/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.226791108898843, 'crowds_classification_87_loss': -7.226791108898843, 'baseline_loss': 2.8393600754163884, 'crowds_classification_87_acc': 0.0, 'baseline_acc': 0.7281144781144782}\n",
      "Tensor(\"baseline_91/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_88/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_88/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2956580136360145, 'crowds_classification_88_loss': -7.2956580136360145, 'baseline_loss': 1.8942893896407138, 'crowds_classification_88_acc': 0.00042087542087542086, 'baseline_acc': 0.7786195286195287}\n",
      "Tensor(\"baseline_92/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_89/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_89/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.210213781607272, 'crowds_classification_89_loss': -7.210213781607272, 'baseline_loss': 3.0569308775442616, 'crowds_classification_89_acc': 0.00010521885521885521, 'baseline_acc': 0.7011784511784511}\n",
      "Tensor(\"baseline_93/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_90/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_90/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.445269566995126, 'crowds_classification_90_loss': -7.445269566995126, 'baseline_loss': 0.6268369796143387, 'crowds_classification_90_acc': 0.0003156565656565657, 'baseline_acc': 0.8695286195286195}\n",
      "Tensor(\"baseline_94/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_91/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_91/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3881735881972395, 'crowds_classification_91_loss': -7.3881735881972395, 'baseline_loss': 1.0669475613218364, 'crowds_classification_91_acc': 0.0, 'baseline_acc': 0.7920875420875421}\n",
      "Tensor(\"baseline_95/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_92/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_92/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -7.283840226003217, 'crowds_classification_92_loss': -7.283840226003217, 'baseline_loss': 2.3595549964944924, 'crowds_classification_92_acc': 0.00021043771043771043, 'baseline_acc': 0.7676767676767676}\n",
      "Tensor(\"baseline_96/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_93/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_93/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.446992763365158, 'crowds_classification_93_loss': -7.446992763365158, 'baseline_loss': 0.6541185999933555, 'crowds_classification_93_acc': 0.00042087542087542086, 'baseline_acc': 0.8787878787878788}\n",
      "Tensor(\"baseline_97/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_94/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_94/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.350095731240732, 'crowds_classification_94_loss': -7.350095731240732, 'baseline_loss': 1.188267505530155, 'crowds_classification_94_acc': 0.0, 'baseline_acc': 0.7895622895622896}\n",
      "Tensor(\"baseline_98/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_95/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_95/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.352036973844072, 'crowds_classification_95_loss': -7.352036973844072, 'baseline_loss': 0.9984524409578304, 'crowds_classification_95_acc': 0.02199074074074074, 'baseline_acc': 0.8080808080808081}\n",
      "Tensor(\"baseline_99/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_96/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_96/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.445569845963809, 'crowds_classification_96_loss': -7.445569845963809, 'baseline_loss': 0.6763344155467721, 'crowds_classification_96_acc': 0.0, 'baseline_acc': 0.8745791245791246}\n",
      "Tensor(\"baseline_100/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_97/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_97/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.398826740406178, 'crowds_classification_97_loss': -7.398826740406178, 'baseline_loss': 0.8524095843912978, 'crowds_classification_97_acc': 0.00010521885521885521, 'baseline_acc': 0.8602693602693603}\n",
      "Tensor(\"baseline_101/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_98/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_98/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.278986062265004, 'crowds_classification_98_loss': -7.278986062265004, 'baseline_loss': 2.5271438926157326, 'crowds_classification_98_acc': 0.02199074074074074, 'baseline_acc': 0.7449494949494949}\n",
      "(500, 4, 4, 512) (500, 8) (10000, 8, 59)\n",
      "\n",
      "Baseline model with 0.05 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.43707897423167624, 'acc': 0.8813131313131313}\n",
      "Test dataset results: \n",
      "{'loss': 0.4651145068489592, 'acc': 0.8779461279461279}\n",
      "Test dataset results: \n",
      "{'loss': 0.49613381535291473, 'acc': 0.877104377104377}\n",
      "Test dataset results: \n",
      "{'loss': 0.5286624698589245, 'acc': 0.867003367003367}\n",
      "Test dataset results: \n",
      "{'loss': 0.48312611119376386, 'acc': 0.8737373737373737}\n",
      "Test dataset results: \n",
      "{'loss': 0.46297855739503696, 'acc': 0.8686868686868687}\n",
      "Test dataset results: \n",
      "{'loss': 0.4416087446671594, 'acc': 0.8821548821548821}\n",
      "Test dataset results: \n",
      "{'loss': 0.45701048109266496, 'acc': 0.8712121212121212}\n",
      "Test dataset results: \n",
      "{'loss': 0.4901131643421371, 'acc': 0.8720538720538721}\n",
      "Test dataset results: \n",
      "{'loss': 0.4823153900478023, 'acc': 0.877104377104377}\n",
      "Test dataset results: \n",
      "{'loss': 0.48839395268748104, 'acc': 0.8703703703703703}\n",
      "Test dataset results: \n",
      "{'loss': 0.4628144155134155, 'acc': 0.8754208754208754}\n",
      "Test dataset results: \n",
      "{'loss': 0.5097068506747685, 'acc': 0.8695286195286195}\n",
      "Test dataset results: \n",
      "{'loss': 0.41532187406224536, 'acc': 0.8796296296296297}\n",
      "Test dataset results: \n",
      "{'loss': 0.4482664580798711, 'acc': 0.8787878787878788}\n",
      "Test dataset results: \n",
      "{'loss': 0.488470172939151, 'acc': 0.8762626262626263}\n",
      "Test dataset results: \n",
      "{'loss': 0.48649492561315444, 'acc': 0.8686868686868687}\n",
      "Test dataset results: \n",
      "{'loss': 0.5274282179973543, 'acc': 0.8653198653198653}\n",
      "Test dataset results: \n",
      "{'loss': 0.4965682941981237, 'acc': 0.8695286195286195}\n",
      "Test dataset results: \n",
      "{'loss': 0.5304816465272948, 'acc': 0.8653198653198653}\n",
      "\n",
      "Crowd noise adaptation model with 0.05 clean data\n",
      "Tensor(\"baseline_102/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_99/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_99/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.285175423028091, 'crowds_classification_99_loss': -7.285175423028091, 'baseline_loss': 3.088046242166248, 'crowds_classification_99_acc': 0.024305555555555556, 'baseline_acc': 0.7104377104377104}\n",
      "Tensor(\"baseline_103/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_100/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_100/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.328687701562439, 'crowds_classification_100_loss': -7.328687701562439, 'baseline_loss': 2.4931795407926995, 'crowds_classification_100_acc': 0.0, 'baseline_acc': 0.7634680134680135}\n",
      "Tensor(\"baseline_104/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_101/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_101/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3540583877049714, 'crowds_classification_101_loss': -7.3540583877049714, 'baseline_loss': 1.1045468352760128, 'crowds_classification_101_acc': 0.00021043771043771043, 'baseline_acc': 0.8518518518518519}\n",
      "Tensor(\"baseline_105/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_102/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_102/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.375310237961586, 'crowds_classification_102_loss': -7.375310237961586, 'baseline_loss': 1.27916126941591, 'crowds_classification_102_acc': 0.00010521885521885521, 'baseline_acc': 0.7904040404040404}\n",
      "Tensor(\"baseline_106/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_103/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_103/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.422409513582686, 'crowds_classification_103_loss': -7.422409513582686, 'baseline_loss': 0.8706350789628045, 'crowds_classification_103_acc': 0.0, 'baseline_acc': 0.8341750841750841}\n",
      "Tensor(\"baseline_107/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_104/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_104/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.3380469938721316, 'crowds_classification_104_loss': -7.3380469938721316, 'baseline_loss': 1.023763813203834, 'crowds_classification_104_acc': 0.00010521885521885521, 'baseline_acc': 0.8493265993265994}\n",
      "Tensor(\"baseline_108/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_105/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_105/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.450548311676642, 'crowds_classification_105_loss': -7.450548311676642, 'baseline_loss': 0.8516294913309993, 'crowds_classification_105_acc': 0.019886363636363636, 'baseline_acc': 0.867003367003367}\n",
      "Tensor(\"baseline_109/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_106/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_106/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.465105104928065, 'crowds_classification_106_loss': -7.465105104928065, 'baseline_loss': 0.7332952420010862, 'crowds_classification_106_acc': 0.0, 'baseline_acc': 0.8804713804713805}\n",
      "Tensor(\"baseline_110/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_107/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_107/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.313569924646757, 'crowds_classification_107_loss': -7.313569924646757, 'baseline_loss': 2.526913058627334, 'crowds_classification_107_acc': 0.00010521885521885521, 'baseline_acc': 0.7685185185185185}\n",
      "Tensor(\"baseline_111/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_108/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_108/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.405862819466125, 'crowds_classification_108_loss': -7.405862819466125, 'baseline_loss': 1.5919822570451367, 'crowds_classification_108_acc': 0.0, 'baseline_acc': 0.7937710437710438}\n",
      "Tensor(\"baseline_112/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_109/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_109/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.357849021551987, 'crowds_classification_109_loss': -7.357849021551987, 'baseline_loss': 2.419643166490677, 'crowds_classification_109_acc': 0.0, 'baseline_acc': 0.7887205387205387}\n",
      "Tensor(\"baseline_113/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_110/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_110/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.14042966534393, 'crowds_classification_110_loss': -7.14042966534393, 'baseline_loss': 4.289359038049725, 'crowds_classification_110_acc': 0.0, 'baseline_acc': 0.6531986531986532}\n",
      "Tensor(\"baseline_114/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_111/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_111/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.42125595298279, 'crowds_classification_111_loss': -7.42125595298279, 'baseline_loss': 0.8569567040461884, 'crowds_classification_111_acc': 0.020622895622895623, 'baseline_acc': 0.8644781144781145}\n",
      "Tensor(\"baseline_115/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_112/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_112/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.29628074369848, 'crowds_classification_112_loss': -7.29628074369848, 'baseline_loss': 2.5807004541484795, 'crowds_classification_112_acc': 0.00010521885521885521, 'baseline_acc': 0.7567340067340067}\n",
      "Tensor(\"baseline_116/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_113/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_113/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.249158165671608, 'crowds_classification_113_loss': -7.249158165671608, 'baseline_loss': 2.340267801124239, 'crowds_classification_113_acc': 0.0003156565656565657, 'baseline_acc': 0.7542087542087542}\n",
      "Tensor(\"baseline_117/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_114/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_114/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.4400287140098085, 'crowds_classification_114_loss': -7.4400287140098085, 'baseline_loss': 0.8087819734531821, 'crowds_classification_114_acc': 0.00010521885521885521, 'baseline_acc': 0.8653198653198653}\n",
      "Tensor(\"baseline_118/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_115/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_115/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.317034316785408, 'crowds_classification_115_loss': -7.317034316785408, 'baseline_loss': 2.6137039997436182, 'crowds_classification_115_acc': 0.020728114478114477, 'baseline_acc': 0.7651515151515151}\n",
      "Tensor(\"baseline_119/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_116/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_116/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.374825204663003, 'crowds_classification_116_loss': -7.374825204663003, 'baseline_loss': 2.2557210532904475, 'crowds_classification_116_acc': 0.00010521885521885521, 'baseline_acc': 0.7895622895622896}\n",
      "Tensor(\"baseline_120/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_117/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_117/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.313739810327087, 'crowds_classification_117_loss': -7.313739810327087, 'baseline_loss': 2.5088513919155457, 'crowds_classification_117_acc': 0.020622895622895623, 'baseline_acc': 0.7676767676767676}\n",
      "Tensor(\"baseline_121/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_118/CrowdLayer:0' shape=(8, 8, 59) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_118/Reshape_2:0\", shape=(?, 8, 59), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.392482797706167, 'crowds_classification_118_loss': -7.392482797706167, 'baseline_loss': 2.333082115610039, 'crowds_classification_118_acc': 0.0, 'baseline_acc': 0.781986531986532}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 20\n",
    "N_CLASSES = 8\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/LabelMe/prepared/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    x_sample = x.copy()\n",
    "    x_sample['train'] = x_sample['train'][:round(x_sample['train'].shape[0]*clean_percent)]\n",
    "    y_gt_sample = y_gt.copy()\n",
    "    y_gt_sample['train'] = y_gt_sample['train'][:round(y_gt_sample['train'].shape[0]*clean_percent)]\n",
    "    y_annot_mix = y_annot.copy()\n",
    "    y_annot_mix['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)] \\\n",
    "        = np.reshape(np.repeat(y_gt['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)], N_ANNOT, axis=1)\n",
    "                     , (-1, N_CLASSES, N_ANNOT))\n",
    "    print(x_sample['train'].shape, y_gt_sample['train'].shape, y_annot_mix['train'].shape)\n",
    "\n",
    "    prefix = 'pct_%.2f_'%clean_percent\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        clean_base_acc_df = pd.DataFrame()\n",
    "        clean_base_loss_df = pd.DataFrame()\n",
    "        clean_history = baseline_gt(x_sample, y_gt_sample, N_CLASSES)\n",
    "        clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "        clean_base_loss_df.loc[:, i] = clean_history.history['loss']\n",
    "            \n",
    "    print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        acc_df = pd.DataFrame()\n",
    "        loss_df = pd.DataFrame()\n",
    "        history, trace_arr = crowd_model(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "        acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "        loss_df.loc[:, i] = history.history['baseline_loss']\n",
    "        \n",
    "    print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        acc_df = pd.DataFrame()\n",
    "        loss_df = pd.DataFrame()\n",
    "        history, trace_arr = crowd_model_pretrain_with_clean_data(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "        acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "        loss_df.loc[:, i] = history.history['baseline_loss']    \n",
    "\n",
    "    #     clean_base_acc_df.to_csv('clean_accuracy.csv')\n",
    "    #     clean_base_loss_df.to_csv('clean_loss.csv')\n",
    "    #     print_history(clean_acc_df, 'accuracy')\n",
    "    #     print_history(clean_loss_df, 'loss')\n",
    "\n",
    "#         acc_df.to_csv(acc_csv)\n",
    "#         loss_df.to_csv(loss_csv)\n",
    "#         trace_df.to_csv(trace_csv)\n",
    "\n",
    "#     print_history(trace_crowd_acc_df, 'accuracy')\n",
    "#     print_history(trace_crowd_loss_df, 'loss')\n",
    "# acc_df.loc[:, str(b)] = history.history['baseline_acc']\n",
    "# loss_df.loc[:, str(b)] = history.history['baseline_loss']\n",
    "\n",
    "# acc_df.to_csv('accuracy.csv')\n",
    "# loss_df.to_csv('loss.csv')\n",
    "# print_history(acc_df, 'accuracy')\n",
    "# print_history(loss_df, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogs and cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 20\n",
    "N_CLASSES = 8\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/LabelMe/prepared/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    x_sample = x.copy()\n",
    "    x_sample['train'] = x_sample['train'][:round(x_sample['train'].shape[0]*clean_percent)]\n",
    "    y_gt_sample = y_gt.copy()\n",
    "    y_gt_sample['train'] = y_gt_sample['train'][:round(y_gt_sample['train'].shape[0]*clean_percent)]\n",
    "    y_annot_mix = y_annot.copy()\n",
    "    y_annot_mix['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)] \\\n",
    "        = np.reshape(np.repeat(y_gt['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)], N_ANNOT, axis=1)\n",
    "                     , (-1, N_CLASSES, N_ANNOT))\n",
    "    print(x_sample['train'].shape, y_gt_sample['train'].shape, y_annot_mix['train'].shape)\n",
    "\n",
    "    prefix = 'pct_%.2f_'%clean_percent\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        clean_base_acc_df = pd.DataFrame()\n",
    "        clean_base_loss_df = pd.DataFrame()\n",
    "        clean_history = baseline_gt(x_sample, y_gt_sample, N_CLASSES)\n",
    "        clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "        clean_base_loss_df.loc[:, i] = clean_history.history['loss']\n",
    "            \n",
    "    print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        acc_df = pd.DataFrame()\n",
    "        loss_df = pd.DataFrame()\n",
    "        history, trace_arr = crowd_model(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "        acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "        loss_df.loc[:, i] = history.history['baseline_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Bird Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cactus Wren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(3069, 7, 7, 512)\n",
      "(3069,)\n",
      "(3069,)\n",
      "\n",
      "Loading AMT data...\n",
      "(3069, 31)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 31\n",
      "\n",
      "Loading test data...\n",
      "(176, 7, 7, 512)\n",
      "(176,)\n",
      "\n",
      "Loading validation data...\n",
      "(363, 7, 7, 512)\n",
      "(363,)\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 4.580449364402077, 'acc': 0.6420454545454546}\n",
      "Test dataset results: \n",
      "{'loss': 2.014761946418069, 'acc': 0.875}\n",
      "Test dataset results: \n",
      "{'loss': 1.6465330990878018, 'acc': 0.7386363636363636}\n",
      "Test dataset results: \n",
      "{'loss': 1.007381016557867, 'acc': 0.9375}\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 2.49463267107223, 'acc': 0.7840909090909091}\n",
      "Test dataset results: \n",
      "{'loss': 2.747402613813227, 'acc': 0.8295454545454546}\n",
      "Test dataset results: \n",
      "{'loss': 9.89710773121227, 'acc': 0.24431818181818182}\n",
      "Test dataset results: \n",
      "{'loss': 1.8316018256274136, 'acc': 0.8863636363636364}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_54/dropout_58/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_16/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_17/CrowdLayer:0' shape=(2, 2, 31) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.965367848222906, 'crowds_classification_s_model_channel_matrix_17_loss': 1.965367848222906, 'baseline_loss': 7.394810849970037, 'crowds_classification_s_model_channel_matrix_17_acc': 0.08522727272727272, 'baseline_acc': 0.3465909090909091}\n",
      "[-1.2145348 -1.222816  -1.2283239 -1.2283609 -1.2212453 -1.2242062\n",
      " -1.2284982 -1.2317678 -1.218075  -1.2257532 -1.2219071 -1.222049\n",
      " -1.2245255 -1.2205535 -1.2268302 -1.2289101 -1.2258186 -1.2163831\n",
      " -1.2224633 -1.218777  -1.2224998 -1.2273726 -1.2308598 -1.2217655\n",
      " -1.224704  -1.2184628 -1.2277637 -1.2150924 -1.2168875 -1.2214345\n",
      " -1.2224519]\n",
      "[-0.05335602  0.37418503 -0.02978235 -0.45479172  0.19985743  0.287719\n",
      " -1.2153287  -0.22152734 -0.58365655 -0.14410618 -0.4113449  -0.02904806\n",
      " -0.33202973 -0.5428674  -0.3458145  -0.01546493 -0.15257566 -0.37932542\n",
      " -1.2395889  -0.4009359  -1.2886758  -0.03542978  0.21425138 -0.09808487\n",
      " -0.8290505  -0.81355363 -1.6498114  -1.6273695  -1.5924728  -0.00810894\n",
      "  0.2408826 ]\n",
      "Test dataset results: \n",
      "{'loss': 1.866253744472157, 'crowds_classification_s_model_channel_matrix_17_loss': 1.866253744472157, 'baseline_loss': 4.304264166138389, 'crowds_classification_s_model_channel_matrix_17_acc': 0.0, 'baseline_acc': 0.7329545454545454}\n",
      "inputs:  [<tf.Tensor 'sequential_55/dropout_59/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_17/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_18/CrowdLayer:0' shape=(2, 2, 31) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.9649707729166204, 'crowds_classification_s_model_channel_matrix_18_loss': 1.9649707729166204, 'baseline_loss': 7.276861971074885, 'crowds_classification_s_model_channel_matrix_18_acc': 0.0, 'baseline_acc': 0.4375}\n",
      "[-1.2250197 -1.224402  -1.2247782 -1.2259915 -1.2222698 -1.2228286\n",
      " -1.2232888 -1.21521   -1.2240584 -1.2160335 -1.2231953 -1.2213252\n",
      " -1.2241437 -1.2271719 -1.2213378 -1.2242043 -1.2230151 -1.2206354\n",
      " -1.2266321 -1.2206974 -1.2221569 -1.2315139 -1.2174493 -1.214993\n",
      " -1.2183373 -1.216233  -1.219172  -1.2206578 -1.2201128 -1.217938\n",
      " -1.2224355]\n",
      "[-0.07025611  0.53250134  0.04522908 -0.24777141  0.24132088  0.21664801\n",
      " -1.2722995  -0.03572831 -0.48678023  0.00391683 -0.4608474   0.14354768\n",
      " -0.25508755 -0.44029063 -0.19981241  0.08891973 -0.22680983 -0.48627758\n",
      " -1.2317109  -0.37202564 -1.2138941   0.27905273  0.26002     0.18675552\n",
      " -0.71749127 -0.7197671  -1.7170966  -1.7213128  -1.7404995  -0.00685543\n",
      "  0.35208002]\n",
      "Test dataset results: \n",
      "{'loss': 1.8877869085832075, 'crowds_classification_s_model_channel_matrix_18_loss': 1.8877869085832075, 'baseline_loss': 4.3963140791112725, 'crowds_classification_s_model_channel_matrix_18_acc': 0.0, 'baseline_acc': 0.7272727272727273}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Cactus Wren/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(3510, 7, 7, 512)\n",
      "(3510,)\n",
      "(3510,)\n",
      "\n",
      "Loading AMT data...\n",
      "(3510, 40)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 40\n",
      "\n",
      "Loading test data...\n",
      "(210, 7, 7, 512)\n",
      "(210,)\n",
      "\n",
      "Loading validation data...\n",
      "(405, 7, 7, 512)\n",
      "(405,)\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 3.363896878560384, 'acc': 0.6476190487543741}\n",
      "Test dataset results: \n",
      "{'loss': 3.453891637211754, 'acc': 0.7857142857142857}\n",
      "Test dataset results: \n",
      "{'loss': 10.050633058093844, 'acc': 0.2857142858562015}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 7.196937047867548, 'acc': 0.4190476207506089}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "Test dataset results: \n",
      "{'loss': 5.360020560310001, 'acc': 0.5095238098076411}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_36/dropout_36/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_10/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_11/CrowdLayer:0' shape=(2, 2, 40) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.954318726630438, 'crowds_classification_s_model_channel_matrix_11_loss': 1.954318726630438, 'baseline_loss': 4.963432365372068, 'crowds_classification_s_model_channel_matrix_11_acc': 0.06904761933145069, 'baseline_acc': 0.5190476173446292}\n",
      "[-1.214625  -1.2225301 -1.2315772 -1.2202654 -1.2156637 -1.219775\n",
      " -1.2205465 -1.2202355 -1.2246528 -1.2240222 -1.2230691 -1.2218487\n",
      " -1.2168307 -1.2156943 -1.2256112 -1.2219603 -1.2200146 -1.2230723\n",
      " -1.2316959 -1.2192756 -1.225567  -1.2213427 -1.2213001 -1.224585\n",
      " -1.2218451 -1.2185788 -1.2295411 -1.2284455 -1.2232418 -1.2289993\n",
      " -1.2284415 -1.2203555 -1.2188916 -1.2198966 -1.2263379 -1.2189612\n",
      " -1.2228476 -1.2209213 -1.2189511 -1.2249522]\n",
      "[-0.11793998 -0.05818248 -0.13086095 -0.2770066  -0.2615344  -0.05057818\n",
      " -0.8260736  -1.7610115  -1.282572   -0.16903532 -0.08748335 -2.1466975\n",
      " -0.10428065 -0.06714797 -0.06941676 -0.5833692  -0.5632809  -0.57106054\n",
      " -1.4741534  -0.3905257  -0.05126268 -0.14119849 -0.19573602 -0.57210696\n",
      " -1.90217    -0.5743197  -2.4272501  -2.3583794  -0.08687335 -0.576204\n",
      " -1.9232346  -0.54916877 -0.58648646 -0.71561116 -2.2863088  -1.7608438\n",
      " -0.43456045 -1.8832152  -1.8755567  -1.8991568 ]\n",
      "Test dataset results: \n",
      "{'loss': 1.9380079757599604, 'crowds_classification_s_model_channel_matrix_11_loss': 1.9380079757599604, 'baseline_loss': 3.453877686318897, 'crowds_classification_s_model_channel_matrix_11_acc': 0.0, 'baseline_acc': 0.7857142857142857}\n",
      "inputs:  [<tf.Tensor 'sequential_37/dropout_37/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_11/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_12/CrowdLayer:0' shape=(2, 2, 40) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.968616415205456, 'crowds_classification_s_model_channel_matrix_12_loss': 1.968616415205456, 'baseline_loss': 8.890265473865327, 'crowds_classification_s_model_channel_matrix_12_acc': 0.0, 'baseline_acc': 0.3666666667376246}\n",
      "[-1.2223322 -1.2280217 -1.2230384 -1.2199016 -1.2236457 -1.2289549\n",
      " -1.2240506 -1.229531  -1.2239687 -1.2258353 -1.2227023 -1.2204988\n",
      " -1.2272933 -1.2167273 -1.2229995 -1.223974  -1.2276158 -1.2272159\n",
      " -1.2142353 -1.2160599 -1.2276628 -1.2206247 -1.2184012 -1.2223468\n",
      " -1.2158633 -1.2275271 -1.2171713 -1.2228966 -1.2204975 -1.2193968\n",
      " -1.2190998 -1.2243402 -1.2179408 -1.2270055 -1.2227778 -1.2170029\n",
      " -1.2180742 -1.2272317 -1.2259493 -1.2253311]\n",
      "[-0.2146227   0.2649197  -0.24356124 -0.3544624  -0.35027838 -0.17893767\n",
      " -0.24130052 -1.7681172  -1.2195606   0.16856796 -0.14874226 -2.0809147\n",
      " -0.19239861 -0.11651659 -0.12091118 -0.5953253  -0.60212994 -0.5649527\n",
      " -1.4135033  -0.34934908 -0.45663053 -0.1899243  -0.6809007  -0.5833593\n",
      " -1.8529342  -0.6219734  -2.3342924  -2.2669568  -0.20060378 -0.5620404\n",
      " -1.8955861  -0.5580819  -0.58448946 -0.20409276 -1.7529298  -1.0968785\n",
      "  0.13220333 -1.8241385  -1.8805497  -1.901188  ]\n",
      "Test dataset results: \n",
      "{'loss': 1.932643416949681, 'crowds_classification_s_model_channel_matrix_12_loss': 1.932643416949681, 'baseline_loss': 3.453877686318897, 'crowds_classification_s_model_channel_matrix_12_acc': 0.0023809523986918586, 'baseline_acc': 0.7857142857142857}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Cape May Warbler/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(4605, 7, 7, 512)\n",
      "(4605,)\n",
      "(4605,)\n",
      "\n",
      "Loading AMT data...\n",
      "(4605, 26)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 26\n",
      "\n",
      "Loading test data...\n",
      "(270, 7, 7, 512)\n",
      "(270,)\n",
      "\n",
      "Loading validation data...\n",
      "(540, 7, 7, 512)\n",
      "(540,)\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 5.301747682359483, 'acc': 0.5111111114422481}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 8.372794212456103, 'acc': 0.3370370357124894}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 6.542580353772199, 'acc': 0.4629629620799312}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 6.45135940975613, 'acc': 0.4555555558866925}\n",
      "Test dataset results: \n",
      "{'loss': 2.746045853473522, 'acc': 0.8296296296296296}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_42/dropout_42/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_12/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_13/CrowdLayer:0' shape=(2, 2, 26) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.98194877659833, 'crowds_classification_s_model_channel_matrix_13_loss': 1.98194877659833, 'baseline_loss': 12.234007828323929, 'crowds_classification_s_model_channel_matrix_13_acc': 0.007407407407407408, 'baseline_acc': 0.18518518518518517}\n",
      "[-1.2201753 -1.2268286 -1.214236  -1.2193261 -1.2176983 -1.2275063\n",
      " -1.225966  -1.2285258 -1.2298713 -1.2230749 -1.2142867 -1.2132062\n",
      " -1.2232786 -1.2168393 -1.2270377 -1.2207963 -1.2258315 -1.2266085\n",
      " -1.2213564 -1.2252138 -1.2309152 -1.2253373 -1.2243654 -1.2218494\n",
      " -1.2136307 -1.221777 ]\n",
      "[-0.52776045 -1.3540683  -0.4177078  -0.11545622 -0.54134107  0.33389422\n",
      " -0.03999372 -0.07815816  0.47176585 -1.0085231  -0.28538436 -1.8284719\n",
      "  0.31436607  0.3176101  -0.447317   -0.01539969 -1.7737968  -1.0395633\n",
      " -0.04230124 -0.09205562 -1.0095183  -1.3070513  -0.13641775 -0.01144341\n",
      " -0.15482259 -0.04833603]\n",
      "Test dataset results: \n",
      "{'loss': 1.8779728456779763, 'crowds_classification_s_model_channel_matrix_13_loss': 1.8779728456779763, 'baseline_loss': 2.686349210915742, 'crowds_classification_s_model_channel_matrix_13_acc': 0.003703703703703704, 'baseline_acc': 0.8333333333333334}\n",
      "inputs:  [<tf.Tensor 'sequential_43/dropout_43/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_13/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_14/CrowdLayer:0' shape=(2, 2, 26) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.964492815512198, 'crowds_classification_s_model_channel_matrix_14_loss': 1.964492815512198, 'baseline_loss': 6.003401629130045, 'crowds_classification_s_model_channel_matrix_14_acc': 0.10370370381408267, 'baseline_acc': 0.4481481482585271}\n",
      "[-1.2240474 -1.220388  -1.2233236 -1.2212448 -1.2210826 -1.2254007\n",
      " -1.2240303 -1.2255814 -1.2255785 -1.2240349 -1.2194164 -1.2251365\n",
      " -1.2174225 -1.2228318 -1.2236248 -1.2192252 -1.216672  -1.2189465\n",
      " -1.2299571 -1.2166123 -1.2205992 -1.2148399 -1.2222056 -1.2236384\n",
      " -1.2202098 -1.2222602]\n",
      "[-1.1409558  -1.8622302   0.11297405 -0.03880447 -0.24117842 -0.33372113\n",
      " -0.81494397 -0.6601999  -0.18892744 -1.7279867  -0.76113975 -2.3207715\n",
      "  0.10346997 -0.0210706   0.13587284 -0.40078482 -2.2298121  -1.5567228\n",
      "  0.06068504  0.13652474 -1.0685005  -1.4518511   0.16919917 -0.19692948\n",
      "  0.17287344  0.08832288]\n",
      "Test dataset results: \n",
      "{'loss': 1.8893143830475985, 'crowds_classification_s_model_channel_matrix_14_loss': 1.8893143830475985, 'baseline_loss': 2.686349210915742, 'crowds_classification_s_model_channel_matrix_14_acc': 0.001851851851851852, 'baseline_acc': 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Evening Grosbeak/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-81138979b248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/yajingyang/Downloads/PetImages/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_annot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mN_ANNOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_annot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 1\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/PetImages/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    x_sample = x.copy()\n",
    "    x_sample['train'] = x_sample['train'][:round(x_sample['train'].shape[0]*clean_percent)]\n",
    "    y_gt_sample = y_gt.copy()\n",
    "    y_gt_sample['train'] = y_gt_sample['train'][:round(y_gt_sample['train'].shape[0]*clean_percent)]\n",
    "    y_annot_mix = y_annot.copy()\n",
    "    y_annot_mix['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)] \\\n",
    "        = np.reshape(np.repeat(y_gt['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)], N_ANNOT, axis=1)\n",
    "                     , (-1, N_CLASSES, N_ANNOT))\n",
    "    print(x_sample['train'].shape, y_gt_sample['train'].shape, y_annot_mix['train'].shape)\n",
    "\n",
    "    prefix = 'pct_%.2f_'%clean_percent\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        clean_base_acc_df = pd.DataFrame()\n",
    "        clean_base_loss_df = pd.DataFrame()\n",
    "        clean_history = baseline_gt(x_sample, y_gt_sample, N_CLASSES)\n",
    "        clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "        clean_base_loss_df.loc[:, i] = clean_history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
