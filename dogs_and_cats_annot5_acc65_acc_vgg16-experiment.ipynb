{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, merge, Reshape, Permute, Multiply, Dot,dot, Concatenate, Add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy, CrowdsClassificationSModel, \\\n",
    "    CrowdsClassificationCModelSingleWeight, CrowdsClassificationCModel, MaskedMultiCrossEntropyCosSim, \\\n",
    "    MaskedMultiCrossEntropyBaseChannel, MaskedMultiCrossEntropyBaseChannelConst, CrowdsClassificationSModelChannelMatrix, \\\n",
    "    MaskedMultiCrossEntropyCurriculumChannelMatrix\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = np.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets\n",
    "\n",
    "def get_data(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "    return x, y_gt, y_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_with_sample(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train_vgg16.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test_vgg16.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid_vgg16.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "\n",
    "    data_train_vgg16_sample = []\n",
    "    labels_train_sample = []\n",
    "    answers_mix = []\n",
    "    for pct in range(1,6):\n",
    "        data_train_vgg16_sample_dict = x.copy()\n",
    "        data_train_vgg16_sample_dict['train'] = load_data(DATA_PATH+'data_train_vgg16_%dpct.npy'%pct)\n",
    "        data_train_vgg16_sample.append(data_train_vgg16_sample_dict)\n",
    "        labels_train_sample.append(load_data(DATA_PATH+'labels_train_%dpct.npy'%pct))\n",
    "        answers_mix.append(load_data(DATA_PATH+'data_train_mix_%dpct.npy'%pct))\n",
    "    \n",
    "    labels_train_sample_bin_missings_list = []\n",
    "    answers_mix_bin_missings_list = []\n",
    "    for j in range(5):\n",
    "        labels_train_sample_bin = y_gt.copy()\n",
    "        labels_train_sample_bin['train'] = one_hot(labels_train_sample[j], N_CLASSES)\n",
    "        labels_train_sample_bin_missings_list.append(labels_train_sample_bin)\n",
    "        answers_mix_bin_missings = y_annot.copy()\n",
    "        answers_mix_bin_list = []\n",
    "        for i in range(len(answers)):\n",
    "            row = []\n",
    "            for r in range(N_ANNOT):\n",
    "                if answers_mix[j][i,r] == -1:\n",
    "                    row.append(-1 * np.ones(N_CLASSES))\n",
    "                else:\n",
    "                    row.append(one_hot(answers_mix[j][i,r], N_CLASSES)[0,:])\n",
    "            answers_mix_bin_list.append(row)\n",
    "        answers_mix_bin_missings['train'] = np.array(answers_mix_bin_list).swapaxes(1,2)\n",
    "        \n",
    "        answers_mix_bin_missings_list.append(answers_mix_bin_missings)\n",
    "        \n",
    "    x_sample = dict(zip(range(1,6), data_train_vgg16_sample))\n",
    "    y_sample = dict(zip(range(1,6), labels_train_sample_bin_missings_list))\n",
    "    y_annot_mix = dict(zip(range(1,6), answers_mix_bin_missings_list))\n",
    "    \n",
    "    return x, y_gt, y_annot, x_sample, y_sample, y_annot_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def eval(model,x_test, y_test):\n",
    "    print('Test dataset results: ')\n",
    "    mets = dict(zip(model.metrics_names,model.evaluate(x_test,y_test, verbose=False)))\n",
    "    print(mets)\n",
    "    return mets\n",
    "\n",
    "\n",
    "def get_trace(model):\n",
    "\n",
    "    channel_matrix = model.get_weights()[-1]\n",
    "    channel_matrix_trace = tf.trace(K.permute_dimensions(channel_matrix, [2, 0, 1]))\n",
    "    channel_matrix_trace_arr = K.eval(channel_matrix_trace)\n",
    "    return channel_matrix_trace_arr\n",
    "\n",
    "\n",
    "def print_single_loss(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['baseline_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['baseline_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_history(df, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Make a data frame\n",
    "    df['x'] = range(df.shape[0])\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    # multiple line plot\n",
    "    num = 0\n",
    "    for column in df.drop('x', axis=1):\n",
    "        num += 1\n",
    "        plt.plot(df['x'], df[column], marker='', color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=2)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(train_data_shape, N_CLASSES):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "#     base_model.add(Dense(64, activation='relu'))\n",
    "#     base_model.add(Dropout(0.4))\n",
    "\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation('softmax'))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(x, y_gt, y_annot, N_CLASSES, model_path):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "    checkpoint = ModelCheckpoint(model_path, verbose=1, monitor='val_acc',\n",
    "                                 save_best_only=True, mode='auto')  \n",
    "    callbacks = [EarlyStopping(monitor='val_acc', patience=5)]\n",
    "\n",
    "    history = baseline_model.fit(x['train'], y_annot['mv'], validation_data=(x['val'], y_gt['val']), \n",
    "                        epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "    mets = eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history, mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_gt(x, y_gt, N_CLASSES, model_path):\n",
    "    train_data_shape = x['train'].shape\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "    checkpoint = ModelCheckpoint(model_path, verbose=1, monitor='val_acc',\n",
    "                                 save_best_only=True, mode='auto')  \n",
    "#     callbacks = [EarlyStopping(monitor='val_acc', patience=5)]\n",
    "\n",
    "    history = baseline_model.fit(x['train'], y_gt['train'], validation_data=(x['val'], y_gt['val']), \n",
    "                        epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "    mets = eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history, mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "#     base_model.add(Dense(64, activation='relu'))\n",
    "#     base_model.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT, name='CrowdLayer')\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_path):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_path, verbose=1, \n",
    "                                 monitor='val_baseline_acc',save_best_only=True, mode='auto')  \n",
    "#     callbacks = [EarlyStopping(monitor='val_baseline_acc', patience=5)]\n",
    "\n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], \n",
    "                        validation_data=(x['val'],[y_annot['val'], y_gt['val']]), \n",
    "                        epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    trace_arr = get_trace(model)\n",
    "    weights = K.squeeze(weights)\n",
    "    \n",
    "    weights =  K.permute_dimensions(model.layers[-1].get_weights(), [2, 0, 1])\n",
    "\n",
    "#     weights = K.permute_dimensions(model.get_weights()[-1], [2, 0, 1])\n",
    "    mets = eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr, mets, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample, y_gt_sample, N_CLASSES, softmax, trace, model_path):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "#     base_model.add(Dense(64, activation='relu'))\n",
    "#     base_model.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    baseline_model = Model(inputs=train_inputs, outputs=baseline_output)\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    baseline_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    \n",
    "    history = baseline_model.fit(x_sample['train'], y_gt_sample['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "    \n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    model.set_weights(baseline_model.get_weights()) \n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_path, verbose=1, \n",
    "                                 monitor='val_baseline_acc',save_best_only=True, mode='auto')  \n",
    "#     callbacks = [EarlyStopping(monitor='val_baseline_acc', patience=5)]\n",
    "\n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], \n",
    "                        validation_data=(x['val'],[y_annot['val'], y_gt['val']]), \n",
    "                        epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    trace_arr = get_trace(model)\n",
    "    mets = eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr, mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(12499, 4, 4, 512)\n",
      "(12499,)\n",
      "(12499,)\n",
      "\n",
      "Loading AMT data...\n",
      "(12499, 5)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 5\n",
      "\n",
      "Loading test data...\n",
      "(6250, 4, 4, 512)\n",
      "(6250,)\n",
      "\n",
      "Loading validation data...\n",
      "(6249, 4, 4, 512)\n",
      "(6249,)\n"
     ]
    }
   ],
   "source": [
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "N_RUNS = 1\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/PetImages/annot_5_acc_0.65/\"\n",
    "x, y_gt, y_annot, x_sample, y_sample, y_annot_mix = get_data_with_sample(DATA_PATH, N_CLASSES)\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "model_dir = \"/home/yajingyang/PycharmProjects/CrowdLayer/dogs_and_cats/0.2/\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "acc_mean = {}\n",
    "acc_std = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crowd noise adaptation model with softmax: True, trace: False\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 9s 730us/step - loss: 0.7113 - crowds_classification_s_model_channel_matrix_14_loss: 0.7113 - baseline_loss: 2.1519 - crowds_classification_s_model_channel_matrix_14_acc: 0.2697 - baseline_acc: 0.8571 - val_loss: 0.6163 - val_crowds_classification_s_model_channel_matrix_14_loss: 0.6163 - val_baseline_loss: 1.1805 - val_crowds_classification_s_model_channel_matrix_14_acc: 0.3444 - val_baseline_acc: 0.9205\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 255us/step - loss: 0.6950 - crowds_classification_s_model_channel_matrix_14_loss: 0.6950 - baseline_loss: 2.0210 - crowds_classification_s_model_channel_matrix_14_acc: 0.2455 - baseline_acc: 0.8708 - val_loss: 0.6114 - val_crowds_classification_s_model_channel_matrix_14_loss: 0.6114 - val_baseline_loss: 2.0089 - val_crowds_classification_s_model_channel_matrix_14_acc: 0.0871 - val_baseline_acc: 0.8720\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.6095808312988281, 'crowds_classification_s_model_channel_matrix_14_loss': 0.6095808312988281, 'baseline_loss': 1.8743485459899902, 'crowds_classification_s_model_channel_matrix_14_acc': 0.08848000000953675, 'baseline_acc': 0.8800000000190735}\n",
      "weight:  [[[0.48603305 0.17427188]\n",
      "  [0.18650627 0.47510856]]\n",
      "\n",
      " [[0.4944124  0.17922316]\n",
      "  [0.18784894 0.5043759 ]]\n",
      "\n",
      " [[0.4738534  0.17433158]\n",
      "  [0.18749894 0.4849059 ]]\n",
      "\n",
      " [[0.4690315  0.16201812]\n",
      "  [0.16568421 0.48169658]]\n",
      "\n",
      " [[0.49455354 0.166303  ]\n",
      "  [0.16715573 0.46826968]]]\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 9s 738us/step - loss: 0.7160 - crowds_classification_s_model_channel_matrix_15_loss: 0.7160 - baseline_loss: 3.4592 - crowds_classification_s_model_channel_matrix_15_acc: 0.2031 - baseline_acc: 0.7757 - val_loss: 0.6291 - val_crowds_classification_s_model_channel_matrix_15_loss: 0.6291 - val_baseline_loss: 1.0948 - val_crowds_classification_s_model_channel_matrix_15_acc: 0.1906 - val_baseline_acc: 0.9273\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 259us/step - loss: 0.7017 - crowds_classification_s_model_channel_matrix_15_loss: 0.7017 - baseline_loss: 2.8692 - crowds_classification_s_model_channel_matrix_15_acc: 0.2325 - baseline_acc: 0.8186 - val_loss: 0.6187 - val_crowds_classification_s_model_channel_matrix_15_loss: 0.6187 - val_baseline_loss: 1.7554 - val_crowds_classification_s_model_channel_matrix_15_acc: 0.4378 - val_baseline_acc: 0.8875\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.6168645944404602, 'crowds_classification_s_model_channel_matrix_15_loss': 0.6168645944404602, 'baseline_loss': 1.7190243700408936, 'crowds_classification_s_model_channel_matrix_15_acc': 0.4309599999809265, 'baseline_acc': 0.890079999961853}\n",
      "weight:  [[[0.42642197 0.1950628 ]\n",
      "  [0.21426007 0.5239863 ]]\n",
      "\n",
      " [[0.42535707 0.17925943]\n",
      "  [0.21234404 0.52717036]]\n",
      "\n",
      " [[0.41418472 0.1904899 ]\n",
      "  [0.20082483 0.5167587 ]]\n",
      "\n",
      " [[0.41415522 0.18232939]\n",
      "  [0.19835657 0.5190849 ]]\n",
      "\n",
      " [[0.41914523 0.20768104]\n",
      "  [0.19933282 0.52477205]]]\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 9s 749us/step - loss: 0.7285 - crowds_classification_s_model_channel_matrix_16_loss: 0.7285 - baseline_loss: 8.0614 - crowds_classification_s_model_channel_matrix_16_acc: 0.1421 - baseline_acc: 0.4986 - val_loss: 0.7192 - val_crowds_classification_s_model_channel_matrix_16_loss: 0.7192 - val_baseline_loss: 7.9494 - val_crowds_classification_s_model_channel_matrix_16_acc: 0.1040 - val_baseline_acc: 0.5068\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 255us/step - loss: 0.7141 - crowds_classification_s_model_channel_matrix_16_loss: 0.7141 - baseline_loss: 8.0868 - crowds_classification_s_model_channel_matrix_16_acc: 0.1804 - baseline_acc: 0.4983 - val_loss: 0.7092 - val_crowds_classification_s_model_channel_matrix_16_loss: 0.7092 - val_baseline_loss: 7.9494 - val_crowds_classification_s_model_channel_matrix_16_acc: 0.2459 - val_baseline_acc: 0.5068\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.7093500501441956, 'crowds_classification_s_model_channel_matrix_16_loss': 0.7093500501441956, 'baseline_loss': 8.108046794281005, 'crowds_classification_s_model_channel_matrix_16_acc': 0.2548000000047684, 'baseline_acc': 0.4969600000190735}\n",
      "weight:  [[[0.33189136 0.39738488]\n",
      "  [0.2801102  0.5970106 ]]\n",
      "\n",
      " [[0.32256287 0.39945295]\n",
      "  [0.27671298 0.5942875 ]]\n",
      "\n",
      " [[0.33485013 0.4041288 ]\n",
      "  [0.3021253  0.6014122 ]]\n",
      "\n",
      " [[0.32787606 0.4103328 ]\n",
      "  [0.27950776 0.58612394]]\n",
      "\n",
      " [[0.3155562  0.40561286]\n",
      "  [0.26036    0.5933813 ]]]\n"
     ]
    }
   ],
   "source": [
    "softmax=True\n",
    "trace = False\n",
    "\n",
    "print('\\nCrowd noise adaptation model with softmax: %s, trace: %s' % (softmax, trace))\n",
    "test_acc_list = []\n",
    "acc_dict = {}\n",
    "loss_dict = {}\n",
    "trace_dict = {}\n",
    "for i in range(3):\n",
    "    history, trace_arr, mets, weights = crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_dir)\n",
    "    acc_dict[i] = history.history['baseline_acc']\n",
    "    loss_dict[i] = history.history['baseline_loss']\n",
    "    trace_dict[i] = trace_arr\n",
    "    print('weight: ', K.eval(weights))\n",
    "    test_acc_list.append(mets['baseline_acc'])\n",
    "\n",
    "# crowd_model_acc_list.append(acc_dict)\n",
    "# crowd_model_loss_list.append(loss_dict)\n",
    "# crowd_model_trace_list.append(trace_dict)\n",
    "# test_acc = np.array(test_acc_list)\n",
    "# print('acc: ', test_acc.mean(), test_acc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_identities(shape, dtype=None):\n",
    "    out = np.zeros(shape)\n",
    "    for r in range(shape[2]):\n",
    "        for i in range(shape[0]):\n",
    "            out[i,i,r] = 1.0\n",
    "    return out\n",
    "\n",
    "\n",
    "class CrowdsClassification(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, num_annotators, conn_type=\"MW\", **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_annotators = num_annotators\n",
    "        self.conn_type = conn_type\n",
    "        super(CrowdsClassification, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.conn_type == \"MW\":\n",
    "            # matrix of weights per annotator\n",
    "            self.kernel = self.add_weight(\"CrowdLayer\", (self.output_dim, self.output_dim, self.num_annotators),\n",
    "                                            initializer=init_identities,\n",
    "                                            trainable=True)\n",
    "        elif self.conn_type == \"VW\":\n",
    "            # vector of weights (one scale per class) per annotator\n",
    "            self.kernel = self.add_weight(\"CrowdLayer\", (self.output_dim, self.num_annotators),\n",
    "                                            initializer=keras.initializers.Ones(),\n",
    "                                            trainable=True)\n",
    "        elif self.conn_type == \"VB\":\n",
    "            # two vectors of weights (one scale and one bias per class) per annotator\n",
    "            self.kernel = []\n",
    "            self.kernel.append(self.add_weight(\"CrowdLayer\", (self.output_dim, self.num_annotators),\n",
    "                                            initializer=keras.initializers.Zeros(),\n",
    "                                            trainable=True))\n",
    "        elif self.conn_type == \"VW+B\":\n",
    "            # two vectors of weights (one scale and one bias per class) per annotator\n",
    "            self.kernel = []\n",
    "            self.kernel.append(self.add_weight(\"CrowdLayer\", (self.output_dim, self.num_annotators),\n",
    "                                            initializer=keras.initializers.Ones(),\n",
    "                                            trainable=True))\n",
    "            self.kernel.append(self.add_weight(\"CrowdLayer\", (self.output_dim, self.num_annotators),\n",
    "                                            initializer=keras.initializers.Zeros(),\n",
    "                                            trainable=True))\n",
    "        elif self.conn_type == \"SW\":\n",
    "            # single weight value per annotator\n",
    "            self.kernel = self.add_weight(\"CrowdLayer\", (self.num_annotators,1),\n",
    "                                            initializer=keras.initializers.Ones(),\n",
    "                                            trainable=True)\n",
    "        else:\n",
    "            raise Exception(\"Unknown connection type for CrowdsClassification layer!\")\n",
    "\n",
    "        super(CrowdsClassification, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        if self.conn_type == \"MW\":\n",
    "            res = K.dot(x, self.kernel)\n",
    "        elif self.conn_type == \"VW\" or self.conn_type == \"VB\" or self.conn_type == \"VW+B\" or self.conn_type == \"SW\":\n",
    "            out = []\n",
    "            for r in range(self.num_annotators):\n",
    "                if self.conn_type == \"VW\":\n",
    "                    out.append(x * self.kernel[:,r])\n",
    "                elif self.conn_type == \"VB\":\n",
    "                    out.append(x + self.kernel[0][:,r])\n",
    "                elif self.conn_type == \"VW+B\":\n",
    "                    out.append(x * self.kernel[0][:,r] + self.kernel[1][:,r])\n",
    "                elif self.conn_type == \"SW\":\n",
    "                    out.append(x * self.kernel[r,0])\n",
    "            res = tf.stack(out)\n",
    "            if len(res.shape) == 3:\n",
    "                res = tf.transpose(res, [1, 2, 0])\n",
    "            elif len(res.shape) == 4:\n",
    "                res = tf.transpose(res, [1, 2, 3, 0])\n",
    "            else:\n",
    "                raise Exception(\"Wrong number of dimensions for output\")\n",
    "        else:\n",
    "            raise Exception(\"Unknown connection type for CrowdsClassification layer!\")\n",
    "\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim, self.num_annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_identities(shape, dtype=None):\n",
    "    out = np.zeros(shape)\n",
    "    for r in range(shape[2]):\n",
    "        for i in range(shape[0]):\n",
    "            out[i,i,r] = 1.0\n",
    "    return out\n",
    "\n",
    "\n",
    "def bias_weights(N_CLASSES, APRIOR_NOISE=0.4):\n",
    "    log_bias_weights = (\n",
    "        np.array([np.array([(1. - APRIOR_NOISE)\n",
    "                            if i == j else\n",
    "                            APRIOR_NOISE / (N_CLASSES - 1.)\n",
    "                            for j in range(N_CLASSES)]) for i in\n",
    "                  range(N_CLASSES)])\n",
    "        + 0.01 * np.random.random((N_CLASSES, N_CLASSES)))\n",
    "    return log_bias_weights\n",
    "\n",
    "\n",
    "def init_bias(shape, dtype=None):\n",
    "    out = np.zeros(shape)\n",
    "    N_CLASSES = shape[0]\n",
    "    for r in range(shape[2]):\n",
    "        out[:,:,r] = log_bias_weights(N_CLASSES)\n",
    "    return out\n",
    "\n",
    "\n",
    "class CrowdsClassificationSModelChannelMatrix(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, num_annotators, conn_type=\"MW\", softmax=False, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_annotators = num_annotators\n",
    "        self.conn_type = conn_type\n",
    "        self.softmax = softmax\n",
    "        super(CrowdsClassificationSModelChannelMatrix, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.conn_type == \"MW\":\n",
    "            # matrix of weights per annotator\n",
    "            self.kernel = []\n",
    "            self.kernel.append(self.add_weight(\"CrowdLayer\", (self.output_dim, self.output_dim, self.num_annotators),\n",
    "                                initializer=init_log_bias,\n",
    "                                trainable=True))\n",
    "        else:\n",
    "            raise Exception(\"Unknown connection type for CrowdsClassification layer!\")\n",
    "\n",
    "        super(CrowdsClassificationSModelChannelMatrix, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.conn_type == \"MW\":\n",
    "            channel_output_l = []\n",
    "            channel_matrix_l = []\n",
    "            for r in range(self.num_annotators):\n",
    "                channel_matrix_w = self.kernel[0][:,:,r]\n",
    "                channel_matrix_w_l = []\n",
    "                for c in range(self.output_dim):\n",
    "                    if self.softmax:\n",
    "                        channel_matrix_w_c = K.softmax(channel_matrix_w[:,c])\n",
    "                    else:\n",
    "                        channel_matrix_w_c = channel_matrix_w[:,c]\n",
    "                    channel_matrix_w_l.append(channel_matrix_w_c)\n",
    "                channel_matrix_w = tf.stack(channel_matrix_w_l)\n",
    "                channel_matrix_l.append(channel_matrix_w)\n",
    "                channel_output_w = K.dot(inputs[1], channel_matrix_w)\n",
    "                channel_output_w = K.dropout(channel_output_w, 0.4)\n",
    "                channel_output_l.append(channel_output_w)\n",
    "            channel_matrix = tf.stack(channel_matrix_l)\n",
    "            channel_output = tf.stack(channel_output_l)\n",
    "            channel_output = K.permute_dimensions(channel_output, (1,2,0))\n",
    "            self.channel_matrix = channel_matrix\n",
    "\n",
    "#             res = K.batch_dot(inputs[1], channel_matrix)\n",
    "        else:\n",
    "            raise Exception(\"Unknown connection type for CrowdsClassification layer!\")\n",
    "\n",
    "        return channel_output\n",
    "\n",
    "    def get_channel_matrix(self):\n",
    "        return self.channel_matrix\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[1][0], self.output_dim, self.num_annotators)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crowd noise adaptation model with softmax: True, trace: False\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 10s 802us/step - loss: 0.7255 - crowds_classification_s_model_channel_matrix_20_loss: 0.7255 - baseline_loss: 2.7766 - crowds_classification_s_model_channel_matrix_20_acc: 0.1757 - baseline_acc: 0.8204 - val_loss: 0.6625 - val_crowds_classification_s_model_channel_matrix_20_loss: 0.6625 - val_baseline_loss: 1.4133 - val_crowds_classification_s_model_channel_matrix_20_acc: 0.1503 - val_baseline_acc: 0.9109\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 263us/step - loss: 0.7174 - crowds_classification_s_model_channel_matrix_20_loss: 0.7174 - baseline_loss: 2.1473 - crowds_classification_s_model_channel_matrix_20_acc: 0.1835 - baseline_acc: 0.8639 - val_loss: 0.6539 - val_crowds_classification_s_model_channel_matrix_20_loss: 0.6539 - val_baseline_loss: 2.0513 - val_crowds_classification_s_model_channel_matrix_20_acc: 0.1535 - val_baseline_acc: 0.8696\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.6495717818832397, 'crowds_classification_s_model_channel_matrix_20_loss': 0.6495717818832397, 'baseline_loss': 2.0526993731021883, 'crowds_classification_s_model_channel_matrix_20_acc': 0.15424000001907348, 'baseline_acc': 0.870719999961853}\n",
      "weight:  [[[0.7512073  0.25437877]\n",
      "  [0.25861698 0.75383484]]\n",
      "\n",
      " [[0.7586201  0.24349381]\n",
      "  [0.25106055 0.7640827 ]]\n",
      "\n",
      " [[0.74080896 0.26134732]\n",
      "  [0.2665666  0.7565239 ]]\n",
      "\n",
      " [[0.7584698  0.25372905]\n",
      "  [0.25788847 0.7484834 ]]\n",
      "\n",
      " [[0.7725573  0.25658098]\n",
      "  [0.23345797 0.750489  ]]]\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 10s 833us/step - loss: 0.7264 - crowds_classification_s_model_channel_matrix_21_loss: 0.7264 - baseline_loss: 2.9269 - crowds_classification_s_model_channel_matrix_21_acc: 0.1988 - baseline_acc: 0.8105 - val_loss: 0.6726 - val_crowds_classification_s_model_channel_matrix_21_loss: 0.6726 - val_baseline_loss: 2.2494 - val_crowds_classification_s_model_channel_matrix_21_acc: 0.1848 - val_baseline_acc: 0.8566\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 264us/step - loss: 0.7176 - crowds_classification_s_model_channel_matrix_21_loss: 0.7176 - baseline_loss: 2.2053 - crowds_classification_s_model_channel_matrix_21_acc: 0.2341 - baseline_acc: 0.8601 - val_loss: 0.6479 - val_crowds_classification_s_model_channel_matrix_21_loss: 0.6479 - val_baseline_loss: 1.7466 - val_crowds_classification_s_model_channel_matrix_21_acc: 0.2476 - val_baseline_acc: 0.8894\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.6480037514877319, 'crowds_classification_s_model_channel_matrix_21_loss': 0.6480037514877319, 'baseline_loss': 1.6445188279855252, 'crowds_classification_s_model_channel_matrix_21_acc': 0.24424000000476837, 'baseline_acc': 0.8953600000190735}\n",
      "weight:  [[[0.75698406 0.2652239 ]\n",
      "  [0.26006413 0.7367745 ]]\n",
      "\n",
      " [[0.7620014  0.23735794]\n",
      "  [0.2448586  0.7761117 ]]\n",
      "\n",
      " [[0.75549096 0.25222751]\n",
      "  [0.25968683 0.7483952 ]]\n",
      "\n",
      " [[0.7608974  0.25536606]\n",
      "  [0.24923977 0.74819654]]\n",
      "\n",
      " [[0.76923394 0.26427078]\n",
      "  [0.23993243 0.7478684 ]]]\n",
      "Train on 12499 samples, validate on 6249 samples\n",
      "Epoch 1/2\n",
      "12499/12499 [==============================] - 11s 878us/step - loss: 0.7232 - crowds_classification_s_model_channel_matrix_22_loss: 0.7232 - baseline_loss: 2.7303 - crowds_classification_s_model_channel_matrix_22_acc: 0.1903 - baseline_acc: 0.8230 - val_loss: 0.6662 - val_crowds_classification_s_model_channel_matrix_22_loss: 0.6662 - val_baseline_loss: 1.4955 - val_crowds_classification_s_model_channel_matrix_22_acc: 0.1947 - val_baseline_acc: 0.9038\n",
      "Epoch 2/2\n",
      "12499/12499 [==============================] - 3s 275us/step - loss: 0.7175 - crowds_classification_s_model_channel_matrix_22_loss: 0.7175 - baseline_loss: 2.2842 - crowds_classification_s_model_channel_matrix_22_acc: 0.2175 - baseline_acc: 0.8547 - val_loss: 0.6602 - val_crowds_classification_s_model_channel_matrix_22_loss: 0.6602 - val_baseline_loss: 2.7782 - val_crowds_classification_s_model_channel_matrix_22_acc: 0.1727 - val_baseline_acc: 0.8245\n",
      "4\n",
      "(8192, 128) (128,) (128, 2) (2,) (2, 2, 5)\n",
      "Test dataset results: \n",
      "{'loss': 0.663395613079071, 'crowds_classification_s_model_channel_matrix_22_loss': 0.663395613079071, 'baseline_loss': 2.809740979232788, 'crowds_classification_s_model_channel_matrix_22_acc': 0.1754400000023842, 'baseline_acc': 0.823199999961853}\n",
      "weight:  [[[0.7743074  0.26572385]\n",
      "  [0.23933642 0.7464947 ]]\n",
      "\n",
      " [[0.7746711  0.23337843]\n",
      "  [0.23272678 0.77905124]]\n",
      "\n",
      " [[0.7535084  0.25124314]\n",
      "  [0.25485688 0.75727594]]\n",
      "\n",
      " [[0.75452787 0.25848463]\n",
      "  [0.24720445 0.75465715]]\n",
      "\n",
      " [[0.78025    0.25773144]\n",
      "  [0.230749   0.75575954]]]\n"
     ]
    }
   ],
   "source": [
    "class MaskedMultiCrossEntropyCurriculumChannelMatrix(object):\n",
    "\n",
    "    def __init__(self, model, a, b):\n",
    "        self.t = tf.transpose(model.get_weights()[-1], perm=[2, 0, 1])\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "\n",
    "        vec = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1)\n",
    "        # vec_base_channel = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=self.y_pred_broad, dim=1)\n",
    "        trace = tf.trace(self.t)\n",
    "        vec = vec - trace * self.b\n",
    "        mask = tf.equal(y_true[:,0,:], -1)\n",
    "        zer = tf.zeros_like(vec)\n",
    "        loss = tf.where(mask, x=zer, y=vec)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "#     base_model.add(Dense(64, activation='relu'))\n",
    "#     base_model.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT, softmax=True)\n",
    "    channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "#     else:\n",
    "#         channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "#         channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_path):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_path, verbose=1, \n",
    "                                 monitor='val_baseline_acc',save_best_only=True, mode='auto')  \n",
    "#     callbacks = [EarlyStopping(monitor='val_baseline_acc', patience=5)]\n",
    "\n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], \n",
    "                        validation_data=(x['val'],[y_annot['val'], y_gt['val']]), \n",
    "                        epochs=2, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    trace_arr = get_trace(model)   \n",
    "    print(len(model.layers))\n",
    "    print(model.get_weights()[0].shape, model.get_weights()[1].shape, \n",
    "          model.get_weights()[2].shape, model.get_weights()[3].shape, model.get_weights()[4].shape)\n",
    "    weights =  K.permute_dimensions(tf.squeeze(model.layers[-1].get_weights()[0]), [2, 0, 1])\n",
    "\n",
    "#     weights = K.permute_dimensions(model.get_weights()[-1], [2, 0, 1])\n",
    "    mets = eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr, mets, weights\n",
    "\n",
    "\n",
    "softmax=True\n",
    "trace = False\n",
    "\n",
    "print('\\nCrowd noise adaptation model with softmax: %s, trace: %s' % (softmax, trace))\n",
    "test_acc_list = []\n",
    "acc_dict = {}\n",
    "loss_dict = {}\n",
    "trace_dict = {}\n",
    "for i in range(3):\n",
    "    history, trace_arr, mets, weights = crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_dir)\n",
    "    acc_dict[i] = history.history['baseline_acc']\n",
    "    loss_dict[i] = history.history['baseline_loss']\n",
    "    trace_dict[i] = trace_arr\n",
    "    print('weight: ', K.eval(weights))\n",
    "    test_acc_list.append(mets['baseline_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline with clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\nBaseline model with clean data')\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(N_RUNS):\n",
    "    clean_base_acc_df = pd.DataFrame()\n",
    "    clean_base_loss_df = pd.DataFrame()\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "    model_path = model_dir + filepath\n",
    "    clean_history, mets = baseline_gt(x, y_gt, N_CLASSES, model_path)\n",
    "    clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "    clean_base_loss_df.loc[:, i] = clean_history.history['loss']\n",
    "    test_acc = mets['acc']\n",
    "    test_acc_list.append(test_acc)\n",
    "test_acc = np.array(test_acc_list)\n",
    "\n",
    "acc_mean['clean_base'] = test_acc.mean()\n",
    "acc_std['clean_base'] = test_acc.std()\n",
    "print(acc_mean['clean_base'], acc_std['clean_base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\nBaseline model with majority vote')\n",
    "test_acc_list = []\n",
    "for i in range(N_RUNS):\n",
    "    mv_acc_df = pd.DataFrame()\n",
    "    mv_loss_df = pd.DataFrame()\n",
    "    mv_history, mets = majority_vote(x, y_gt, y_annot, N_CLASSES, model_dir)\n",
    "    mv_acc_df = mv_history.history['acc']\n",
    "    mv_loss_df = mv_history.history['loss']\n",
    "    test_acc = mets['acc']\n",
    "    test_acc_list.append(test_acc)\n",
    "test_acc = np.array(test_acc_list)\n",
    "\n",
    "acc_mean['mv'] = test_acc.mean()\n",
    "acc_std['mv'] = test_acc.std()\n",
    "print(acc_mean['mv'], acc_std['mv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crowd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crowd_model_acc_list = []\n",
    "crowd_model_loss_list = []\n",
    "crowd_model_trace_list = []\n",
    "test_acc_lists = []\n",
    "model_name = ['base_crowd', 'trace_crowd', 'softmax_crowd', 'softmax_trace_crowd']\n",
    "m = 0\n",
    "\n",
    "for softmax in [False, True]:\n",
    "    for trace in [False, True]:\n",
    "        print('\\nCrowd noise adaptation model with softmax: %s, trace: %s' % (softmax, trace))\n",
    "        test_acc_list = []\n",
    "        acc_dict = {}\n",
    "        loss_dict = {}\n",
    "        trace_dict = {}\n",
    "        for i in range(N_RUNS):\n",
    "            history, trace_arr, mets = crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_dir)\n",
    "            acc_dict[i] = history.history['baseline_acc']\n",
    "            loss_dict[i] = history.history['baseline_loss']\n",
    "            trace_dict[i] = trace_arr\n",
    "            test_acc_list.append(mets['baseline_acc'])\n",
    "            \n",
    "        crowd_model_acc_list.append(acc_dict)\n",
    "        crowd_model_loss_list.append(loss_dict)\n",
    "        crowd_model_trace_list.append(trace_dict)\n",
    "        test_acc = np.array(test_acc_list)\n",
    "        acc_mean[model_name[m]] = test_acc.mean()\n",
    "        acc_std[model_name[m]] = test_acc.std()\n",
    "        m+=1\n",
    "        print(model_name[m], acc_mean[model_name[m]], acc_std[model_name[m]])\n",
    "\n",
    "for j in range(4):\n",
    "    print(model_name[j], acc_mean[model_name[j]], acc_std[model_name[j]])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.8575795260867937, 'acc': 0.9398399999618531}\n",
      "\n",
      "Crowd noise adaptation model with 0.01 clean data\n",
      "WARNING:tensorflow:From /home/yajingyang/PycharmProjects/CrowdLayer/crowd_layer/crowd_layers.py:276: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Test dataset results: \n",
      "{'loss': -1.306828787727356, 'crowds_classification_1_loss': -1.306828787727356, 'baseline_loss': 8.010048790893554, 'crowds_classification_1_acc': 0.0, 'baseline_acc': 0.5030400000047683}\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': -1.5115323276138306, 'crowds_classification_2_loss': -1.5115323276138306, 'baseline_loss': 0.8598640452689119, 'crowds_classification_2_acc': 0.0, 'baseline_acc': 0.94656}\n",
      "\n",
      "Baseline model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.9977716064417235, 'acc': 0.936}\n",
      "\n",
      "Crowd noise adaptation model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': -1.3068443245697021, 'crowds_classification_3_loss': -1.3068443245697021, 'baseline_loss': 8.108046794281005, 'crowds_classification_3_acc': 0.0, 'baseline_acc': 0.4969600000190735}\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.02 clean data\n"
     ]
    }
   ],
   "source": [
    "sample_base_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "sample_base_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "mix_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "mix_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "mix_trace_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "pretrain_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "pretrain_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "pretrain_trace_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        clean_history, mets = baseline_gt(x_sample[int(clean_percent*100)], y_sample[int(clean_percent*100)], N_CLASSES, model_dir)\n",
    "        sample_base_acc_dict[clean_percent] = clean_history.history['acc']\n",
    "        sample_base_loss_dict[clean_percent] = clean_history.history['loss']\n",
    "        test_acc_list.append(mets['acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'sample_base_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "#     print(clean_percent, acc_mean[model_desc], acc_std[model_desc])\n",
    "            \n",
    "    print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        history, trace_arr, mets = crowd_model(x, y_gt, y_annot_mix[int(clean_percent*100)], N_CLASSES, False, True, model_dir)\n",
    "        mix_acc_dict[clean_percent] = history.history['baseline_acc']\n",
    "        mix_loss_dict[clean_percent] = history.history['baseline_loss']\n",
    "        mix_trace_dict[clean_percent] = trace_arr\n",
    "        test_acc_list.append(mets['baseline_acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'mix_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "#     print(clean_percent, acc_mean[model_desc], acc_std[model_desc])\n",
    "    \n",
    "    print('\\nCrowd noise adaptation model pretrain with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        history, trace_arr, mets = crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample[int(clean_percent*100)], y_sample[int(clean_percent*100)], N_CLASSES, False, True, model_dir)\n",
    "        pretrain_trace_dict[clean_percent] = history.history['baseline_acc']\n",
    "        pretrain_trace_dict[clean_percent] = history.history['baseline_loss'] \n",
    "        pretrain_trace_dict[clean_percent] = trace_arr\n",
    "        test_acc_list.append(mets['baseline_acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'pretrain_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "#     print(clean_percent, acc_mean[model_desc], acc_std[model_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(10000, 4, 4, 512)\n",
      "(10000,)\n",
      "(10000,)\n",
      "\n",
      "Loading AMT data...\n",
      "(10000, 59)\n",
      "N_CLASSES: 8\n",
      "N_ANNOT: 59\n",
      "\n",
      "Loading test data...\n",
      "(1188, 4, 4, 512)\n",
      "(1188,)\n",
      "\n",
      "Loading validation data...\n",
      "(500, 4, 4, 512)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "N_CLASSES = 8\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "N_RUNS = 30\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/LabelMe/prepared/with_sample/\"\n",
    "x, y_gt, y_annot, x_sample, y_sample, y_annot_mix = get_data_with_sample(DATA_PATH, N_CLASSES)\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "model_dir = \"/home/yajingyang/PycharmProjects/CrowdLayer/dogs_and_cats/0.2/\"\n",
    "# if not os.path.isdir(model_dir):\n",
    "#     os.mkdir(model_dir)\n",
    "    \n",
    "acc_mean = {}\n",
    "acc_std = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crowd noise adaptation model with softmax: False, trace: False\n"
     ]
    }
   ],
   "source": [
    "crowd_model_acc_list = []\n",
    "crowd_model_loss_list = []\n",
    "crowd_model_trace_list = []\n",
    "test_acc_lists = []\n",
    "model_name = ['base_crowd', 'trace_crowd', 'softmax_crowd', 'softmax_trace_crowd']\n",
    "m = 0\n",
    "\n",
    "for softmax in [False, True]:\n",
    "    for trace in [False, True]:\n",
    "        print('\\nCrowd noise adaptation model with softmax: %s, trace: %s' % (softmax, trace))\n",
    "        test_acc_list = []\n",
    "        acc_dict = {}\n",
    "        loss_dict = {}\n",
    "        trace_dict = {}\n",
    "        for i in range(N_RUNS):\n",
    "            history, trace_arr, mets = crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace, model_dir)\n",
    "            acc_dict[i] = history.history['baseline_acc']\n",
    "            loss_dict[i] = history.history['baseline_loss']\n",
    "            trace_dict[i] = trace_arr\n",
    "            test_acc_list.append(mets['baseline_acc'])\n",
    "            \n",
    "        crowd_model_acc_list.append(acc_dict)\n",
    "        crowd_model_loss_list.append(loss_dict)\n",
    "        crowd_model_trace_list.append(trace_dict)\n",
    "        test_acc = np.array(test_acc_list)\n",
    "        acc_mean[model_name[m]] = test_acc.mean()\n",
    "        acc_std[model_name[m]] = test_acc.std()\n",
    "        m+=1\n",
    "for j in range(4):\n",
    "    print(model_name[j], acc_mean[model_name[j]], acc_std[model_name[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': 2.1243054886859674, 'acc': 0.45791245791245794}\n",
      "Test dataset results: \n",
      "{'loss': 1.801091168464635, 'acc': 0.45875420875420875}\n",
      "Test dataset results: \n",
      "{'loss': 1.6508558948433358, 'acc': 0.44191919191919193}\n",
      "Test dataset results: \n",
      "{'loss': 2.1716030221996885, 'acc': 0.398989898989899}\n",
      "Test dataset results: \n",
      "{'loss': 1.8531769582318136, 'acc': 0.40404040404040403}\n",
      "Test dataset results: \n",
      "{'loss': 2.0309800692278928, 'acc': 0.414983164983165}\n",
      "Test dataset results: \n",
      "{'loss': 2.620486430447511, 'acc': 0.41919191919191917}\n",
      "Test dataset results: \n",
      "{'loss': 1.9096883033662533, 'acc': 0.4074074074074074}\n",
      "Test dataset results: \n",
      "{'loss': 2.0237568669046215, 'acc': 0.4377104377104377}\n",
      "Test dataset results: \n",
      "{'loss': 1.9927224486765236, 'acc': 0.3813131313131313}\n",
      "Test dataset results: \n",
      "{'loss': 2.096517644747339, 'acc': 0.4377104377104377}\n",
      "Test dataset results: \n",
      "{'loss': 1.7104399380860504, 'acc': 0.4234006734006734}\n",
      "Test dataset results: \n",
      "{'loss': 2.3297799026925956, 'acc': 0.4031986531986532}\n",
      "Test dataset results: \n",
      "{'loss': 1.7073568932536476, 'acc': 0.4116161616161616}\n",
      "Test dataset results: \n",
      "{'loss': 2.455832085625491, 'acc': 0.38552188552188554}\n",
      "Test dataset results: \n",
      "{'loss': 1.9239263245553682, 'acc': 0.4537037037037037}\n",
      "Test dataset results: \n",
      "{'loss': 2.0707213734135483, 'acc': 0.39225589225589225}\n",
      "Test dataset results: \n",
      "{'loss': 1.749980006555114, 'acc': 0.42424242424242425}\n",
      "Test dataset results: \n",
      "{'loss': 1.496158576974965, 'acc': 0.515993265993266}\n",
      "Test dataset results: \n",
      "{'loss': 2.2504044702960186, 'acc': 0.4107744107744108}\n",
      "Test dataset results: \n",
      "{'loss': 1.9316352063959294, 'acc': 0.39646464646464646}\n",
      "Test dataset results: \n",
      "{'loss': 2.014941875380699, 'acc': 0.398989898989899}\n",
      "Test dataset results: \n",
      "{'loss': 2.7769833518198443, 'acc': 0.35269360269360267}\n",
      "Test dataset results: \n",
      "{'loss': 1.8593631336584637, 'acc': 0.4006734006734007}\n",
      "Test dataset results: \n",
      "{'loss': 1.6504787026029644, 'acc': 0.43686868686868685}\n",
      "Test dataset results: \n",
      "{'loss': 1.9948688436437536, 'acc': 0.4385521885521885}\n",
      "Test dataset results: \n",
      "{'loss': 1.9051749666130502, 'acc': 0.48148148148148145}\n",
      "Test dataset results: \n",
      "{'loss': 1.6339804837197969, 'acc': 0.4772727272727273}\n",
      "Test dataset results: \n",
      "{'loss': 1.9294112411010949, 'acc': 0.37962962962962965}\n",
      "Test dataset results: \n",
      "{'loss': 1.7197691687831171, 'acc': 0.4452861952861953}\n",
      "0.01 0.42295173961840626 0.03405693346059892\n",
      "\n",
      "Crowd noise adaptation model with 0.01 clean data\n",
      "WARNING:tensorflow:From /home/yajingyang/PycharmProjects/CrowdLayer/crowd_layer/crowd_layers.py:276: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Test dataset results: \n",
      "{'loss': -7.0495122787526965, 'crowds_classification_1_loss': -7.0495122787526965, 'baseline_loss': 0.7857128903018906, 'crowds_classification_1_acc': 0.0015782828282828283, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -7.0552678252711445, 'crowds_classification_2_loss': -7.0552678252711445, 'baseline_loss': 0.8503587888647811, 'crowds_classification_2_acc': 0.020938552188552187, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -7.237801696314956, 'crowds_classification_3_loss': -7.237801696314956, 'baseline_loss': 0.8235030379585345, 'crowds_classification_3_acc': 0.03019781144781145, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -6.995124190744727, 'crowds_classification_4_loss': -6.995124190744727, 'baseline_loss': 0.819474463134703, 'crowds_classification_4_acc': 0.0023148148148148147, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -7.047927999335909, 'crowds_classification_5_loss': -7.047927999335909, 'baseline_loss': 0.8486202393818383, 'crowds_classification_5_acc': 0.00484006734006734, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.869086456619931, 'crowds_classification_6_loss': -6.869086456619931, 'baseline_loss': 3.126158558208533, 'crowds_classification_6_acc': 0.021043771043771045, 'baseline_acc': 0.7104377104377104}\n",
      "Test dataset results: \n",
      "{'loss': -7.121366483193857, 'crowds_classification_7_loss': -7.121366483193857, 'baseline_loss': 1.0688009832041834, 'crowds_classification_7_acc': 0.020728114478114477, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -6.912977289270471, 'crowds_classification_8_loss': -6.912977289270471, 'baseline_loss': 2.3368893392158276, 'crowds_classification_8_acc': 0.0006313131313131314, 'baseline_acc': 0.7390572390572391}\n",
      "Test dataset results: \n",
      "{'loss': -6.84545130039305, 'crowds_classification_9_loss': -6.84545130039305, 'baseline_loss': 2.562287444618816, 'crowds_classification_9_acc': 0.020728114478114477, 'baseline_acc': 0.7424242424242424}\n",
      "Test dataset results: \n",
      "{'loss': -7.0698412147033896, 'crowds_classification_10_loss': -7.0698412147033896, 'baseline_loss': 0.815008683224218, 'crowds_classification_10_acc': 0.021464646464646464, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -7.00846810292716, 'crowds_classification_11_loss': -7.00846810292716, 'baseline_loss': 2.4488236966671466, 'crowds_classification_11_acc': 0.0006313131313131314, 'baseline_acc': 0.7558922558922558}\n",
      "Test dataset results: \n",
      "{'loss': -7.06350896816061, 'crowds_classification_12_loss': -7.06350896816061, 'baseline_loss': 0.881468863097907, 'crowds_classification_12_acc': 0.032302188552188554, 'baseline_acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': -7.2229772673712835, 'crowds_classification_13_loss': -7.2229772673712835, 'baseline_loss': 0.7847079752320393, 'crowds_classification_13_acc': 0.03125, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -7.198080175252073, 'crowds_classification_14_loss': -7.198080175252073, 'baseline_loss': 0.8127335585428007, 'crowds_classification_14_acc': 0.012836700336700337, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -7.010900993539829, 'crowds_classification_15_loss': -7.010900993539829, 'baseline_loss': 2.708277025219266, 'crowds_classification_15_acc': 0.01999158249158249, 'baseline_acc': 0.7314814814814815}\n",
      "Test dataset results: \n",
      "{'loss': -7.00729827045993, 'crowds_classification_16_loss': -7.00729827045993, 'baseline_loss': 0.8240362243595147, 'crowds_classification_16_acc': 0.004208754208754209, 'baseline_acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': -7.146272081317323, 'crowds_classification_17_loss': -7.146272081317323, 'baseline_loss': 1.1006836286096862, 'crowds_classification_17_acc': 0.028303872053872053, 'baseline_acc': 0.8156565656565656}\n",
      "Test dataset results: \n",
      "{'loss': -7.04722454732516, 'crowds_classification_18_loss': -7.04722454732516, 'baseline_loss': 2.6846084810969004, 'crowds_classification_18_acc': 0.02030723905723906, 'baseline_acc': 0.7382154882154882}\n",
      "Test dataset results: \n",
      "{'loss': -7.0827335267757325, 'crowds_classification_19_loss': -7.0827335267757325, 'baseline_loss': 0.9589216010864535, 'crowds_classification_19_acc': 0.02030723905723906, 'baseline_acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': -6.960292479004523, 'crowds_classification_20_loss': -6.960292479004523, 'baseline_loss': 2.488910098669906, 'crowds_classification_20_acc': 0.00010521885521885521, 'baseline_acc': 0.7752525252525253}\n",
      "Test dataset results: \n",
      "{'loss': -7.16281837084478, 'crowds_classification_21_loss': -7.16281837084478, 'baseline_loss': 0.7256662025796845, 'crowds_classification_21_acc': 0.023148148148148147, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -7.191720872615725, 'crowds_classification_22_loss': -7.191720872615725, 'baseline_loss': 0.8719404086671293, 'crowds_classification_22_acc': 0.03103956228956229, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -6.826736093771578, 'crowds_classification_23_loss': -6.826736093771578, 'baseline_loss': 2.1000826358795166, 'crowds_classification_23_acc': 0.0012626262626262627, 'baseline_acc': 0.7676767676767676}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.244180178401446, 'crowds_classification_24_loss': -7.244180178401446, 'baseline_loss': 0.7573958964170167, 'crowds_classification_24_acc': 0.030829124579124578, 'baseline_acc': 0.8569023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -6.970684393487796, 'crowds_classification_25_loss': -6.970684393487796, 'baseline_loss': 2.7157080116296055, 'crowds_classification_25_acc': 0.024305555555555556, 'baseline_acc': 0.7289562289562289}\n",
      "Test dataset results: \n",
      "{'loss': -7.14579469667942, 'crowds_classification_26_loss': -7.14579469667942, 'baseline_loss': 0.7556540212697453, 'crowds_classification_26_acc': 0.02798821548821549, 'baseline_acc': 0.8535353535353535}\n",
      "Test dataset results: \n",
      "{'loss': -7.163216504183683, 'crowds_classification_27_loss': -7.163216504183683, 'baseline_loss': 0.9052759992203327, 'crowds_classification_27_acc': 0.02925084175084175, 'baseline_acc': 0.8400673400673401}\n",
      "Test dataset results: \n",
      "{'loss': -6.972976373100923, 'crowds_classification_28_loss': -6.972976373100923, 'baseline_loss': 2.7225289328732476, 'crowds_classification_28_acc': 0.007891414141414142, 'baseline_acc': 0.7643097643097643}\n",
      "Test dataset results: \n",
      "{'loss': -7.1684243510467835, 'crowds_classification_29_loss': -7.1684243510467835, 'baseline_loss': 0.8653146612523782, 'crowds_classification_29_acc': 0.029356060606060608, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -7.0582331326674135, 'crowds_classification_30_loss': -7.0582331326674135, 'baseline_loss': 0.9700993848117915, 'crowds_classification_30_acc': 0.020622895622895623, 'baseline_acc': 0.8249158249158249}\n",
      "0.01 0.8058922558922557 0.04488913419894496\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': -6.907012517203386, 'crowds_classification_31_loss': -6.907012517203386, 'baseline_loss': 0.8844716254910234, 'crowds_classification_31_acc': 0.004945286195286195, 'baseline_acc': 0.8409090909090909}\n",
      "Test dataset results: \n",
      "{'loss': -6.9729930415298, 'crowds_classification_32_loss': -6.9729930415298, 'baseline_loss': 0.9258314514099949, 'crowds_classification_32_acc': 0.011994949494949494, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.868990461432974, 'crowds_classification_33_loss': -6.868990461432974, 'baseline_loss': 1.3887327184982172, 'crowds_classification_33_acc': 0.0010521885521885522, 'baseline_acc': 0.8097643097643098}\n",
      "Test dataset results: \n",
      "{'loss': -6.914488959392715, 'crowds_classification_34_loss': -6.914488959392715, 'baseline_loss': 0.7997522741079531, 'crowds_classification_34_acc': 0.00042087542087542086, 'baseline_acc': 0.8400673400673401}\n",
      "Test dataset results: \n",
      "{'loss': -6.851395812500206, 'crowds_classification_35_loss': -6.851395812500206, 'baseline_loss': 0.840301539134297, 'crowds_classification_35_acc': 0.01231060606060606, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.819794348193339, 'crowds_classification_36_loss': -6.819794348193339, 'baseline_loss': 1.137583654687461, 'crowds_classification_36_acc': 0.02356902356902357, 'baseline_acc': 0.8106060606060606}\n",
      "Test dataset results: \n",
      "{'loss': -6.848303027425953, 'crowds_classification_37_loss': -6.848303027425953, 'baseline_loss': 0.9593492095117215, 'crowds_classification_37_acc': 0.021675084175084174, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -6.959412647016121, 'crowds_classification_38_loss': -6.959412647016121, 'baseline_loss': 1.0737997036088596, 'crowds_classification_38_acc': 0.029566498316498317, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -6.954396305662213, 'crowds_classification_39_loss': -6.954396305662213, 'baseline_loss': 1.2100944948517514, 'crowds_classification_39_acc': 0.03177609427609428, 'baseline_acc': 0.8156565656565656}\n",
      "Test dataset results: \n",
      "{'loss': -7.097458821755868, 'crowds_classification_40_loss': -7.097458821755868, 'baseline_loss': 0.9083149207987969, 'crowds_classification_40_acc': 0.03272306397306397, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -6.938015371059328, 'crowds_classification_41_loss': -6.938015371059328, 'baseline_loss': 0.9259058027230329, 'crowds_classification_41_acc': 0.028093434343434344, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.9399389947704995, 'crowds_classification_42_loss': -6.9399389947704995, 'baseline_loss': 1.0631835396944072, 'crowds_classification_42_acc': 0.031144781144781145, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -7.070635458435675, 'crowds_classification_43_loss': -7.070635458435675, 'baseline_loss': 0.930685015846834, 'crowds_classification_43_acc': 0.032196969696969696, 'baseline_acc': 0.8560606060606061}\n",
      "Test dataset results: \n",
      "{'loss': -6.984947922253849, 'crowds_classification_44_loss': -6.984947922253849, 'baseline_loss': 0.8779671701292197, 'crowds_classification_44_acc': 0.02135942760942761, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.851274975221165, 'crowds_classification_45_loss': -6.851274975221165, 'baseline_loss': 0.8557176371465628, 'crowds_classification_45_acc': 0.01231060606060606, 'baseline_acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': -6.987516276362769, 'crowds_classification_46_loss': -6.987516276362769, 'baseline_loss': 0.8561889967012536, 'crowds_classification_46_acc': 0.03156565656565657, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.979281085107463, 'crowds_classification_47_loss': -6.979281085107463, 'baseline_loss': 0.9700729290418553, 'crowds_classification_47_acc': 0.032302188552188554, 'baseline_acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': -7.025108579834703, 'crowds_classification_48_loss': -7.025108579834703, 'baseline_loss': 0.961497312894574, 'crowds_classification_48_acc': 0.03419612794612795, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -6.919892006048851, 'crowds_classification_49_loss': -6.919892006048851, 'baseline_loss': 0.938727084509652, 'crowds_classification_49_acc': 0.01167929292929293, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -7.0421893219353775, 'crowds_classification_50_loss': -7.0421893219353775, 'baseline_loss': 1.069031364162161, 'crowds_classification_50_acc': 0.032407407407407406, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -6.953365399781301, 'crowds_classification_51_loss': -6.953365399781301, 'baseline_loss': 0.9293846808631061, 'crowds_classification_51_acc': 0.010732323232323232, 'baseline_acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': -6.879742546916409, 'crowds_classification_52_loss': -6.879742546916409, 'baseline_loss': 1.1987668916031167, 'crowds_classification_52_acc': 0.02335858585858586, 'baseline_acc': 0.8148148148148148}\n",
      "Test dataset results: \n",
      "{'loss': -6.916622428380279, 'crowds_classification_53_loss': -6.916622428380279, 'baseline_loss': 0.8428827772287006, 'crowds_classification_53_acc': 0.03167087542087542, 'baseline_acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': -6.954966657490843, 'crowds_classification_54_loss': -6.954966657490843, 'baseline_loss': 1.0172187365867473, 'crowds_classification_54_acc': 0.029461279461279462, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -7.0101093828477445, 'crowds_classification_55_loss': -7.0101093828477445, 'baseline_loss': 0.9258474044482967, 'crowds_classification_55_acc': 0.03019781144781145, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -6.864428740157824, 'crowds_classification_56_loss': -6.864428740157824, 'baseline_loss': 0.9555547698479309, 'crowds_classification_56_acc': 0.007575757575757576, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -6.9362446113869, 'crowds_classification_57_loss': -6.9362446113869, 'baseline_loss': 1.1794335151009687, 'crowds_classification_57_acc': 0.03177609427609428, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -6.891573366492685, 'crowds_classification_58_loss': -6.891573366492685, 'baseline_loss': 1.0466160847482457, 'crowds_classification_58_acc': 0.002840909090909091, 'baseline_acc': 0.8215488215488216}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -6.898029690238362, 'crowds_classification_59_loss': -6.898029690238362, 'baseline_loss': 0.9571079032288657, 'crowds_classification_59_acc': 0.022832491582491583, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -6.971898387176822, 'crowds_classification_60_loss': -6.971898387176822, 'baseline_loss': 0.9209376478266685, 'crowds_classification_60_acc': 0.03188131313131313, 'baseline_acc': 0.8341750841750841}\n",
      "0.01 0.8315656565656563 0.010523232237048078\n",
      "\n",
      "Baseline model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.5477059678196505, 'acc': 0.5244107744107744}\n",
      "Test dataset results: \n",
      "{'loss': 1.6929704857996417, 'acc': 0.48063973063973064}\n",
      "Test dataset results: \n",
      "{'loss': 1.1683529414713183, 'acc': 0.57996632996633}\n",
      "Test dataset results: \n",
      "{'loss': 1.2473490928559994, 'acc': 0.563973063973064}\n",
      "Test dataset results: \n",
      "{'loss': 1.3468182982017698, 'acc': 0.5311447811447811}\n",
      "Test dataset results: \n",
      "{'loss': 1.2170211550362584, 'acc': 0.5555555555555556}\n",
      "Test dataset results: \n",
      "{'loss': 1.2269851431902812, 'acc': 0.5925925925925926}\n",
      "Test dataset results: \n",
      "{'loss': 1.510701921652463, 'acc': 0.5176767676767676}\n",
      "Test dataset results: \n",
      "{'loss': 1.1870243746022182, 'acc': 0.5749158249158249}\n",
      "Test dataset results: \n",
      "{'loss': 1.4051911090359543, 'acc': 0.5664983164983165}\n",
      "Test dataset results: \n",
      "{'loss': 1.3138847854803708, 'acc': 0.5681818181818182}\n",
      "Test dataset results: \n",
      "{'loss': 1.209115253353761, 'acc': 0.6026936026936027}\n",
      "Test dataset results: \n",
      "{'loss': 1.2834350575100293, 'acc': 0.5429292929292929}\n",
      "Test dataset results: \n",
      "{'loss': 1.2744017347743615, 'acc': 0.5664983164983165}\n",
      "Test dataset results: \n",
      "{'loss': 1.3076483430685821, 'acc': 0.5378787878787878}\n",
      "Test dataset results: \n",
      "{'loss': 1.406878082439153, 'acc': 0.5732323232323232}\n",
      "Test dataset results: \n",
      "{'loss': 1.3590897698996445, 'acc': 0.5782828282828283}\n",
      "Test dataset results: \n",
      "{'loss': 1.503392345576174, 'acc': 0.5252525252525253}\n",
      "Test dataset results: \n",
      "{'loss': 1.274905638662653, 'acc': 0.5892255892255892}\n",
      "Test dataset results: \n",
      "{'loss': 1.1233180089028998, 'acc': 0.6085858585858586}\n",
      "Test dataset results: \n",
      "{'loss': 1.1061029516486607, 'acc': 0.6161616161616161}\n",
      "Test dataset results: \n",
      "{'loss': 1.363606482543528, 'acc': 0.569023569023569}\n",
      "Test dataset results: \n",
      "{'loss': 1.3122229014181528, 'acc': 0.5656565656565656}\n",
      "Test dataset results: \n",
      "{'loss': 1.3207980310074006, 'acc': 0.5715488215488216}\n",
      "Test dataset results: \n",
      "{'loss': 1.2088095614003012, 'acc': 0.5909090909090909}\n",
      "Test dataset results: \n",
      "{'loss': 1.3037993153337677, 'acc': 0.5782828282828283}\n",
      "Test dataset results: \n",
      "{'loss': 1.8071832945852568, 'acc': 0.47895622895622897}\n",
      "Test dataset results: \n",
      "{'loss': 1.3201712510401151, 'acc': 0.5572390572390572}\n",
      "Test dataset results: \n",
      "{'loss': 1.2379579814997586, 'acc': 0.5909090909090909}\n",
      "Test dataset results: \n",
      "{'loss': 1.126678193458403, 'acc': 0.5934343434343434}\n",
      "0.02 0.5630751964085298 0.0329897131172882\n",
      "\n",
      "Crowd noise adaptation model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': -7.07588103483823, 'crowds_classification_61_loss': -7.07588103483823, 'baseline_loss': 0.6877245086461606, 'crowds_classification_61_acc': 0.022095959595959596, 'baseline_acc': 0.8569023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -7.074298043042321, 'crowds_classification_62_loss': -7.074298043042321, 'baseline_loss': 1.039725534091092, 'crowds_classification_62_acc': 0.01210016835016835, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -7.21300990974863, 'crowds_classification_63_loss': -7.21300990974863, 'baseline_loss': 0.9794175216445217, 'crowds_classification_63_acc': 0.024726430976430975, 'baseline_acc': 0.7996632996632996}\n",
      "Test dataset results: \n",
      "{'loss': -7.09423990281744, 'crowds_classification_64_loss': -7.09423990281744, 'baseline_loss': 2.291745376795129, 'crowds_classification_64_acc': 0.01914983164983165, 'baseline_acc': 0.7744107744107744}\n",
      "Test dataset results: \n",
      "{'loss': -7.070599947714244, 'crowds_classification_65_loss': -7.070599947714244, 'baseline_loss': 0.687252333958442, 'crowds_classification_65_acc': 0.021780303030303032, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -6.9683900730377095, 'crowds_classification_66_loss': -6.9683900730377095, 'baseline_loss': 2.7109164420001033, 'crowds_classification_66_acc': 0.001367845117845118, 'baseline_acc': 0.7281144781144782}\n",
      "Test dataset results: \n",
      "{'loss': -7.109877225124475, 'crowds_classification_67_loss': -7.109877225124475, 'baseline_loss': 0.7803822496423015, 'crowds_classification_67_acc': 0.029776936026936027, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -7.014787010873608, 'crowds_classification_68_loss': -7.014787010873608, 'baseline_loss': 0.7693319753163591, 'crowds_classification_68_acc': 0.029776936026936027, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -7.1836804107383445, 'crowds_classification_69_loss': -7.1836804107383445, 'baseline_loss': 0.8039314043441605, 'crowds_classification_69_acc': 0.028093434343434344, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -7.04768424082284, 'crowds_classification_70_loss': -7.04768424082284, 'baseline_loss': 0.9404632660375299, 'crowds_classification_70_acc': 0.005681818181818182, 'baseline_acc': 0.8080808080808081}\n",
      "Test dataset results: \n",
      "{'loss': -7.076497395833333, 'crowds_classification_71_loss': -7.076497395833333, 'baseline_loss': 0.9024375613891717, 'crowds_classification_71_acc': 0.0006313131313131314, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -7.11930577120797, 'crowds_classification_72_loss': -7.11930577120797, 'baseline_loss': 0.8113675100559538, 'crowds_classification_72_acc': 0.0211489898989899, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -7.302175101206359, 'crowds_classification_73_loss': -7.302175101206359, 'baseline_loss': 0.917094432469771, 'crowds_classification_73_acc': 0.029987373737373736, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.854572475959958, 'crowds_classification_74_loss': -6.854572475959958, 'baseline_loss': 4.110450834537596, 'crowds_classification_74_acc': 0.0005260942760942761, 'baseline_acc': 0.6439393939393939}\n",
      "Test dataset results: \n",
      "{'loss': -6.911615641430171, 'crowds_classification_75_loss': -6.911615641430171, 'baseline_loss': 2.03212859734943, 'crowds_classification_75_acc': 0.021780303030303032, 'baseline_acc': 0.7592592592592593}\n",
      "Test dataset results: \n",
      "{'loss': -7.036898545544557, 'crowds_classification_76_loss': -7.036898545544557, 'baseline_loss': 2.4866770367429716, 'crowds_classification_76_acc': 0.003367003367003367, 'baseline_acc': 0.7533670033670034}\n",
      "Test dataset results: \n",
      "{'loss': -7.169654181509307, 'crowds_classification_77_loss': -7.169654181509307, 'baseline_loss': 0.956652845597829, 'crowds_classification_77_acc': 0.031144781144781145, 'baseline_acc': 0.8055555555555556}\n",
      "Test dataset results: \n",
      "{'loss': -7.105278647708571, 'crowds_classification_78_loss': -7.105278647708571, 'baseline_loss': 0.706937742391319, 'crowds_classification_78_acc': 0.028093434343434344, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -7.052932108291472, 'crowds_classification_79_loss': -7.052932108291472, 'baseline_loss': 0.7100120637854341, 'crowds_classification_79_acc': 0.02598905723905724, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -7.15565708109024, 'crowds_classification_80_loss': -7.15565708109024, 'baseline_loss': 0.7658490135462999, 'crowds_classification_80_acc': 0.030303030303030304, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -7.19312595277523, 'crowds_classification_81_loss': -7.19312595277523, 'baseline_loss': 0.6920318420888599, 'crowds_classification_81_acc': 0.009364478114478115, 'baseline_acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': -7.138760097901829, 'crowds_classification_82_loss': -7.138760097901829, 'baseline_loss': 1.0610613393462467, 'crowds_classification_82_acc': 0.032617845117845115, 'baseline_acc': 0.8005050505050505}\n",
      "Test dataset results: \n",
      "{'loss': -7.154897999683213, 'crowds_classification_83_loss': -7.154897999683213, 'baseline_loss': 0.6269348065513076, 'crowds_classification_83_acc': 0.0007365319865319865, 'baseline_acc': 0.8535353535353535}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.101573797990176, 'crowds_classification_84_loss': -7.101573797990176, 'baseline_loss': 0.7793949295951662, 'crowds_classification_84_acc': 0.010416666666666666, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -7.030849496924917, 'crowds_classification_85_loss': -7.030849496924917, 'baseline_loss': 0.9219747696460698, 'crowds_classification_85_acc': 0.008312289562289563, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -7.137381458924676, 'crowds_classification_86_loss': -7.137381458924676, 'baseline_loss': 1.1815676966098825, 'crowds_classification_86_acc': 0.0030513468013468013, 'baseline_acc': 0.8038720538720538}\n",
      "Test dataset results: \n",
      "{'loss': -7.139444648216068, 'crowds_classification_87_loss': -7.139444648216068, 'baseline_loss': 0.9991874734962026, 'crowds_classification_87_acc': 0.011784511784511785, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -6.987733213187067, 'crowds_classification_88_loss': -6.987733213187067, 'baseline_loss': 2.449620587556852, 'crowds_classification_88_acc': 0.021675084175084174, 'baseline_acc': 0.7592592592592593}\n",
      "Test dataset results: \n",
      "{'loss': -7.017592059241401, 'crowds_classification_89_loss': -7.017592059241401, 'baseline_loss': 0.7605746658863844, 'crowds_classification_89_acc': 0.001367845117845118, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -7.0331388162041355, 'crowds_classification_90_loss': -7.0331388162041355, 'baseline_loss': 0.7115548879209191, 'crowds_classification_90_acc': 0.021780303030303032, 'baseline_acc': 0.8543771043771043}\n",
      "0.02 0.8118686868686867 0.04490920210740751\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': -6.9227903716090555, 'crowds_classification_91_loss': -6.9227903716090555, 'baseline_loss': 0.9518182486521475, 'crowds_classification_91_acc': 0.00726010101010101, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -6.844369958948206, 'crowds_classification_92_loss': -6.844369958948206, 'baseline_loss': 0.938599824403673, 'crowds_classification_92_acc': 0.03146043771043771, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -6.9164348066053805, 'crowds_classification_93_loss': -6.9164348066053805, 'baseline_loss': 0.9080498603796617, 'crowds_classification_93_acc': 0.027567340067340067, 'baseline_acc': 0.8459595959595959}\n",
      "Test dataset results: \n",
      "{'loss': -6.9312295094885, 'crowds_classification_94_loss': -6.9312295094885, 'baseline_loss': 0.886135312041851, 'crowds_classification_94_acc': 0.023148148148148147, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -6.842348659078682, 'crowds_classification_95_loss': -6.842348659078682, 'baseline_loss': 1.2866659748433815, 'crowds_classification_95_acc': 0.023463804713804715, 'baseline_acc': 0.8047138047138047}\n",
      "Test dataset results: \n",
      "{'loss': -6.857519551158353, 'crowds_classification_96_loss': -6.857519551158353, 'baseline_loss': 0.8513066376800891, 'crowds_classification_96_acc': 0.0010521885521885522, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -6.921089644383902, 'crowds_classification_97_loss': -6.921089644383902, 'baseline_loss': 0.8844340538867884, 'crowds_classification_97_acc': 0.01936026936026936, 'baseline_acc': 0.8425925925925926}\n",
      "Test dataset results: \n",
      "{'loss': -6.856680513632418, 'crowds_classification_98_loss': -6.856680513632418, 'baseline_loss': 0.8340809265095176, 'crowds_classification_98_acc': 0.021254208754208755, 'baseline_acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': -7.018156629620177, 'crowds_classification_99_loss': -7.018156629620177, 'baseline_loss': 1.1209158094643743, 'crowds_classification_99_acc': 0.034406565656565656, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -6.841957162927698, 'crowds_classification_100_loss': -6.841957162927698, 'baseline_loss': 0.959913351319053, 'crowds_classification_100_acc': 0.023674242424242424, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.878499400334728, 'crowds_classification_101_loss': -6.878499400334728, 'baseline_loss': 0.9022629219875593, 'crowds_classification_101_acc': 0.024726430976430975, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -6.905550661311808, 'crowds_classification_102_loss': -6.905550661311808, 'baseline_loss': 1.0835147632743773, 'crowds_classification_102_acc': 0.009364478114478115, 'baseline_acc': 0.8156565656565656}\n",
      "Test dataset results: \n",
      "{'loss': -6.866477723876234, 'crowds_classification_103_loss': -6.866477723876234, 'baseline_loss': 0.8325581543135021, 'crowds_classification_103_acc': 0.03282828282828283, 'baseline_acc': 0.8484848484848485}\n",
      "Test dataset results: \n",
      "{'loss': -6.896482143337879, 'crowds_classification_104_loss': -6.896482143337879, 'baseline_loss': 0.9228431578186225, 'crowds_classification_104_acc': 0.026199494949494948, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -6.8678168206905275, 'crowds_classification_105_loss': -6.8678168206905275, 'baseline_loss': 0.7837820223372693, 'crowds_classification_105_acc': 0.020833333333333332, 'baseline_acc': 0.8543771043771043}\n",
      "Test dataset results: \n",
      "{'loss': -7.026693018197211, 'crowds_classification_106_loss': -7.026693018197211, 'baseline_loss': 1.0014892294658002, 'crowds_classification_106_acc': 0.029566498316498317, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -6.880118610883, 'crowds_classification_107_loss': -6.880118610883, 'baseline_loss': 0.9562044677885895, 'crowds_classification_107_acc': 0.022727272727272728, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -6.853254803102025, 'crowds_classification_108_loss': -6.853254803102025, 'baseline_loss': 1.1084807534209806, 'crowds_classification_108_acc': 0.03356481481481482, 'baseline_acc': 0.8164983164983165}\n",
      "Test dataset results: \n",
      "{'loss': -6.91621583078044, 'crowds_classification_109_loss': -6.91621583078044, 'baseline_loss': 1.1384151704383618, 'crowds_classification_109_acc': 0.03272306397306397, 'baseline_acc': 0.8131313131313131}\n",
      "Test dataset results: \n",
      "{'loss': -7.0307695135123, 'crowds_classification_110_loss': -7.0307695135123, 'baseline_loss': 0.9993888157535884, 'crowds_classification_110_acc': 0.03388047138047138, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.880321342134315, 'crowds_classification_111_loss': -6.880321342134315, 'baseline_loss': 0.9401764186845484, 'crowds_classification_111_acc': 0.002946127946127946, 'baseline_acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': -6.924362195461286, 'crowds_classification_112_loss': -6.924362195461286, 'baseline_loss': 0.964409925150199, 'crowds_classification_112_acc': 0.02135942760942761, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -6.849196507874563, 'crowds_classification_113_loss': -6.849196507874563, 'baseline_loss': 0.7960716680645541, 'crowds_classification_113_acc': 0.021464646464646464, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -7.030903463010435, 'crowds_classification_114_loss': -7.030903463010435, 'baseline_loss': 1.0904303139408027, 'crowds_classification_114_acc': 0.03345959595959596, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -6.929079500513045, 'crowds_classification_115_loss': -6.929079500513045, 'baseline_loss': 1.2603170492432334, 'crowds_classification_115_acc': 0.03282828282828283, 'baseline_acc': 0.8164983164983165}\n",
      "Test dataset results: \n",
      "{'loss': -7.007380206175525, 'crowds_classification_116_loss': -7.007380206175525, 'baseline_loss': 0.9971444472695641, 'crowds_classification_116_acc': 0.03188131313131313, 'baseline_acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': -6.900247952753446, 'crowds_classification_117_loss': -6.900247952753446, 'baseline_loss': 0.7676026669114527, 'crowds_classification_117_acc': 0.013257575757575758, 'baseline_acc': 0.8484848484848485}\n",
      "Test dataset results: \n",
      "{'loss': -6.854445553789235, 'crowds_classification_118_loss': -6.854445553789235, 'baseline_loss': 0.8627926580331944, 'crowds_classification_118_acc': 0.028724747474747476, 'baseline_acc': 0.8324915824915825}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -6.80191461646597, 'crowds_classification_119_loss': -6.80191461646597, 'baseline_loss': 0.9307503841491259, 'crowds_classification_119_acc': 0.029987373737373736, 'baseline_acc': 0.8173400673400674}\n",
      "Test dataset results: \n",
      "{'loss': -7.001889206343629, 'crowds_classification_120_loss': -7.001889206343629, 'baseline_loss': 0.8437947028815144, 'crowds_classification_120_acc': 0.02704124579124579, 'baseline_acc': 0.8367003367003367}\n",
      "0.02 0.8314534231200895 0.011328109517545986\n",
      "\n",
      "Baseline model with 0.03 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.110034982363383, 'acc': 0.6742424242424242}\n",
      "Test dataset results: \n",
      "{'loss': 1.048101994927082, 'acc': 0.6607744107744108}\n",
      "Test dataset results: \n",
      "{'loss': 1.145135258986091, 'acc': 0.6582491582491582}\n",
      "Test dataset results: \n",
      "{'loss': 1.1453323017065773, 'acc': 0.6590909090909091}\n",
      "Test dataset results: \n",
      "{'loss': 1.0248626464545125, 'acc': 0.7045454545454546}\n",
      "Test dataset results: \n",
      "{'loss': 1.1728449560175038, 'acc': 0.6574074074074074}\n",
      "Test dataset results: \n",
      "{'loss': 1.2405940187900557, 'acc': 0.6742424242424242}\n",
      "Test dataset results: \n",
      "{'loss': 1.2851896277983181, 'acc': 0.6372053872053872}\n",
      "Test dataset results: \n",
      "{'loss': 1.250489182945855, 'acc': 0.6540404040404041}\n",
      "Test dataset results: \n",
      "{'loss': 1.1790380035385941, 'acc': 0.6717171717171717}\n",
      "Test dataset results: \n",
      "{'loss': 1.1858365214633622, 'acc': 0.6523569023569024}\n",
      "Test dataset results: \n",
      "{'loss': 1.2873048063881871, 'acc': 0.6405723905723906}\n",
      "Test dataset results: \n",
      "{'loss': 1.1483637209893878, 'acc': 0.6776094276094277}\n",
      "Test dataset results: \n",
      "{'loss': 1.1481284912587817, 'acc': 0.6262626262626263}\n",
      "Test dataset results: \n",
      "{'loss': 1.0613331727507942, 'acc': 0.6717171717171717}\n",
      "Test dataset results: \n",
      "{'loss': 1.1579335123200207, 'acc': 0.6641414141414141}\n",
      "Test dataset results: \n",
      "{'loss': 1.257880697547386, 'acc': 0.6346801346801347}\n",
      "Test dataset results: \n",
      "{'loss': 1.1236578248165272, 'acc': 0.6616161616161617}\n",
      "Test dataset results: \n",
      "{'loss': 1.3438568107206814, 'acc': 0.6119528619528619}\n",
      "Test dataset results: \n",
      "{'loss': 1.2215377224816217, 'acc': 0.6616161616161617}\n",
      "Test dataset results: \n",
      "{'loss': 1.044862219079175, 'acc': 0.680976430976431}\n",
      "Test dataset results: \n",
      "{'loss': 1.1821428186162954, 'acc': 0.6304713804713805}\n",
      "Test dataset results: \n",
      "{'loss': 1.0905397349736505, 'acc': 0.6750841750841751}\n",
      "Test dataset results: \n",
      "{'loss': 1.077650694333343, 'acc': 0.6868686868686869}\n",
      "Test dataset results: \n",
      "{'loss': 1.1303152620591699, 'acc': 0.6212121212121212}\n",
      "Test dataset results: \n",
      "{'loss': 1.351954080641069, 'acc': 0.6287878787878788}\n",
      "Test dataset results: \n",
      "{'loss': 1.2870416832893385, 'acc': 0.6473063973063973}\n",
      "Test dataset results: \n",
      "{'loss': 1.0943149751685686, 'acc': 0.6616161616161617}\n",
      "Test dataset results: \n",
      "{'loss': 1.1941843929916922, 'acc': 0.664983164983165}\n",
      "Test dataset results: \n",
      "{'loss': 1.2771582436882687, 'acc': 0.6035353535353535}\n",
      "0.03 0.655162738496072 0.022731775475032114\n",
      "\n",
      "Crowd noise adaptation model with 0.03 clean data\n",
      "Test dataset results: \n",
      "{'loss': -7.128231721293646, 'crowds_classification_121_loss': -7.128231721293646, 'baseline_loss': 0.8864587776187293, 'crowds_classification_121_acc': 0.023148148148148147, 'baseline_acc': 0.8089225589225589}\n",
      "Test dataset results: \n",
      "{'loss': -7.19156847899209, 'crowds_classification_122_loss': -7.19156847899209, 'baseline_loss': 2.270776211861082, 'crowds_classification_122_acc': 0.016203703703703703, 'baseline_acc': 0.7676767676767676}\n",
      "Test dataset results: \n",
      "{'loss': -7.141349912893893, 'crowds_classification_123_loss': -7.141349912893893, 'baseline_loss': 2.6170357327268583, 'crowds_classification_123_acc': 0.021780303030303032, 'baseline_acc': 0.7415824915824916}\n",
      "Test dataset results: \n",
      "{'loss': -7.173718503830007, 'crowds_classification_124_loss': -7.173718503830007, 'baseline_loss': 0.593143947146557, 'crowds_classification_124_acc': 0.026199494949494948, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.195210362122918, 'crowds_classification_125_loss': -7.195210362122918, 'baseline_loss': 0.6965567470600308, 'crowds_classification_125_acc': 0.029356060606060608, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -7.210271177067098, 'crowds_classification_126_loss': -7.210271177067098, 'baseline_loss': 0.66396323297963, 'crowds_classification_126_acc': 0.021464646464646464, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -7.215734765987204, 'crowds_classification_127_loss': -7.215734765987204, 'baseline_loss': 0.7561090556459394, 'crowds_classification_127_acc': 0.0281986531986532, 'baseline_acc': 0.8501683501683501}\n",
      "Test dataset results: \n",
      "{'loss': -7.177933578940754, 'crowds_classification_128_loss': -7.177933578940754, 'baseline_loss': 0.6857046909906246, 'crowds_classification_128_acc': 0.010311447811447811, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -7.2383496367971505, 'crowds_classification_129_loss': -7.2383496367971505, 'baseline_loss': 0.9274637238545851, 'crowds_classification_129_acc': 0.013783670033670033, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.970584994614726, 'crowds_classification_130_loss': -6.970584994614726, 'baseline_loss': 2.1845346428328494, 'crowds_classification_130_acc': 0.0007365319865319865, 'baseline_acc': 0.7702020202020202}\n",
      "Test dataset results: \n",
      "{'loss': -7.104366137122466, 'crowds_classification_131_loss': -7.104366137122466, 'baseline_loss': 1.0310130174097727, 'crowds_classification_131_acc': 0.023042929292929292, 'baseline_acc': 0.8089225589225589}\n",
      "Test dataset results: \n",
      "{'loss': -7.277436702741116, 'crowds_classification_132_loss': -7.277436702741116, 'baseline_loss': 0.7429923468272543, 'crowds_classification_132_acc': 0.030618686868686868, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -7.265660888016826, 'crowds_classification_133_loss': -7.265660888016826, 'baseline_loss': 0.9474937062748153, 'crowds_classification_133_acc': 0.022832491582491583, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -7.187084254190978, 'crowds_classification_134_loss': -7.187084254190978, 'baseline_loss': 0.7200672540575366, 'crowds_classification_134_acc': 0.028409090909090908, 'baseline_acc': 0.8484848484848485}\n",
      "Test dataset results: \n",
      "{'loss': -7.303471250566168, 'crowds_classification_135_loss': -7.303471250566168, 'baseline_loss': 0.6703656528784772, 'crowds_classification_135_acc': 0.025147306397306397, 'baseline_acc': 0.8501683501683501}\n",
      "Test dataset results: \n",
      "{'loss': -7.1934756574405965, 'crowds_classification_136_loss': -7.1934756574405965, 'baseline_loss': 0.7735406488355764, 'crowds_classification_136_acc': 0.0017887205387205388, 'baseline_acc': 0.8409090909090909}\n",
      "Test dataset results: \n",
      "{'loss': -7.113632497562704, 'crowds_classification_137_loss': -7.113632497562704, 'baseline_loss': 0.7326581049146075, 'crowds_classification_137_acc': 0.022832491582491583, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -7.300479479510375, 'crowds_classification_138_loss': -7.300479479510375, 'baseline_loss': 0.7095112688964866, 'crowds_classification_138_acc': 0.03009259259259259, 'baseline_acc': 0.8409090909090909}\n",
      "Test dataset results: \n",
      "{'loss': -7.181653427355217, 'crowds_classification_139_loss': -7.181653427355217, 'baseline_loss': 0.7191334209257505, 'crowds_classification_139_acc': 0.0031565656565656565, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -7.365816972071073, 'crowds_classification_140_loss': -7.365816972071073, 'baseline_loss': 0.6966723061169839, 'crowds_classification_140_acc': 0.028093434343434344, 'baseline_acc': 0.8518518518518519}\n",
      "Test dataset results: \n",
      "{'loss': -7.181002963672984, 'crowds_classification_141_loss': -7.181002963672984, 'baseline_loss': 0.7932619096253456, 'crowds_classification_141_acc': 0.0008417508417508417, 'baseline_acc': 0.835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': -7.265368341195463, 'crowds_classification_142_loss': -7.265368341195463, 'baseline_loss': 0.6667192703307377, 'crowds_classification_142_acc': 0.026936026936026935, 'baseline_acc': 0.8409090909090909}\n",
      "Test dataset results: \n",
      "{'loss': -7.123542321651471, 'crowds_classification_143_loss': -7.123542321651471, 'baseline_loss': 0.650263071135439, 'crowds_classification_143_acc': 0.020622895622895623, 'baseline_acc': 0.8459595959595959}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.165602134935783, 'crowds_classification_144_loss': -7.165602134935783, 'baseline_loss': 0.9119568954312841, 'crowds_classification_144_acc': 0.026199494949494948, 'baseline_acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': -7.2075915095782035, 'crowds_classification_145_loss': -7.2075915095782035, 'baseline_loss': 0.767613927121917, 'crowds_classification_145_acc': 0.0035774410774410776, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -7.189385428573146, 'crowds_classification_146_loss': -7.189385428573146, 'baseline_loss': 0.9383493028675215, 'crowds_classification_146_acc': 0.01125841750841751, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -7.126834813191834, 'crowds_classification_147_loss': -7.126834813191834, 'baseline_loss': 0.7454321022687938, 'crowds_classification_147_acc': 0.03019781144781145, 'baseline_acc': 0.8459595959595959}\n",
      "Test dataset results: \n",
      "{'loss': -7.266965896593601, 'crowds_classification_148_loss': -7.266965896593601, 'baseline_loss': 0.8545242591246787, 'crowds_classification_148_acc': 0.03188131313131313, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -7.222766691586787, 'crowds_classification_149_loss': -7.222766691586787, 'baseline_loss': 0.6694930236698803, 'crowds_classification_149_acc': 0.03019781144781145, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.176577291906081, 'crowds_classification_150_loss': -7.176577291906081, 'baseline_loss': 1.112883764786351, 'crowds_classification_150_acc': 0.022832491582491583, 'baseline_acc': 0.8198653198653199}\n",
      "0.03 0.8299663299663298 0.026195452092527462\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.03 clean data\n",
      "Test dataset results: \n",
      "{'loss': -6.838600447683623, 'crowds_classification_151_loss': -6.838600447683623, 'baseline_loss': 0.9813776175783138, 'crowds_classification_151_acc': 0.023253367003367005, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -6.846380627115166, 'crowds_classification_152_loss': -6.846380627115166, 'baseline_loss': 0.9082015132663226, 'crowds_classification_152_acc': 0.02335858585858586, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.872857093811035, 'crowds_classification_153_loss': -6.872857093811035, 'baseline_loss': 1.256742421023372, 'crowds_classification_153_acc': 0.0022095959595959595, 'baseline_acc': 0.8156565656565656}\n",
      "Test dataset results: \n",
      "{'loss': -6.854395811806624, 'crowds_classification_154_loss': -6.854395811806624, 'baseline_loss': 0.7868706789683011, 'crowds_classification_154_acc': 0.010732323232323232, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -7.0779032048954305, 'crowds_classification_155_loss': -7.0779032048954305, 'baseline_loss': 0.9153923773314955, 'crowds_classification_155_acc': 0.034617003367003366, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -6.869717991311943, 'crowds_classification_156_loss': -6.869717991311943, 'baseline_loss': 1.011699165358688, 'crowds_classification_156_acc': 0.027251683501683503, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -6.79875433404839, 'crowds_classification_157_loss': -6.79875433404839, 'baseline_loss': 0.9848215734517133, 'crowds_classification_157_acc': 0.0006313131313131314, 'baseline_acc': 0.8080808080808081}\n",
      "Test dataset results: \n",
      "{'loss': -6.876728920021442, 'crowds_classification_158_loss': -6.876728920021442, 'baseline_loss': 0.8548803887508735, 'crowds_classification_158_acc': 0.010416666666666666, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -6.913390731169319, 'crowds_classification_159_loss': -6.913390731169319, 'baseline_loss': 0.9172065233441715, 'crowds_classification_159_acc': 0.020728114478114477, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -6.835160399928237, 'crowds_classification_160_loss': -6.835160399928237, 'baseline_loss': 0.9817661087370481, 'crowds_classification_160_acc': 0.023148148148148147, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -6.870836911378084, 'crowds_classification_161_loss': -6.870836911378084, 'baseline_loss': 0.9918921946074425, 'crowds_classification_161_acc': 0.023253367003367005, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -7.00106846523606, 'crowds_classification_162_loss': -7.00106846523606, 'baseline_loss': 0.8666786473144084, 'crowds_classification_162_acc': 0.032617845117845115, 'baseline_acc': 0.8442760942760943}\n",
      "Test dataset results: \n",
      "{'loss': -6.998377539894798, 'crowds_classification_163_loss': -6.998377539894798, 'baseline_loss': 0.967355989697405, 'crowds_classification_163_acc': 0.03135521885521886, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -6.933853595746487, 'crowds_classification_164_loss': -6.933853595746487, 'baseline_loss': 0.9769535241303621, 'crowds_classification_164_acc': 0.03135521885521886, 'baseline_acc': 0.8265993265993266}\n",
      "Test dataset results: \n",
      "{'loss': -6.9317589143309934, 'crowds_classification_165_loss': -6.9317589143309934, 'baseline_loss': 0.9231102347988572, 'crowds_classification_165_acc': 0.02882996632996633, 'baseline_acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': -6.94558080840191, 'crowds_classification_166_loss': -6.94558080840191, 'baseline_loss': 1.0063670259533506, 'crowds_classification_166_acc': 0.026515151515151516, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -6.837224870418459, 'crowds_classification_167_loss': -6.837224870418459, 'baseline_loss': 0.9879551281029929, 'crowds_classification_167_acc': 0.023674242424242424, 'baseline_acc': 0.8249158249158249}\n",
      "Test dataset results: \n",
      "{'loss': -6.921733008490668, 'crowds_classification_168_loss': -6.921733008490668, 'baseline_loss': 0.9606907460775841, 'crowds_classification_168_acc': 0.020938552188552187, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -6.8847784787316115, 'crowds_classification_169_loss': -6.8847784787316115, 'baseline_loss': 0.9621737736343133, 'crowds_classification_169_acc': 0.025673400673400674, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -6.831647510079021, 'crowds_classification_170_loss': -6.831647510079021, 'baseline_loss': 0.7667846330739432, 'crowds_classification_170_acc': 0.021464646464646464, 'baseline_acc': 0.8425925925925926}\n",
      "Test dataset results: \n",
      "{'loss': -6.875719269518098, 'crowds_classification_171_loss': -6.875719269518098, 'baseline_loss': 0.934672702843894, 'crowds_classification_171_acc': 0.001893939393939394, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -6.829895945911857, 'crowds_classification_172_loss': -6.829895945911857, 'baseline_loss': 0.9242189976101371, 'crowds_classification_172_acc': 0.027356902356902357, 'baseline_acc': 0.8106060606060606}\n",
      "Test dataset results: \n",
      "{'loss': -6.8364461423572065, 'crowds_classification_173_loss': -6.8364461423572065, 'baseline_loss': 0.9110565081188574, 'crowds_classification_173_acc': 0.006734006734006734, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -6.909118157845956, 'crowds_classification_174_loss': -6.909118157845956, 'baseline_loss': 0.9249210530760312, 'crowds_classification_174_acc': 0.003367003367003367, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -6.8340866878779245, 'crowds_classification_175_loss': -6.8340866878779245, 'baseline_loss': 1.0375575294799677, 'crowds_classification_175_acc': 0.004313973063973064, 'baseline_acc': 0.8240740740740741}\n",
      "Test dataset results: \n",
      "{'loss': -6.994435531924469, 'crowds_classification_176_loss': -6.994435531924469, 'baseline_loss': 0.9039844630054051, 'crowds_classification_176_acc': 0.031144781144781145, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.94480274662827, 'crowds_classification_177_loss': -6.94480274662827, 'baseline_loss': 0.9981057505916666, 'crowds_classification_177_acc': 0.013257575757575758, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -6.852723428295919, 'crowds_classification_178_loss': -6.852723428295919, 'baseline_loss': 0.9480291842059656, 'crowds_classification_178_acc': 0.02199074074074074, 'baseline_acc': 0.8383838383838383}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -6.956112280437842, 'crowds_classification_179_loss': -6.956112280437842, 'baseline_loss': 0.8866790503589593, 'crowds_classification_179_acc': 0.002840909090909091, 'baseline_acc': 0.8425925925925926}\n",
      "Test dataset results: \n",
      "{'loss': -7.001610532754198, 'crowds_classification_180_loss': -7.001610532754198, 'baseline_loss': 1.0440166567260971, 'crowds_classification_180_acc': 0.03324915824915825, 'baseline_acc': 0.8333333333333334}\n",
      "0.03 0.8294332210998874 0.009374558461421996\n",
      "\n",
      "Baseline model with 0.04 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.072236053470008, 'acc': 0.6843434343434344}\n",
      "Test dataset results: \n",
      "{'loss': 0.9583428391703853, 'acc': 0.6717171717171717}\n",
      "Test dataset results: \n",
      "{'loss': 1.0137150135104505, 'acc': 0.6843434343434344}\n",
      "Test dataset results: \n",
      "{'loss': 0.9936923807116871, 'acc': 0.6835016835016835}\n",
      "Test dataset results: \n",
      "{'loss': 1.0350774185424703, 'acc': 0.6868686868686869}\n",
      "Test dataset results: \n",
      "{'loss': 1.0829286881368168, 'acc': 0.6759259259259259}\n",
      "Test dataset results: \n",
      "{'loss': 1.0789141050894253, 'acc': 0.6742424242424242}\n",
      "Test dataset results: \n",
      "{'loss': 1.3844375917405793, 'acc': 0.6287878787878788}\n",
      "Test dataset results: \n",
      "{'loss': 1.27652739354657, 'acc': 0.6683501683501684}\n",
      "Test dataset results: \n",
      "{'loss': 1.161352627907538, 'acc': 0.6582491582491582}\n",
      "Test dataset results: \n",
      "{'loss': 0.9404314302836203, 'acc': 0.6902356902356902}\n",
      "Test dataset results: \n",
      "{'loss': 1.030211442548418, 'acc': 0.6776094276094277}\n",
      "Test dataset results: \n",
      "{'loss': 1.0893035508968212, 'acc': 0.6750841750841751}\n",
      "Test dataset results: \n",
      "{'loss': 1.1038784083693918, 'acc': 0.6624579124579124}\n",
      "Test dataset results: \n",
      "{'loss': 1.0147218339973025, 'acc': 0.6851851851851852}\n",
      "Test dataset results: \n",
      "{'loss': 1.101788111708381, 'acc': 0.6767676767676768}\n",
      "Test dataset results: \n",
      "{'loss': 1.0310113751125656, 'acc': 0.6978114478114478}\n",
      "Test dataset results: \n",
      "{'loss': 0.9533368814673889, 'acc': 0.6952861952861953}\n",
      "Test dataset results: \n",
      "{'loss': 1.1044683753440676, 'acc': 0.6548821548821548}\n",
      "Test dataset results: \n",
      "{'loss': 1.2264056508789962, 'acc': 0.6548821548821548}\n",
      "Test dataset results: \n",
      "{'loss': 1.1320213245020971, 'acc': 0.6599326599326599}\n",
      "Test dataset results: \n",
      "{'loss': 0.8557626650188909, 'acc': 0.7138047138047138}\n",
      "Test dataset results: \n",
      "{'loss': 0.9520270863566735, 'acc': 0.6944444444444444}\n",
      "Test dataset results: \n",
      "{'loss': 1.0206874994316486, 'acc': 0.6868686868686869}\n",
      "Test dataset results: \n",
      "{'loss': 0.9977964975617148, 'acc': 0.6851851851851852}\n",
      "Test dataset results: \n",
      "{'loss': 1.189807395341019, 'acc': 0.6498316498316499}\n",
      "Test dataset results: \n",
      "{'loss': 0.9032034387291481, 'acc': 0.6952861952861953}\n",
      "Test dataset results: \n",
      "{'loss': 1.2049496399633812, 'acc': 0.680976430976431}\n",
      "Test dataset results: \n",
      "{'loss': 1.3756478903470215, 'acc': 0.6245791245791246}\n",
      "Test dataset results: \n",
      "{'loss': 0.9572988607064642, 'acc': 0.6835016835016835}\n",
      "0.04 0.6753647586980922 0.019279994279689922\n",
      "\n",
      "Crowd noise adaptation model with 0.04 clean data\n",
      "Test dataset results: \n",
      "{'loss': -7.248805543790361, 'crowds_classification_181_loss': -7.248805543790361, 'baseline_loss': 0.8777494676386066, 'crowds_classification_181_acc': 0.023463804713804715, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -7.2475383402121185, 'crowds_classification_182_loss': -7.2475383402121185, 'baseline_loss': 0.687482129531577, 'crowds_classification_182_acc': 0.03019781144781145, 'baseline_acc': 0.8493265993265994}\n",
      "Test dataset results: \n",
      "{'loss': -7.279508815469967, 'crowds_classification_183_loss': -7.279508815469967, 'baseline_loss': 0.8440089181617454, 'crowds_classification_183_acc': 0.030303030303030304, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -7.177326197576041, 'crowds_classification_184_loss': -7.177326197576041, 'baseline_loss': 0.7618717040979501, 'crowds_classification_184_acc': 0.029566498316498317, 'baseline_acc': 0.8442760942760943}\n",
      "Test dataset results: \n",
      "{'loss': -7.078173309865624, 'crowds_classification_185_loss': -7.078173309865624, 'baseline_loss': 2.3927730829627425, 'crowds_classification_185_acc': 0.02988215488215488, 'baseline_acc': 0.7449494949494949}\n",
      "Test dataset results: \n",
      "{'loss': -7.327426053056813, 'crowds_classification_186_loss': -7.327426053056813, 'baseline_loss': 0.7563700444419376, 'crowds_classification_186_acc': 0.03135521885521886, 'baseline_acc': 0.8644781144781145}\n",
      "Test dataset results: \n",
      "{'loss': -7.3185454201618025, 'crowds_classification_187_loss': -7.3185454201618025, 'baseline_loss': 0.7489127398841711, 'crowds_classification_187_acc': 0.02241161616161616, 'baseline_acc': 0.8484848484848485}\n",
      "Test dataset results: \n",
      "{'loss': -7.306246229293772, 'crowds_classification_188_loss': -7.306246229293772, 'baseline_loss': 0.666436516551257, 'crowds_classification_188_acc': 0.019886363636363636, 'baseline_acc': 0.8560606060606061}\n",
      "Test dataset results: \n",
      "{'loss': -7.220657740377818, 'crowds_classification_189_loss': -7.220657740377818, 'baseline_loss': 0.6717674248171375, 'crowds_classification_189_acc': 0.032196969696969696, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.29101794657081, 'crowds_classification_190_loss': -7.29101794657081, 'baseline_loss': 0.6026592873725162, 'crowds_classification_190_acc': 0.0022095959595959595, 'baseline_acc': 0.8459595959595959}\n",
      "Test dataset results: \n",
      "{'loss': -7.065021962830515, 'crowds_classification_191_loss': -7.065021962830515, 'baseline_loss': 2.3382235531207574, 'crowds_classification_191_acc': 0.003472222222222222, 'baseline_acc': 0.7626262626262627}\n",
      "Test dataset results: \n",
      "{'loss': -7.2351036730037395, 'crowds_classification_192_loss': -7.2351036730037395, 'baseline_loss': 0.6970929757312492, 'crowds_classification_192_acc': 0.032512626262626264, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -7.193338864580148, 'crowds_classification_193_loss': -7.193338864580148, 'baseline_loss': 1.1768842688714616, 'crowds_classification_193_acc': 0.021464646464646464, 'baseline_acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': -7.1098667812668515, 'crowds_classification_194_loss': -7.1098667812668515, 'baseline_loss': 0.5707873632686106, 'crowds_classification_194_acc': 0.03135521885521886, 'baseline_acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': -7.356620759674997, 'crowds_classification_195_loss': -7.356620759674997, 'baseline_loss': 0.570106529464525, 'crowds_classification_195_acc': 0.02356902356902357, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -7.126163331747858, 'crowds_classification_196_loss': -7.126163331747858, 'baseline_loss': 0.7542366774545776, 'crowds_classification_196_acc': 0.009574915824915825, 'baseline_acc': 0.8240740740740741}\n",
      "Test dataset results: \n",
      "{'loss': -7.1851729672364515, 'crowds_classification_197_loss': -7.1851729672364515, 'baseline_loss': 0.7797725991618754, 'crowds_classification_197_acc': 0.00484006734006734, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -7.31531927802346, 'crowds_classification_198_loss': -7.31531927802346, 'baseline_loss': 0.7067424040099587, 'crowds_classification_198_acc': 0.028935185185185185, 'baseline_acc': 0.8468013468013468}\n",
      "Test dataset results: \n",
      "{'loss': -7.223228619957612, 'crowds_classification_199_loss': -7.223228619957612, 'baseline_loss': 0.693476876726857, 'crowds_classification_199_acc': 0.03188131313131313, 'baseline_acc': 0.8468013468013468}\n",
      "Test dataset results: \n",
      "{'loss': -7.155708579503326, 'crowds_classification_200_loss': -7.155708579503326, 'baseline_loss': 2.7223875128259563, 'crowds_classification_200_acc': 0.0, 'baseline_acc': 0.7281144781144782}\n",
      "Test dataset results: \n",
      "{'loss': -7.197745209189778, 'crowds_classification_201_loss': -7.197745209189778, 'baseline_loss': 0.6631476699377752, 'crowds_classification_201_acc': 0.025147306397306397, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.238020603102867, 'crowds_classification_202_loss': -7.238020603102867, 'baseline_loss': 0.6158131912149966, 'crowds_classification_202_acc': 0.022937710437710437, 'baseline_acc': 0.8442760942760943}\n",
      "Test dataset results: \n",
      "{'loss': -7.158113200255115, 'crowds_classification_203_loss': -7.158113200255115, 'baseline_loss': 2.6249636387584183, 'crowds_classification_203_acc': 0.000946969696969697, 'baseline_acc': 0.7390572390572391}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.241660802051275, 'crowds_classification_204_loss': -7.241660802051275, 'baseline_loss': 0.8776078217659735, 'crowds_classification_204_acc': 0.011153198653198653, 'baseline_acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': -7.099870204925537, 'crowds_classification_205_loss': -7.099870204925537, 'baseline_loss': 0.8232337474220931, 'crowds_classification_205_acc': 0.007891414141414142, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -7.246424851594148, 'crowds_classification_206_loss': -7.246424851594148, 'baseline_loss': 0.9811092741059936, 'crowds_classification_206_acc': 0.029356060606060608, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -7.1951862922822585, 'crowds_classification_207_loss': -7.1951862922822585, 'baseline_loss': 0.9854209235420933, 'crowds_classification_207_acc': 0.030934343434343436, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -7.173074221370196, 'crowds_classification_208_loss': -7.173074221370196, 'baseline_loss': 2.6103102606956403, 'crowds_classification_208_acc': 0.003682659932659933, 'baseline_acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': -7.183265098417648, 'crowds_classification_209_loss': -7.183265098417648, 'baseline_loss': 0.8730872983382607, 'crowds_classification_209_acc': 0.0012626262626262627, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -7.239652094214853, 'crowds_classification_210_loss': -7.239652094214853, 'baseline_loss': 0.9388410329618037, 'crowds_classification_210_acc': 0.03272306397306397, 'baseline_acc': 0.8341750841750841}\n",
      "0.04 0.8229236812570144 0.038910675589892425\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.04 clean data\n",
      "Test dataset results: \n",
      "{'loss': -6.9173583823823765, 'crowds_classification_211_loss': -6.9173583823823765, 'baseline_loss': 1.0487028210853486, 'crowds_classification_211_acc': 0.0031565656565656565, 'baseline_acc': 0.8215488215488216}\n",
      "Test dataset results: \n",
      "{'loss': -6.862828070065791, 'crowds_classification_212_loss': -6.862828070065791, 'baseline_loss': 0.7946573980027176, 'crowds_classification_212_acc': 0.013257575757575758, 'baseline_acc': 0.8459595959595959}\n",
      "Test dataset results: \n",
      "{'loss': -6.864934497409397, 'crowds_classification_213_loss': -6.864934497409397, 'baseline_loss': 0.7203665539271101, 'crowds_classification_213_acc': 0.02199074074074074, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -7.04116046308267, 'crowds_classification_214_loss': -7.04116046308267, 'baseline_loss': 0.9688487461821399, 'crowds_classification_214_acc': 0.03125, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -6.851820790005052, 'crowds_classification_215_loss': -6.851820790005052, 'baseline_loss': 1.1152436945976232, 'crowds_classification_215_acc': 0.011047979797979798, 'baseline_acc': 0.8173400673400674}\n",
      "Test dataset results: \n",
      "{'loss': -7.016260834253998, 'crowds_classification_216_loss': -7.016260834253998, 'baseline_loss': 1.3443006939358182, 'crowds_classification_216_acc': 0.03324915824915825, 'baseline_acc': 0.8173400673400674}\n",
      "Test dataset results: \n",
      "{'loss': -6.8885175017796785, 'crowds_classification_217_loss': -6.8885175017796785, 'baseline_loss': 1.061592497734309, 'crowds_classification_217_acc': 0.02556818181818182, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -7.001734284038094, 'crowds_classification_218_loss': -7.001734284038094, 'baseline_loss': 0.9318593388253992, 'crowds_classification_218_acc': 0.032617845117845115, 'baseline_acc': 0.8400673400673401}\n",
      "Test dataset results: \n",
      "{'loss': -6.902012080054492, 'crowds_classification_219_loss': -6.902012080054492, 'baseline_loss': 1.1429445780688263, 'crowds_classification_219_acc': 0.023148148148148147, 'baseline_acc': 0.8198653198653199}\n",
      "Test dataset results: \n",
      "{'loss': -6.967509046548144, 'crowds_classification_220_loss': -6.967509046548144, 'baseline_loss': 1.0097417801819266, 'crowds_classification_220_acc': 0.03377525252525253, 'baseline_acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': -7.0448019239637585, 'crowds_classification_221_loss': -7.0448019239637585, 'baseline_loss': 1.0682954083413188, 'crowds_classification_221_acc': 0.03282828282828283, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -6.8952627390723435, 'crowds_classification_222_loss': -6.8952627390723435, 'baseline_loss': 1.060528356117833, 'crowds_classification_222_acc': 0.011784511784511785, 'baseline_acc': 0.8156565656565656}\n",
      "Test dataset results: \n",
      "{'loss': -6.978322802970706, 'crowds_classification_223_loss': -6.978322802970706, 'baseline_loss': 1.124956078802295, 'crowds_classification_223_acc': 0.03303872053872054, 'baseline_acc': 0.8198653198653199}\n",
      "Test dataset results: \n",
      "{'loss': -6.838602623152813, 'crowds_classification_224_loss': -6.838602623152813, 'baseline_loss': 0.9349577102797602, 'crowds_classification_224_acc': 0.0017887205387205388, 'baseline_acc': 0.8249158249158249}\n",
      "Test dataset results: \n",
      "{'loss': -6.8986336422287655, 'crowds_classification_225_loss': -6.8986336422287655, 'baseline_loss': 0.8453108787298302, 'crowds_classification_225_acc': 0.021254208754208755, 'baseline_acc': 0.8468013468013468}\n",
      "Test dataset results: \n",
      "{'loss': -6.886563575629032, 'crowds_classification_226_loss': -6.886563575629032, 'baseline_loss': 0.9561931978070776, 'crowds_classification_226_acc': 0.023989898989898988, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -6.9377507585467715, 'crowds_classification_227_loss': -6.9377507585467715, 'baseline_loss': 1.202706526425551, 'crowds_classification_227_acc': 0.03272306397306397, 'baseline_acc': 0.8114478114478114}\n",
      "Test dataset results: \n",
      "{'loss': -6.928038129902849, 'crowds_classification_228_loss': -6.928038129902849, 'baseline_loss': 0.9243155745946197, 'crowds_classification_228_acc': 0.002946127946127946, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -6.935055735938075, 'crowds_classification_229_loss': -6.935055735938075, 'baseline_loss': 0.8761965946454291, 'crowds_classification_229_acc': 0.027462121212121212, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.94243014942516, 'crowds_classification_230_loss': -6.94243014942516, 'baseline_loss': 1.0763023417405408, 'crowds_classification_230_acc': 0.022832491582491583, 'baseline_acc': 0.8207070707070707}\n",
      "Test dataset results: \n",
      "{'loss': -6.9228090003684715, 'crowds_classification_231_loss': -6.9228090003684715, 'baseline_loss': 0.9591659349624557, 'crowds_classification_231_acc': 0.008733164983164983, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -7.063278007186222, 'crowds_classification_232_loss': -7.063278007186222, 'baseline_loss': 0.9344084272223842, 'crowds_classification_232_acc': 0.03324915824915825, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -6.915334101076479, 'crowds_classification_233_loss': -6.915334101076479, 'baseline_loss': 0.8670446869199123, 'crowds_classification_233_acc': 0.0242003367003367, 'baseline_acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': -6.8888930118445195, 'crowds_classification_234_loss': -6.8888930118445195, 'baseline_loss': 0.8450513276701122, 'crowds_classification_234_acc': 0.030513468013468013, 'baseline_acc': 0.8468013468013468}\n",
      "Test dataset results: \n",
      "{'loss': -6.869412293739191, 'crowds_classification_235_loss': -6.869412293739191, 'baseline_loss': 1.2944425890341351, 'crowds_classification_235_acc': 0.001473063973063973, 'baseline_acc': 0.8122895622895623}\n",
      "Test dataset results: \n",
      "{'loss': -6.869786334760262, 'crowds_classification_236_loss': -6.869786334760262, 'baseline_loss': 1.0824758565987802, 'crowds_classification_236_acc': 0.0032617845117845117, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -6.821102396004931, 'crowds_classification_237_loss': -6.821102396004931, 'baseline_loss': 0.9356800208388756, 'crowds_classification_237_acc': 0.022095959595959596, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.870307850115227, 'crowds_classification_238_loss': -6.870307850115227, 'baseline_loss': 0.8317535146694592, 'crowds_classification_238_acc': 0.001893939393939394, 'baseline_acc': 0.8526936026936027}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -6.8719407605001015, 'crowds_classification_239_loss': -6.8719407605001015, 'baseline_loss': 1.023356152855186, 'crowds_classification_239_acc': 0.0021043771043771043, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -6.983989890577014, 'crowds_classification_240_loss': -6.983989890577014, 'baseline_loss': 0.985672834545675, 'crowds_classification_240_acc': 0.029671717171717172, 'baseline_acc': 0.8282828282828283}\n",
      "0.04 0.8314814814814813 0.011611687682493883\n",
      "\n",
      "Baseline model with 0.05 clean data\n",
      "Test dataset results: \n",
      "{'loss': 0.8376661866702616, 'acc': 0.7407407407407407}\n",
      "Test dataset results: \n",
      "{'loss': 0.8799682637858471, 'acc': 0.7239057239057239}\n",
      "Test dataset results: \n",
      "{'loss': 0.8339177880825017, 'acc': 0.7558922558922558}\n",
      "Test dataset results: \n",
      "{'loss': 0.764257732866589, 'acc': 0.7558922558922558}\n",
      "Test dataset results: \n",
      "{'loss': 0.871265344748192, 'acc': 0.7314814814814815}\n",
      "Test dataset results: \n",
      "{'loss': 0.8931780842819599, 'acc': 0.73989898989899}\n",
      "Test dataset results: \n",
      "{'loss': 0.7993934526036083, 'acc': 0.7542087542087542}\n",
      "Test dataset results: \n",
      "{'loss': 0.818156721967238, 'acc': 0.7584175084175084}\n",
      "Test dataset results: \n",
      "{'loss': 0.8171426949878333, 'acc': 0.7323232323232324}\n",
      "Test dataset results: \n",
      "{'loss': 0.7691745292458069, 'acc': 0.7609427609427609}\n",
      "Test dataset results: \n",
      "{'loss': 0.9521239475168363, 'acc': 0.7297979797979798}\n",
      "Test dataset results: \n",
      "{'loss': 0.8189090965974211, 'acc': 0.75}\n",
      "Test dataset results: \n",
      "{'loss': 0.7811304011730232, 'acc': 0.7584175084175084}\n",
      "Test dataset results: \n",
      "{'loss': 0.788679488780924, 'acc': 0.7508417508417509}\n",
      "Test dataset results: \n",
      "{'loss': 0.824925716075833, 'acc': 0.75}\n",
      "Test dataset results: \n",
      "{'loss': 0.8797091703723978, 'acc': 0.7281144781144782}\n",
      "Test dataset results: \n",
      "{'loss': 0.8956628738830387, 'acc': 0.7432659932659933}\n",
      "Test dataset results: \n",
      "{'loss': 0.8685850357567823, 'acc': 0.7407407407407407}\n",
      "Test dataset results: \n",
      "{'loss': 0.8081492657211895, 'acc': 0.7558922558922558}\n",
      "Test dataset results: \n",
      "{'loss': 0.8871600371518922, 'acc': 0.7365319865319865}\n",
      "Test dataset results: \n",
      "{'loss': 0.8442756292795894, 'acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': 0.8080859862594091, 'acc': 0.7407407407407407}\n",
      "Test dataset results: \n",
      "{'loss': 0.8055988853477468, 'acc': 0.7609427609427609}\n",
      "Test dataset results: \n",
      "{'loss': 0.8250714557540135, 'acc': 0.76010101010101}\n",
      "Test dataset results: \n",
      "{'loss': 0.880792126611427, 'acc': 0.7348484848484849}\n",
      "Test dataset results: \n",
      "{'loss': 1.0306564895592956, 'acc': 0.7239057239057239}\n",
      "Test dataset results: \n",
      "{'loss': 0.9657332345793143, 'acc': 0.7441077441077442}\n",
      "Test dataset results: \n",
      "{'loss': 0.8127865242436277, 'acc': 0.7567340067340067}\n",
      "Test dataset results: \n",
      "{'loss': 0.9356021474206487, 'acc': 0.7382154882154882}\n",
      "Test dataset results: \n",
      "{'loss': 0.9348676021050926, 'acc': 0.7356902356902357}\n",
      "0.05 0.7450897867564533 0.011730442203696234\n",
      "\n",
      "Crowd noise adaptation model with 0.05 clean data\n",
      "Test dataset results: \n",
      "{'loss': -7.311930226155805, 'crowds_classification_241_loss': -7.311930226155805, 'baseline_loss': 0.7601748743731065, 'crowds_classification_241_acc': 0.03303872053872054, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -7.244172243959574, 'crowds_classification_242_loss': -7.244172243959574, 'baseline_loss': 2.4074218939450454, 'crowds_classification_242_acc': 0.020622895622895623, 'baseline_acc': 0.7474747474747475}\n",
      "Test dataset results: \n",
      "{'loss': -7.153905764171973, 'crowds_classification_243_loss': -7.153905764171973, 'baseline_loss': 0.6072921119128614, 'crowds_classification_243_acc': 0.03103956228956229, 'baseline_acc': 0.8602693602693603}\n",
      "Test dataset results: \n",
      "{'loss': -7.285238760489005, 'crowds_classification_244_loss': -7.285238760489005, 'baseline_loss': 0.7366891248992046, 'crowds_classification_244_acc': 0.021675084175084174, 'baseline_acc': 0.8535353535353535}\n",
      "Test dataset results: \n",
      "{'loss': -7.228198806043426, 'crowds_classification_245_loss': -7.228198806043426, 'baseline_loss': 0.8286507538071385, 'crowds_classification_245_acc': 0.02135942760942761, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -7.1503324428391375, 'crowds_classification_246_loss': -7.1503324428391375, 'baseline_loss': 0.7559149963185442, 'crowds_classification_246_acc': 0.03167087542087542, 'baseline_acc': 0.82996632996633}\n",
      "Test dataset results: \n",
      "{'loss': -7.2043471432695485, 'crowds_classification_247_loss': -7.2043471432695485, 'baseline_loss': 0.5554884904713342, 'crowds_classification_247_acc': 0.02640993265993266, 'baseline_acc': 0.8518518518518519}\n",
      "Test dataset results: \n",
      "{'loss': -7.242647887079001, 'crowds_classification_248_loss': -7.242647887079001, 'baseline_loss': 0.6752927496367658, 'crowds_classification_248_acc': 0.030829124579124578, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.270952362805504, 'crowds_classification_249_loss': -7.270952362805504, 'baseline_loss': 0.6418235234662213, 'crowds_classification_249_acc': 0.03125, 'baseline_acc': 0.8552188552188552}\n",
      "Test dataset results: \n",
      "{'loss': -7.2446635499947805, 'crowds_classification_250_loss': -7.2446635499947805, 'baseline_loss': 0.8319635437393831, 'crowds_classification_250_acc': 0.03356481481481482, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -7.301690586488255, 'crowds_classification_251_loss': -7.301690586488255, 'baseline_loss': 0.7393119168602658, 'crowds_classification_251_acc': 0.030829124579124578, 'baseline_acc': 0.8501683501683501}\n",
      "Test dataset results: \n",
      "{'loss': -7.107760243142895, 'crowds_classification_252_loss': -7.107760243142895, 'baseline_loss': 0.9462845581348496, 'crowds_classification_252_acc': 0.03409090909090909, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -6.92428856666642, 'crowds_classification_253_loss': -6.92428856666642, 'baseline_loss': 6.508039269987525, 'crowds_classification_253_acc': 0.020622895622895623, 'baseline_acc': 0.5496632996632996}\n",
      "Test dataset results: \n",
      "{'loss': -7.033436182773475, 'crowds_classification_254_loss': -7.033436182773475, 'baseline_loss': 1.9093763876442957, 'crowds_classification_254_acc': 0.02251683501683502, 'baseline_acc': 0.7516835016835017}\n",
      "Test dataset results: \n",
      "{'loss': -7.3021097568550495, 'crowds_classification_255_loss': -7.3021097568550495, 'baseline_loss': 0.7978346715822364, 'crowds_classification_255_acc': 0.026304713804713806, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -7.3060788755063655, 'crowds_classification_256_loss': -7.3060788755063655, 'baseline_loss': 0.7069776408399395, 'crowds_classification_256_acc': 0.02220117845117845, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -7.322850066804725, 'crowds_classification_257_loss': -7.322850066804725, 'baseline_loss': 0.6293726691643798, 'crowds_classification_257_acc': 0.03009259259259259, 'baseline_acc': 0.8627946127946128}\n",
      "Test dataset results: \n",
      "{'loss': -7.1810496214664346, 'crowds_classification_258_loss': -7.1810496214664346, 'baseline_loss': 0.8177591167415432, 'crowds_classification_258_acc': 0.03156565656565657, 'baseline_acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': -7.298004076537058, 'crowds_classification_259_loss': -7.298004076537058, 'baseline_loss': 0.7593166576039911, 'crowds_classification_259_acc': 0.03314393939393939, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.295189873538034, 'crowds_classification_260_loss': -7.295189873538034, 'baseline_loss': 0.7140333061216255, 'crowds_classification_260_acc': 0.029356060606060608, 'baseline_acc': 0.8585858585858586}\n",
      "Test dataset results: \n",
      "{'loss': -7.334442263901836, 'crowds_classification_261_loss': -7.334442263901836, 'baseline_loss': 0.7738096747133467, 'crowds_classification_261_acc': 0.022306397306397305, 'baseline_acc': 0.8383838383838383}\n",
      "Test dataset results: \n",
      "{'loss': -7.1596493656787805, 'crowds_classification_262_loss': -7.1596493656787805, 'baseline_loss': 2.2782022227856147, 'crowds_classification_262_acc': 0.022832491582491583, 'baseline_acc': 0.7592592592592593}\n",
      "Test dataset results: \n",
      "{'loss': -7.327121028193721, 'crowds_classification_263_loss': -7.327121028193721, 'baseline_loss': 0.8010598227933601, 'crowds_classification_263_acc': 0.03345959595959596, 'baseline_acc': 0.8333333333333334}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -7.3450520030577175, 'crowds_classification_264_loss': -7.3450520030577175, 'baseline_loss': 0.5978403608957525, 'crowds_classification_264_acc': 0.030618686868686868, 'baseline_acc': 0.8602693602693603}\n",
      "Test dataset results: \n",
      "{'loss': -7.199581159084333, 'crowds_classification_265_loss': -7.199581159084333, 'baseline_loss': 0.8075601693355676, 'crowds_classification_265_acc': 0.028409090909090908, 'baseline_acc': 0.8173400673400674}\n",
      "Test dataset results: \n",
      "{'loss': -7.378962532839791, 'crowds_classification_266_loss': -7.378962532839791, 'baseline_loss': 0.7008813756082194, 'crowds_classification_266_acc': 0.030303030303030304, 'baseline_acc': 0.8417508417508418}\n",
      "Test dataset results: \n",
      "{'loss': -7.277306148901531, 'crowds_classification_267_loss': -7.277306148901531, 'baseline_loss': 0.6840198833231974, 'crowds_classification_267_acc': 0.03177609427609428, 'baseline_acc': 0.8526936026936027}\n",
      "Test dataset results: \n",
      "{'loss': -7.04713297693015, 'crowds_classification_268_loss': -7.04713297693015, 'baseline_loss': 2.496400401723706, 'crowds_classification_268_acc': 0.001893939393939394, 'baseline_acc': 0.7508417508417509}\n",
      "Test dataset results: \n",
      "{'loss': -7.3309922121992015, 'crowds_classification_269_loss': -7.3309922121992015, 'baseline_loss': 0.84376018475553, 'crowds_classification_269_acc': 0.022306397306397305, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -7.327184977354826, 'crowds_classification_270_loss': -7.327184977354826, 'baseline_loss': 0.749288583293507, 'crowds_classification_270_acc': 0.03409090909090909, 'baseline_acc': 0.8468013468013468}\n",
      "0.05 0.8223905723905722 0.06044646338738149\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.05 clean data\n",
      "Test dataset results: \n",
      "{'loss': -6.903894588200733, 'crowds_classification_271_loss': -6.903894588200733, 'baseline_loss': 1.2572663835804871, 'crowds_classification_271_acc': 0.022832491582491583, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -6.908272672582556, 'crowds_classification_272_loss': -6.908272672582556, 'baseline_loss': 1.0335974563352186, 'crowds_classification_272_acc': 0.0032617845117845117, 'baseline_acc': 0.8392255892255892}\n",
      "Test dataset results: \n",
      "{'loss': -6.84031572085037, 'crowds_classification_273_loss': -6.84031572085037, 'baseline_loss': 1.1387074639901569, 'crowds_classification_273_acc': 0.01210016835016835, 'baseline_acc': 0.8122895622895623}\n",
      "Test dataset results: \n",
      "{'loss': -7.0701904457426235, 'crowds_classification_274_loss': -7.0701904457426235, 'baseline_loss': 0.9110210727310742, 'crowds_classification_274_acc': 0.03356481481481482, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -6.850940413747974, 'crowds_classification_275_loss': -6.850940413747974, 'baseline_loss': 0.9108830609606573, 'crowds_classification_275_acc': 0.011784511784511785, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -7.03325890852546, 'crowds_classification_276_loss': -7.03325890852546, 'baseline_loss': 1.1287511824959455, 'crowds_classification_276_acc': 0.032617845117845115, 'baseline_acc': 0.8291245791245792}\n",
      "Test dataset results: \n",
      "{'loss': -6.845840386670045, 'crowds_classification_277_loss': -6.845840386670045, 'baseline_loss': 0.9770038999140463, 'crowds_classification_277_acc': 0.0022095959595959595, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -6.994199728724932, 'crowds_classification_278_loss': -6.994199728724932, 'baseline_loss': 1.0175547521523756, 'crowds_classification_278_acc': 0.03135521885521886, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.882295878246577, 'crowds_classification_279_loss': -6.882295878246577, 'baseline_loss': 0.9879568977789446, 'crowds_classification_279_acc': 0.022937710437710437, 'baseline_acc': 0.8358585858585859}\n",
      "Test dataset results: \n",
      "{'loss': -6.847098957408559, 'crowds_classification_280_loss': -6.847098957408559, 'baseline_loss': 0.8820792783290048, 'crowds_classification_280_acc': 0.006207912457912458, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -7.011337556421556, 'crowds_classification_281_loss': -7.011337556421556, 'baseline_loss': 1.0538002053896587, 'crowds_classification_281_acc': 0.03103956228956229, 'baseline_acc': 0.8324915824915825}\n",
      "Test dataset results: \n",
      "{'loss': -7.001291653925321, 'crowds_classification_282_loss': -7.001291653925321, 'baseline_loss': 0.973421654610672, 'crowds_classification_282_acc': 0.032196969696969696, 'baseline_acc': 0.8425925925925926}\n",
      "Test dataset results: \n",
      "{'loss': -6.857313958884088, 'crowds_classification_283_loss': -6.857313958884088, 'baseline_loss': 0.9494690413928594, 'crowds_classification_283_acc': 0.021885521885521887, 'baseline_acc': 0.8375420875420876}\n",
      "Test dataset results: \n",
      "{'loss': -6.848166905669652, 'crowds_classification_284_loss': -6.848166905669652, 'baseline_loss': 0.9341040773925556, 'crowds_classification_284_acc': 0.000946969696969697, 'baseline_acc': 0.8308080808080808}\n",
      "Test dataset results: \n",
      "{'loss': -6.952722899440162, 'crowds_classification_285_loss': -6.952722899440162, 'baseline_loss': 0.9789297283197493, 'crowds_classification_285_acc': 0.03019781144781145, 'baseline_acc': 0.8274410774410774}\n",
      "Test dataset results: \n",
      "{'loss': -6.999478333726876, 'crowds_classification_286_loss': -6.999478333726876, 'baseline_loss': 1.1982189400830252, 'crowds_classification_286_acc': 0.030618686868686868, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -6.886408969609424, 'crowds_classification_287_loss': -6.886408969609424, 'baseline_loss': 0.8675163876428347, 'crowds_classification_287_acc': 0.0006313131313131314, 'baseline_acc': 0.8434343434343434}\n",
      "Test dataset results: \n",
      "{'loss': -6.996996776824848, 'crowds_classification_288_loss': -6.996996776824848, 'baseline_loss': 0.9514774433791838, 'crowds_classification_288_acc': 0.034827441077441075, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -6.843897455067747, 'crowds_classification_289_loss': -6.843897455067747, 'baseline_loss': 0.9815296020973411, 'crowds_classification_289_acc': 0.025147306397306397, 'baseline_acc': 0.8282828282828283}\n",
      "Test dataset results: \n",
      "{'loss': -6.814072673168247, 'crowds_classification_290_loss': -6.814072673168247, 'baseline_loss': 0.9252611529019344, 'crowds_classification_290_acc': 0.005892255892255892, 'baseline_acc': 0.8367003367003367}\n",
      "Test dataset results: \n",
      "{'loss': -6.978894395860357, 'crowds_classification_291_loss': -6.978894395860357, 'baseline_loss': 1.0668148506771435, 'crowds_classification_291_acc': 0.03314393939393939, 'baseline_acc': 0.819023569023569}\n",
      "Test dataset results: \n",
      "{'loss': -6.8691427667534315, 'crowds_classification_292_loss': -6.8691427667534315, 'baseline_loss': 1.2811905810929307, 'crowds_classification_292_acc': 0.023674242424242424, 'baseline_acc': 0.8080808080808081}\n",
      "Test dataset results: \n",
      "{'loss': -6.980475539711589, 'crowds_classification_293_loss': -6.980475539711589, 'baseline_loss': 1.2419756755282982, 'crowds_classification_293_acc': 0.037037037037037035, 'baseline_acc': 0.8122895622895623}\n",
      "Test dataset results: \n",
      "{'loss': -6.98449918316671, 'crowds_classification_294_loss': -6.98449918316671, 'baseline_loss': 0.8419527993584522, 'crowds_classification_294_acc': 0.010521885521885523, 'baseline_acc': 0.8476430976430976}\n",
      "Test dataset results: \n",
      "{'loss': -6.841473794545389, 'crowds_classification_295_loss': -6.841473794545389, 'baseline_loss': 0.9662634127268486, 'crowds_classification_295_acc': 0.02220117845117845, 'baseline_acc': 0.8257575757575758}\n",
      "Test dataset results: \n",
      "{'loss': -6.855213062530415, 'crowds_classification_296_loss': -6.855213062530415, 'baseline_loss': 0.8482833918723394, 'crowds_classification_296_acc': 0.020412457912457913, 'baseline_acc': 0.8341750841750841}\n",
      "Test dataset results: \n",
      "{'loss': -7.037935974622013, 'crowds_classification_297_loss': -7.037935974622013, 'baseline_loss': 0.9880409881422415, 'crowds_classification_297_acc': 0.034722222222222224, 'baseline_acc': 0.8316498316498316}\n",
      "Test dataset results: \n",
      "{'loss': -7.027776910801126, 'crowds_classification_298_loss': -7.027776910801126, 'baseline_loss': 0.9475473867798293, 'crowds_classification_298_acc': 0.030513468013468013, 'baseline_acc': 0.8291245791245792}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset results: \n",
      "{'loss': -6.964468196586326, 'crowds_classification_299_loss': -6.964468196586326, 'baseline_loss': 0.8536269677934623, 'crowds_classification_299_acc': 0.028935185185185185, 'baseline_acc': 0.8451178451178452}\n",
      "Test dataset results: \n",
      "{'loss': -6.829018372879285, 'crowds_classification_300_loss': -6.829018372879285, 'baseline_loss': 0.8766870470560761, 'crowds_classification_300_acc': 0.022095959595959596, 'baseline_acc': 0.8434343434343434}\n",
      "0.05 0.8316217732884397 0.01008576134200633\n"
     ]
    }
   ],
   "source": [
    "sample_base_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "sample_base_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "mix_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "mix_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "mix_trace_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "pretrain_acc_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "pretrain_loss_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "pretrain_trace_dict = {0.01:[], 0.02:[], 0.03:[], 0.04:[], 0.05:[]}\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        clean_history, mets = baseline_gt(x_sample[int(clean_percent*100)], y_sample[int(clean_percent*100)], N_CLASSES, model_dir)\n",
    "        sample_base_acc_dict[clean_percent] = clean_history.history['acc']\n",
    "        sample_base_loss_dict[clean_percent] = clean_history.history['loss']\n",
    "        test_acc_list.append(mets['acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'sample_base_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "    print(clean_percent, acc_mean[model_desc], acc_std[model_desc])\n",
    "            \n",
    "    print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        history, trace_arr, mets = crowd_model(x, y_gt, y_annot_mix[int(clean_percent*100)], N_CLASSES, False, True, model_dir)\n",
    "        mix_acc_dict[clean_percent] = history.history['baseline_acc']\n",
    "        mix_loss_dict[clean_percent] = history.history['baseline_loss']\n",
    "        mix_trace_dict[clean_percent] = trace_arr\n",
    "        test_acc_list.append(mets['baseline_acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'mix_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "    print(clean_percent, acc_mean[model_desc], acc_std[model_desc])\n",
    "    \n",
    "    print('\\nCrowd noise adaptation model pretrain with %.2f clean data' % (clean_percent))\n",
    "    test_acc_list = []\n",
    "    for i in range(N_RUNS):\n",
    "        history, trace_arr, mets = crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample[int(clean_percent*100)], y_sample[int(clean_percent*100)], N_CLASSES, False, True, model_dir)\n",
    "        pretrain_trace_dict[clean_percent] = history.history['baseline_acc']\n",
    "        pretrain_trace_dict[clean_percent] = history.history['baseline_loss'] \n",
    "        pretrain_trace_dict[clean_percent] = trace_arr\n",
    "        test_acc_list.append(mets['baseline_acc'])\n",
    "    test_acc = np.array(test_acc_list)\n",
    "    model_desc = 'pretrain_%s'%clean_percent\n",
    "    acc_mean[model_desc] = test_acc.mean()\n",
    "    acc_std[model_desc] = test_acc.std()\n",
    "    print(clean_percent, acc_mean[model_desc], acc_std[model_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
