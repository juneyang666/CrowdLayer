{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yajingyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, merge, Reshape, Permute, Multiply, Dot,dot, Concatenate, Add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy, CrowdsClassificationSModel, \\\n",
    "    CrowdsClassificationCModelSingleWeight, CrowdsClassificationCModel, MaskedMultiCrossEntropyCosSim, \\\n",
    "    MaskedMultiCrossEntropyBaseChannel, MaskedMultiCrossEntropyBaseChannelConst, CrowdsClassificationSModelChannelMatrix, \\\n",
    "    MaskedMultiCrossEntropyCurriculumChannelMatrix\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = np.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets\n",
    "\n",
    "def get_data(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train_vgg16.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test_vgg16.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid_vgg16.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "    return x, y_gt, y_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def eval(model,x_test, y_test):\n",
    "    print('Test dataset results: ')\n",
    "    print(dict(zip(model.metrics_names,model.evaluate(x_test,y_test, verbose=False))))\n",
    "\n",
    "\n",
    "def get_trace(model):\n",
    "\n",
    "    channel_matrix = model.get_weights()[-1]\n",
    "    channel_matrix_trace = tf.trace(K.permute_dimensions(channel_matrix, [2, 0, 1]))\n",
    "    channel_matrix_trace_arr = K.eval(channel_matrix_trace)\n",
    "    return channel_matrix_trace_arr\n",
    "\n",
    "\n",
    "def print_single_loss(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['baseline_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['baseline_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_history(df, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Make a data frame\n",
    "    df['x'] = range(df.shape[0])\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    # multiple line plot\n",
    "    num = 0\n",
    "    for column in df.drop('x', axis=1):\n",
    "        num += 1\n",
    "        plt.plot(df['x'], df[column], marker='', color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=2)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(train_data_shape, N_CLASSES):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Dense(128, activation='relu'))\n",
    "    base_model.add(Dropout(0.4))\n",
    "    base_model.add(Dense(64, activation='relu'))\n",
    "    base_model.add(Dropout(0.4))\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation(\"softmax\"))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def majority_vote(x, y_gt, y_annot, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_annot['mv'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_gt(x, y_gt, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_gt['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    hidden_layers = Sequential()\n",
    "    hidden_layers.add(Flatten(input_shape=train_data_shape[1:]))\n",
    "    hidden_layers.add(Dense(128, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "    hidden_layers.add(Dense(64, activation='relu'))\n",
    "    hidden_layers.add(Dropout(0.4))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = hidden_layers(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "    \n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with LabelMe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(10000, 4, 4, 512)\n",
      "(10000,)\n",
      "(10000,)\n",
      "\n",
      "Loading AMT data...\n",
      "(10000, 59)\n",
      "N_CLASSES: 8\n",
      "N_ANNOT: 59\n",
      "\n",
      "Loading test data...\n",
      "(1188, 4, 4, 512)\n",
      "(1188,)\n",
      "\n",
      "Loading validation data...\n",
      "(500, 4, 4, 512)\n",
      "(500,)\n",
      "(10000, 8, 20) (1188, 8, 20)\n",
      "\n",
      "Crowd noise adaptation model with 20 annotators\n",
      "Tensor(\"baseline_200/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_201/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_201/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.214194564305572, 'crowds_classification_201_loss': -7.214194564305572, 'baseline_loss': 1.026378773504034, 'crowds_classification_201_acc': 0.0026304713804713806, 'baseline_acc': 0.8223905723905723}\n",
      "Tensor(\"baseline_201/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_202/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_202/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.207406525660043, 'crowds_classification_202_loss': -7.207406525660043, 'baseline_loss': 1.0333523239802431, 'crowds_classification_202_acc': 0.02220117845117845, 'baseline_acc': 0.8324915824915825}\n",
      "Tensor(\"baseline_202/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_203/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_203/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.124283119484231, 'crowds_classification_203_loss': -7.124283119484231, 'baseline_loss': 2.4689068763143687, 'crowds_classification_203_acc': 0.02199074074074074, 'baseline_acc': 0.7575757575757576}\n",
      "Tensor(\"baseline_203/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_204/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_204/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.209602079808913, 'crowds_classification_204_loss': -7.209602079808913, 'baseline_loss': 1.0518854672860618, 'crowds_classification_204_acc': 0.0007365319865319865, 'baseline_acc': 0.8417508417508418}\n",
      "Tensor(\"baseline_204/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_205/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_205/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.174350942425455, 'crowds_classification_205_loss': -7.174350942425455, 'baseline_loss': 1.0667450686396172, 'crowds_classification_205_acc': 0.0361952861952862, 'baseline_acc': 0.8324915824915825}\n",
      "Tensor(\"baseline_205/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_206/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_206/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.2123776349154385, 'crowds_classification_206_loss': -7.2123776349154385, 'baseline_loss': 1.0398368340499875, 'crowds_classification_206_acc': 0.0008417508417508417, 'baseline_acc': 0.8358585858585859}\n",
      "Tensor(\"baseline_206/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_207/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_207/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.112694454514218, 'crowds_classification_207_loss': -7.112694454514218, 'baseline_loss': 2.559076710582181, 'crowds_classification_207_acc': 0.014625420875420875, 'baseline_acc': 0.76010101010101}\n",
      "Tensor(\"baseline_207/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_208/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_208/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.164892344362406, 'crowds_classification_208_loss': -7.164892344362406, 'baseline_loss': 1.0510009292400244, 'crowds_classification_208_acc': 0.01273148148148148, 'baseline_acc': 0.8215488215488216}\n",
      "Tensor(\"baseline_208/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_209/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_209/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.199039439962368, 'crowds_classification_209_loss': -7.199039439962368, 'baseline_loss': 1.0950166068920986, 'crowds_classification_209_acc': 0.03756313131313131, 'baseline_acc': 0.8282828282828283}\n",
      "Tensor(\"baseline_209/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_210/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_210/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.086569609465422, 'crowds_classification_210_loss': -7.086569609465422, 'baseline_loss': 2.708377877419645, 'crowds_classification_210_acc': 0.021464646464646464, 'baseline_acc': 0.7533670033670034}\n",
      "Tensor(\"baseline_210/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_211/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_211/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.168061821549027, 'crowds_classification_211_loss': -7.168061821549027, 'baseline_loss': 1.1978564599547723, 'crowds_classification_211_acc': 0.013783670033670033, 'baseline_acc': 0.819023569023569}\n",
      "Tensor(\"baseline_211/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_212/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_212/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.158568876760977, 'crowds_classification_212_loss': -7.158568876760977, 'baseline_loss': 1.1991333988454407, 'crowds_classification_212_acc': 0.021254208754208755, 'baseline_acc': 0.8223905723905723}\n",
      "Tensor(\"baseline_212/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_213/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_213/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.18090478017274, 'crowds_classification_213_loss': -7.18090478017274, 'baseline_loss': 1.097239104083893, 'crowds_classification_213_acc': 0.0035774410774410776, 'baseline_acc': 0.8291245791245792}\n",
      "Tensor(\"baseline_213/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_214/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_214/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.195329439760458, 'crowds_classification_214_loss': -7.195329439760458, 'baseline_loss': 1.0140044274253877, 'crowds_classification_214_acc': 0.001473063973063973, 'baseline_acc': 0.8063973063973064}\n",
      "Tensor(\"baseline_214/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_215/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_215/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.125177017366044, 'crowds_classification_215_loss': -7.125177017366044, 'baseline_loss': 1.2537562564165905, 'crowds_classification_215_acc': 0.001999158249158249, 'baseline_acc': 0.8114478114478114}\n",
      "Tensor(\"baseline_215/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_216/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_216/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.158770464887523, 'crowds_classification_216_loss': -7.158770464887523, 'baseline_loss': 2.356258232234303, 'crowds_classification_216_acc': 0.019675925925925927, 'baseline_acc': 0.7668350168350169}\n",
      "Tensor(\"baseline_216/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_217/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_217/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.100264013014257, 'crowds_classification_217_loss': -7.100264013014257, 'baseline_loss': 3.131433862252067, 'crowds_classification_217_acc': 0.0003156565656565657, 'baseline_acc': 0.7121212121212122}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"baseline_217/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_218/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_218/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.1871120696918736, 'crowds_classification_218_loss': -7.1871120696918736, 'baseline_loss': 1.0413835338169477, 'crowds_classification_218_acc': 0.008101851851851851, 'baseline_acc': 0.8291245791245792}\n",
      "Tensor(\"baseline_218/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_219/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_219/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.155349028230917, 'crowds_classification_219_loss': -7.155349028230917, 'baseline_loss': 1.2548983656998836, 'crowds_classification_219_acc': 0.00042087542087542086, 'baseline_acc': 0.813973063973064}\n",
      "Tensor(\"baseline_219/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_220/CrowdLayer:0' shape=(8, 8, 20) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_220/Reshape_2:0\", shape=(?, 8, 20), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.184860531328503, 'crowds_classification_220_loss': -7.184860531328503, 'baseline_loss': 0.9719321473630189, 'crowds_classification_220_acc': 0.021254208754208755, 'baseline_acc': 0.8257575757575758}\n",
      "(10000, 8, 21) (1188, 8, 21)\n",
      "\n",
      "Crowd noise adaptation model with 21 annotators\n",
      "Tensor(\"baseline_220/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_221/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_221/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.096523683079164, 'crowds_classification_221_loss': -7.096523683079164, 'baseline_loss': 2.1917623349713153, 'crowds_classification_221_acc': 0.022306397306397305, 'baseline_acc': 0.73989898989899}\n",
      "Tensor(\"baseline_221/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_222/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_222/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.20584695026128, 'crowds_classification_222_loss': -7.20584695026128, 'baseline_loss': 0.941225380122486, 'crowds_classification_222_acc': 0.02156986531986532, 'baseline_acc': 0.8333333333333334}\n",
      "Tensor(\"baseline_222/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_223/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_223/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.133022456056742, 'crowds_classification_223_loss': -7.133022456056742, 'baseline_loss': 1.1553728589756722, 'crowds_classification_223_acc': 0.029776936026936027, 'baseline_acc': 0.8097643097643098}\n",
      "Tensor(\"baseline_223/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_224/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_224/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.143627641979693, 'crowds_classification_224_loss': -7.143627641979693, 'baseline_loss': 1.0173171137017434, 'crowds_classification_224_acc': 0.01273148148148148, 'baseline_acc': 0.7971380471380471}\n",
      "Tensor(\"baseline_224/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_225/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_225/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.136625031429509, 'crowds_classification_225_loss': -7.136625031429509, 'baseline_loss': 1.0384993387593164, 'crowds_classification_225_acc': 0.00968013468013468, 'baseline_acc': 0.797979797979798}\n",
      "Tensor(\"baseline_225/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_226/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_226/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.181818987785365, 'crowds_classification_226_loss': -7.181818987785365, 'baseline_loss': 1.1095951963216066, 'crowds_classification_226_acc': 0.03409090909090909, 'baseline_acc': 0.8291245791245792}\n",
      "Tensor(\"baseline_226/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_227/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_227/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.170665530644683, 'crowds_classification_227_loss': -7.170665530644683, 'baseline_loss': 1.1465902477879115, 'crowds_classification_227_acc': 0.028935185185185185, 'baseline_acc': 0.8232323232323232}\n",
      "Tensor(\"baseline_227/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_228/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_228/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.163180877865364, 'crowds_classification_228_loss': -7.163180877865364, 'baseline_loss': 1.164359876177327, 'crowds_classification_228_acc': 0.03177609427609428, 'baseline_acc': 0.8164983164983165}\n",
      "Tensor(\"baseline_228/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_229/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_229/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.122221720339072, 'crowds_classification_229_loss': -7.122221720339072, 'baseline_loss': 1.2837904631489454, 'crowds_classification_229_acc': 0.0, 'baseline_acc': 0.7920875420875421}\n",
      "Tensor(\"baseline_229/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_230/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_230/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.183736113988189, 'crowds_classification_230_loss': -7.183736113988189, 'baseline_loss': 1.0427987646775614, 'crowds_classification_230_acc': 0.03419612794612795, 'baseline_acc': 0.8223905723905723}\n",
      "Tensor(\"baseline_230/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_231/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_231/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.093144448919328, 'crowds_classification_231_loss': -7.093144448919328, 'baseline_loss': 1.2384523663055214, 'crowds_classification_231_acc': 0.01273148148148148, 'baseline_acc': 0.7626262626262627}\n",
      "Tensor(\"baseline_231/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_232/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_232/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n",
      "(8,)\n",
      "Test dataset results: \n",
      "{'loss': -7.15434491835058, 'crowds_classification_232_loss': -7.15434491835058, 'baseline_loss': 1.104866796069675, 'crowds_classification_232_acc': 0.010101010101010102, 'baseline_acc': 0.8097643097643098}\n",
      "Tensor(\"baseline_232/Softmax:0\", shape=(?, 8), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_233/CrowdLayer:0' shape=(8, 8, 21) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_233/Reshape_2:0\", shape=(?, 8, 21), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 20\n",
    "N_CLASSES = 8\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/LabelMe/prepared/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "for N_ANNOT in range(20, 41):\n",
    "    y_annot_reduced = y_annot.copy()\n",
    "    y_annot_reduced['train'] = y_annot_reduced['train'][:,:,:N_ANNOT]\n",
    "    y_annot_reduced['test'] = y_annot_reduced['test'][:,:,:N_ANNOT]\n",
    "\n",
    "    print(y_annot_reduced['train'].shape, y_annot_reduced['test'].shape)\n",
    "\n",
    "    prefix = 'annot_%d_'%N_ANNOT\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "            \n",
    "    print('\\nCrowd noise adaptation model with %d annotators' % (N_ANNOT))\n",
    "    for i in range(NUM_RUNS):\n",
    "        acc_df = pd.DataFrame()\n",
    "        loss_df = pd.DataFrame()\n",
    "        history, trace_arr = crowd_model(x, y_gt, y_annot_reduced, N_CLASSES, False, True)\n",
    "        acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "        loss_df.loc[:, i] = history.history['baseline_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Bird Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cactus Wren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(3069, 7, 7, 512)\n",
      "(3069,)\n",
      "(3069,)\n",
      "\n",
      "Loading AMT data...\n",
      "(3069, 31)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 31\n",
      "\n",
      "Loading test data...\n",
      "(176, 7, 7, 512)\n",
      "(176,)\n",
      "\n",
      "Loading validation data...\n",
      "(363, 7, 7, 512)\n",
      "(363,)\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 4.580449364402077, 'acc': 0.6420454545454546}\n",
      "Test dataset results: \n",
      "{'loss': 2.014761946418069, 'acc': 0.875}\n",
      "Test dataset results: \n",
      "{'loss': 1.6465330990878018, 'acc': 0.7386363636363636}\n",
      "Test dataset results: \n",
      "{'loss': 1.007381016557867, 'acc': 0.9375}\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 2.49463267107223, 'acc': 0.7840909090909091}\n",
      "Test dataset results: \n",
      "{'loss': 2.747402613813227, 'acc': 0.8295454545454546}\n",
      "Test dataset results: \n",
      "{'loss': 9.89710773121227, 'acc': 0.24431818181818182}\n",
      "Test dataset results: \n",
      "{'loss': 1.8316018256274136, 'acc': 0.8863636363636364}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_54/dropout_58/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_16/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_17/CrowdLayer:0' shape=(2, 2, 31) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.965367848222906, 'crowds_classification_s_model_channel_matrix_17_loss': 1.965367848222906, 'baseline_loss': 7.394810849970037, 'crowds_classification_s_model_channel_matrix_17_acc': 0.08522727272727272, 'baseline_acc': 0.3465909090909091}\n",
      "[-1.2145348 -1.222816  -1.2283239 -1.2283609 -1.2212453 -1.2242062\n",
      " -1.2284982 -1.2317678 -1.218075  -1.2257532 -1.2219071 -1.222049\n",
      " -1.2245255 -1.2205535 -1.2268302 -1.2289101 -1.2258186 -1.2163831\n",
      " -1.2224633 -1.218777  -1.2224998 -1.2273726 -1.2308598 -1.2217655\n",
      " -1.224704  -1.2184628 -1.2277637 -1.2150924 -1.2168875 -1.2214345\n",
      " -1.2224519]\n",
      "[-0.05335602  0.37418503 -0.02978235 -0.45479172  0.19985743  0.287719\n",
      " -1.2153287  -0.22152734 -0.58365655 -0.14410618 -0.4113449  -0.02904806\n",
      " -0.33202973 -0.5428674  -0.3458145  -0.01546493 -0.15257566 -0.37932542\n",
      " -1.2395889  -0.4009359  -1.2886758  -0.03542978  0.21425138 -0.09808487\n",
      " -0.8290505  -0.81355363 -1.6498114  -1.6273695  -1.5924728  -0.00810894\n",
      "  0.2408826 ]\n",
      "Test dataset results: \n",
      "{'loss': 1.866253744472157, 'crowds_classification_s_model_channel_matrix_17_loss': 1.866253744472157, 'baseline_loss': 4.304264166138389, 'crowds_classification_s_model_channel_matrix_17_acc': 0.0, 'baseline_acc': 0.7329545454545454}\n",
      "inputs:  [<tf.Tensor 'sequential_55/dropout_59/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_17/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_18/CrowdLayer:0' shape=(2, 2, 31) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.9649707729166204, 'crowds_classification_s_model_channel_matrix_18_loss': 1.9649707729166204, 'baseline_loss': 7.276861971074885, 'crowds_classification_s_model_channel_matrix_18_acc': 0.0, 'baseline_acc': 0.4375}\n",
      "[-1.2250197 -1.224402  -1.2247782 -1.2259915 -1.2222698 -1.2228286\n",
      " -1.2232888 -1.21521   -1.2240584 -1.2160335 -1.2231953 -1.2213252\n",
      " -1.2241437 -1.2271719 -1.2213378 -1.2242043 -1.2230151 -1.2206354\n",
      " -1.2266321 -1.2206974 -1.2221569 -1.2315139 -1.2174493 -1.214993\n",
      " -1.2183373 -1.216233  -1.219172  -1.2206578 -1.2201128 -1.217938\n",
      " -1.2224355]\n",
      "[-0.07025611  0.53250134  0.04522908 -0.24777141  0.24132088  0.21664801\n",
      " -1.2722995  -0.03572831 -0.48678023  0.00391683 -0.4608474   0.14354768\n",
      " -0.25508755 -0.44029063 -0.19981241  0.08891973 -0.22680983 -0.48627758\n",
      " -1.2317109  -0.37202564 -1.2138941   0.27905273  0.26002     0.18675552\n",
      " -0.71749127 -0.7197671  -1.7170966  -1.7213128  -1.7404995  -0.00685543\n",
      "  0.35208002]\n",
      "Test dataset results: \n",
      "{'loss': 1.8877869085832075, 'crowds_classification_s_model_channel_matrix_18_loss': 1.8877869085832075, 'baseline_loss': 4.3963140791112725, 'crowds_classification_s_model_channel_matrix_18_acc': 0.0, 'baseline_acc': 0.7272727272727273}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Cactus Wren/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(3510, 7, 7, 512)\n",
      "(3510,)\n",
      "(3510,)\n",
      "\n",
      "Loading AMT data...\n",
      "(3510, 40)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 40\n",
      "\n",
      "Loading test data...\n",
      "(210, 7, 7, 512)\n",
      "(210,)\n",
      "\n",
      "Loading validation data...\n",
      "(405, 7, 7, 512)\n",
      "(405,)\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 3.363896878560384, 'acc': 0.6476190487543741}\n",
      "Test dataset results: \n",
      "{'loss': 3.453891637211754, 'acc': 0.7857142857142857}\n",
      "Test dataset results: \n",
      "{'loss': 10.050633058093844, 'acc': 0.2857142858562015}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 7.196937047867548, 'acc': 0.4190476207506089}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "Test dataset results: \n",
      "{'loss': 5.360020560310001, 'acc': 0.5095238098076411}\n",
      "Test dataset results: \n",
      "{'loss': 3.453877686318897, 'acc': 0.7857142857142857}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_36/dropout_36/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_10/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_11/CrowdLayer:0' shape=(2, 2, 40) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.954318726630438, 'crowds_classification_s_model_channel_matrix_11_loss': 1.954318726630438, 'baseline_loss': 4.963432365372068, 'crowds_classification_s_model_channel_matrix_11_acc': 0.06904761933145069, 'baseline_acc': 0.5190476173446292}\n",
      "[-1.214625  -1.2225301 -1.2315772 -1.2202654 -1.2156637 -1.219775\n",
      " -1.2205465 -1.2202355 -1.2246528 -1.2240222 -1.2230691 -1.2218487\n",
      " -1.2168307 -1.2156943 -1.2256112 -1.2219603 -1.2200146 -1.2230723\n",
      " -1.2316959 -1.2192756 -1.225567  -1.2213427 -1.2213001 -1.224585\n",
      " -1.2218451 -1.2185788 -1.2295411 -1.2284455 -1.2232418 -1.2289993\n",
      " -1.2284415 -1.2203555 -1.2188916 -1.2198966 -1.2263379 -1.2189612\n",
      " -1.2228476 -1.2209213 -1.2189511 -1.2249522]\n",
      "[-0.11793998 -0.05818248 -0.13086095 -0.2770066  -0.2615344  -0.05057818\n",
      " -0.8260736  -1.7610115  -1.282572   -0.16903532 -0.08748335 -2.1466975\n",
      " -0.10428065 -0.06714797 -0.06941676 -0.5833692  -0.5632809  -0.57106054\n",
      " -1.4741534  -0.3905257  -0.05126268 -0.14119849 -0.19573602 -0.57210696\n",
      " -1.90217    -0.5743197  -2.4272501  -2.3583794  -0.08687335 -0.576204\n",
      " -1.9232346  -0.54916877 -0.58648646 -0.71561116 -2.2863088  -1.7608438\n",
      " -0.43456045 -1.8832152  -1.8755567  -1.8991568 ]\n",
      "Test dataset results: \n",
      "{'loss': 1.9380079757599604, 'crowds_classification_s_model_channel_matrix_11_loss': 1.9380079757599604, 'baseline_loss': 3.453877686318897, 'crowds_classification_s_model_channel_matrix_11_acc': 0.0, 'baseline_acc': 0.7857142857142857}\n",
      "inputs:  [<tf.Tensor 'sequential_37/dropout_37/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_11/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_12/CrowdLayer:0' shape=(2, 2, 40) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.968616415205456, 'crowds_classification_s_model_channel_matrix_12_loss': 1.968616415205456, 'baseline_loss': 8.890265473865327, 'crowds_classification_s_model_channel_matrix_12_acc': 0.0, 'baseline_acc': 0.3666666667376246}\n",
      "[-1.2223322 -1.2280217 -1.2230384 -1.2199016 -1.2236457 -1.2289549\n",
      " -1.2240506 -1.229531  -1.2239687 -1.2258353 -1.2227023 -1.2204988\n",
      " -1.2272933 -1.2167273 -1.2229995 -1.223974  -1.2276158 -1.2272159\n",
      " -1.2142353 -1.2160599 -1.2276628 -1.2206247 -1.2184012 -1.2223468\n",
      " -1.2158633 -1.2275271 -1.2171713 -1.2228966 -1.2204975 -1.2193968\n",
      " -1.2190998 -1.2243402 -1.2179408 -1.2270055 -1.2227778 -1.2170029\n",
      " -1.2180742 -1.2272317 -1.2259493 -1.2253311]\n",
      "[-0.2146227   0.2649197  -0.24356124 -0.3544624  -0.35027838 -0.17893767\n",
      " -0.24130052 -1.7681172  -1.2195606   0.16856796 -0.14874226 -2.0809147\n",
      " -0.19239861 -0.11651659 -0.12091118 -0.5953253  -0.60212994 -0.5649527\n",
      " -1.4135033  -0.34934908 -0.45663053 -0.1899243  -0.6809007  -0.5833593\n",
      " -1.8529342  -0.6219734  -2.3342924  -2.2669568  -0.20060378 -0.5620404\n",
      " -1.8955861  -0.5580819  -0.58448946 -0.20409276 -1.7529298  -1.0968785\n",
      "  0.13220333 -1.8241385  -1.8805497  -1.901188  ]\n",
      "Test dataset results: \n",
      "{'loss': 1.932643416949681, 'crowds_classification_s_model_channel_matrix_12_loss': 1.932643416949681, 'baseline_loss': 3.453877686318897, 'crowds_classification_s_model_channel_matrix_12_acc': 0.0023809523986918586, 'baseline_acc': 0.7857142857142857}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Cape May Warbler/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(4605, 7, 7, 512)\n",
      "(4605,)\n",
      "(4605,)\n",
      "\n",
      "Loading AMT data...\n",
      "(4605, 26)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 26\n",
      "\n",
      "Loading test data...\n",
      "(270, 7, 7, 512)\n",
      "(270,)\n",
      "\n",
      "Loading validation data...\n",
      "(540, 7, 7, 512)\n",
      "(540,)\n",
      "\n",
      "baseline model with majority vote labels:\n",
      "Test dataset results: \n",
      "{'loss': 5.301747682359483, 'acc': 0.5111111114422481}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 8.372794212456103, 'acc': 0.3370370357124894}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "\n",
      "baseline model with clean labels:\n",
      "Test dataset results: \n",
      "{'loss': 6.542580353772199, 'acc': 0.4629629620799312}\n",
      "Test dataset results: \n",
      "{'loss': 2.686349210915742, 'acc': 0.8333333333333334}\n",
      "Test dataset results: \n",
      "{'loss': 6.45135940975613, 'acc': 0.4555555558866925}\n",
      "Test dataset results: \n",
      "{'loss': 2.746045853473522, 'acc': 0.8296296296296296}\n",
      "\n",
      "crowd noise adaptation model:\n",
      "inputs:  [<tf.Tensor 'sequential_42/dropout_42/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_12/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_13/CrowdLayer:0' shape=(2, 2, 26) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.98194877659833, 'crowds_classification_s_model_channel_matrix_13_loss': 1.98194877659833, 'baseline_loss': 12.234007828323929, 'crowds_classification_s_model_channel_matrix_13_acc': 0.007407407407407408, 'baseline_acc': 0.18518518518518517}\n",
      "[-1.2201753 -1.2268286 -1.214236  -1.2193261 -1.2176983 -1.2275063\n",
      " -1.225966  -1.2285258 -1.2298713 -1.2230749 -1.2142867 -1.2132062\n",
      " -1.2232786 -1.2168393 -1.2270377 -1.2207963 -1.2258315 -1.2266085\n",
      " -1.2213564 -1.2252138 -1.2309152 -1.2253373 -1.2243654 -1.2218494\n",
      " -1.2136307 -1.221777 ]\n",
      "[-0.52776045 -1.3540683  -0.4177078  -0.11545622 -0.54134107  0.33389422\n",
      " -0.03999372 -0.07815816  0.47176585 -1.0085231  -0.28538436 -1.8284719\n",
      "  0.31436607  0.3176101  -0.447317   -0.01539969 -1.7737968  -1.0395633\n",
      " -0.04230124 -0.09205562 -1.0095183  -1.3070513  -0.13641775 -0.01144341\n",
      " -0.15482259 -0.04833603]\n",
      "Test dataset results: \n",
      "{'loss': 1.8779728456779763, 'crowds_classification_s_model_channel_matrix_13_loss': 1.8779728456779763, 'baseline_loss': 2.686349210915742, 'crowds_classification_s_model_channel_matrix_13_acc': 0.003703703703703704, 'baseline_acc': 0.8333333333333334}\n",
      "inputs:  [<tf.Tensor 'sequential_43/dropout_43/cond/Merge:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'baseline_13/Softmax:0' shape=(?, 2) dtype=float32>]\n",
      "weights:  [<tf.Variable 'crowds_classification_s_model_channel_matrix_14/CrowdLayer:0' shape=(2, 2, 26) dtype=float32_ref>]\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': 1.964492815512198, 'crowds_classification_s_model_channel_matrix_14_loss': 1.964492815512198, 'baseline_loss': 6.003401629130045, 'crowds_classification_s_model_channel_matrix_14_acc': 0.10370370381408267, 'baseline_acc': 0.4481481482585271}\n",
      "[-1.2240474 -1.220388  -1.2233236 -1.2212448 -1.2210826 -1.2254007\n",
      " -1.2240303 -1.2255814 -1.2255785 -1.2240349 -1.2194164 -1.2251365\n",
      " -1.2174225 -1.2228318 -1.2236248 -1.2192252 -1.216672  -1.2189465\n",
      " -1.2299571 -1.2166123 -1.2205992 -1.2148399 -1.2222056 -1.2236384\n",
      " -1.2202098 -1.2222602]\n",
      "[-1.1409558  -1.8622302   0.11297405 -0.03880447 -0.24117842 -0.33372113\n",
      " -0.81494397 -0.6601999  -0.18892744 -1.7279867  -0.76113975 -2.3207715\n",
      "  0.10346997 -0.0210706   0.13587284 -0.40078482 -2.2298121  -1.5567228\n",
      "  0.06068504  0.13652474 -1.0685005  -1.4518511   0.16919917 -0.19692948\n",
      "  0.17287344  0.08832288]\n",
      "Test dataset results: \n",
      "{'loss': 1.8893143830475985, 'crowds_classification_s_model_channel_matrix_14_loss': 1.8893143830475985, 'baseline_loss': 2.686349210915742, 'crowds_classification_s_model_channel_matrix_14_acc': 0.001851851851851852, 'baseline_acc': 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 2\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = '/home/yajingyang/PycharmProjects/online_crowdsourcing/data/classification/cub_40/images/Evening Grosbeak/'\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "    \n",
    "print('\\nbaseline model with majority vote labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    majority_vote(x, y_gt, y_annot, N_CLASSES)\n",
    "    \n",
    "print('\\nbaseline model with clean labels:')\n",
    "for i in range(NUM_RUNS):\n",
    "    baseline_gt(x, y_gt, N_CLASSES)\n",
    "\n",
    "    \n",
    "print('\\ncrowd noise adaptation model:')\n",
    "for i in range(NUM_RUNS):\n",
    "    crowd_model(x, y_gt, y_annot, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
