{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yajingyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, merge, Reshape, Permute, Multiply, Dot,dot, Concatenate, Add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy, CrowdsClassificationSModel, \\\n",
    "    CrowdsClassificationCModelSingleWeight, CrowdsClassificationCModel, MaskedMultiCrossEntropyCosSim, \\\n",
    "    MaskedMultiCrossEntropyBaseChannel, MaskedMultiCrossEntropyBaseChannelConst, CrowdsClassificationSModelChannelMatrix, \\\n",
    "    MaskedMultiCrossEntropyCurriculumChannelMatrix\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = np.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets\n",
    "\n",
    "def get_data(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "    return x, y_gt, y_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def eval(model,x_test, y_test):\n",
    "    print('Test dataset results: ')\n",
    "    print(dict(zip(model.metrics_names,model.evaluate(x_test,y_test, verbose=False))))\n",
    "\n",
    "\n",
    "def get_trace(model):\n",
    "\n",
    "    channel_matrix = model.get_weights()[-1]\n",
    "    channel_matrix_trace = tf.trace(K.permute_dimensions(channel_matrix, [2, 0, 1]))\n",
    "    channel_matrix_trace_arr = K.eval(channel_matrix_trace)\n",
    "    return channel_matrix_trace_arr\n",
    "\n",
    "\n",
    "def print_single_loss(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['baseline_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['baseline_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_history(df, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Make a data frame\n",
    "    df['x'] = range(df.shape[0])\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    # multiple line plot\n",
    "    num = 0\n",
    "    for column in df.drop('x', axis=1):\n",
    "        num += 1\n",
    "        plt.plot(df['x'], df[column], marker='', color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=2)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(train_data_shape, N_CLASSES):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation('softmax'))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(x, y_gt, y_annot, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_annot['mv'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_gt(x, y_gt, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_gt['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "    \n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "    \n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample, y_gt_sample, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    baseline_model = Model(inputs=train_inputs, outputs=baseline_output)\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    baseline_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    \n",
    "    history = baseline_model.fit(x_sample['train'], y_gt_sample['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "    \n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    model.set_weights(baseline_model.get_weights()) \n",
    "\n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(DATA_PATH, N_CLASSES):\n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train = load_data(DATA_PATH+\"data_train.npy\")\n",
    "    print(data_train.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test = load_data(DATA_PATH+\"data_test.npy\")\n",
    "    print(data_test.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid = load_data(DATA_PATH+\"data_valid.npy\")\n",
    "    print(data_valid.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "    \n",
    "    x = {'train': data_train, 'test': data_test, 'val': data_valid}\n",
    "    y = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(12499, 150, 150, 3)\n",
      "(12499,)\n",
      "(12499,)\n",
      "\n",
      "Loading AMT data...\n",
      "(12499, 200)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 200\n",
      "\n",
      "Loading test data...\n",
      "(6250, 150, 150, 3)\n",
      "(6250,)\n",
      "\n",
      "Loading validation data...\n",
      "(6249, 150, 150, 3)\n",
      "(6249,)\n",
      "(125, 150, 150, 3) (125, 2) (12499, 2, 200)\n",
      "\n",
      "Crowd noise adaptation model pretrain with 0.01 clean data\n",
      "Tensor(\"baseline_1/Softmax:0\", shape=(?, 2), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_2/CrowdLayer:0' shape=(2, 2, 200) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_2/Reshape_2:0\", shape=(?, 2, 200), dtype=float32)\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': -1.3057886334609985, 'crowds_classification_2_loss': -1.3057886334609985, 'baseline_loss': 0.6937303191757203, 'crowds_classification_2_acc': 0.0, 'baseline_acc': 0.4969600000190735}\n",
      "Tensor(\"baseline_2/Softmax:0\", shape=(?, 2), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_3/CrowdLayer:0' shape=(2, 2, 200) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_3/Reshape_2:0\", shape=(?, 2, 200), dtype=float32)\n",
      "(2,)\n",
      "Test dataset results: \n",
      "{'loss': -1.3057318170928955, 'crowds_classification_3_loss': -1.3057318170928955, 'baseline_loss': 8.108046794281005, 'crowds_classification_3_acc': 0.0, 'baseline_acc': 0.4969600000190735}\n",
      "Tensor(\"baseline_3/Softmax:0\", shape=(?, 2), dtype=float32)\n",
      "<tf.Variable 'crowds_classification_4/CrowdLayer:0' shape=(2, 2, 200) dtype=float32_ref>\n",
      "Tensor(\"crowds_classification_4/Reshape_2:0\", shape=(?, 2, 200), dtype=float32)\n",
      "(2,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aa86f684a38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0macc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mloss_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrowd_model_pretrain_with_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_annot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0macc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'baseline_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'baseline_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-855a39131701>\u001b[0m in \u001b[0;36mcrowd_model_pretrain_with_clean_data\u001b[0;34m(x, y_gt, y_annot, x_sample, y_gt_sample, N_CLASSES, softmax, trace)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n\u001b[0;32m---> 65\u001b[0;31m                               batch_size=BATCH_SIZE, verbose=0)\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtrace_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_annot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 20\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/PetImages/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    x_sample = x.copy()\n",
    "    x_sample['train'] = x_sample['train'][:round(x_sample['train'].shape[0]*clean_percent)]\n",
    "    y_gt_sample = y_gt.copy()\n",
    "    y_gt_sample['train'] = y_gt_sample['train'][:round(y_gt_sample['train'].shape[0]*clean_percent)]\n",
    "    y_annot_mix = y_annot.copy()\n",
    "    y_annot_mix['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)] \\\n",
    "        = np.reshape(np.repeat(y_gt['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)], N_ANNOT, axis=1)\n",
    "                     , (-1, N_CLASSES, N_ANNOT))\n",
    "    \n",
    "    print(x_sample['train'].shape, y_gt_sample['train'].shape, y_annot_mix['train'].shape)\n",
    "\n",
    "    prefix = 'pct_%.2f_'%clean_percent\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "\n",
    "#     print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "#     for i in range(NUM_RUNS):\n",
    "#         clean_base_acc_df = pd.DataFrame()\n",
    "#         clean_base_loss_df = pd.DataFrame()\n",
    "#         clean_history = baseline_gt(x_sample, y_gt_sample, N_CLASSES)\n",
    "#         clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "#         clean_base_loss_df.loc[:, i] = clean_history.history['loss']\n",
    "            \n",
    "#     print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "#     for i in range(NUM_RUNS):\n",
    "#         acc_df = pd.DataFrame()\n",
    "#         loss_df = pd.DataFrame()\n",
    "#         history, trace_arr = crowd_model(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "#         acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "#         loss_df.loc[:, i] = history.history['baseline_loss']\n",
    "        \n",
    "    print('\\nCrowd noise adaptation model pretrain with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        acc_df = pd.DataFrame()\n",
    "        loss_df = pd.DataFrame()\n",
    "        history, trace_arr = crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample, y_gt_sample, N_CLASSES, False, True)\n",
    "        acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "        loss_df.loc[:, i] = history.history['baseline_loss']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
