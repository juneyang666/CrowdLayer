{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yajingyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, merge, Reshape, Permute, Multiply, Dot,dot, Concatenate, Add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import keras as keras\n",
    "\n",
    "# packages for learning from crowds\n",
    "from crowd_layer.crowd_layers import CrowdsClassification, MaskedMultiCrossEntropy, CrowdsClassificationSModel, \\\n",
    "    CrowdsClassificationCModelSingleWeight, CrowdsClassificationCModel, MaskedMultiCrossEntropyCosSim, \\\n",
    "    MaskedMultiCrossEntropyBaseChannel, MaskedMultiCrossEntropyBaseChannelConst, CrowdsClassificationSModelChannelMatrix, \\\n",
    "    MaskedMultiCrossEntropyCurriculumChannelMatrix\n",
    "from crowd_layer.crowd_aggregators import CrowdsCategoricalAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent tensorflow from allocating the entire GPU memory at once\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    data = np.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def one_hot(target, n_classes):\n",
    "    targets = np.array([target]).reshape(-1)\n",
    "    one_hot_targets = np.eye(n_classes)[targets]\n",
    "    return one_hot_targets\n",
    "\n",
    "def get_data(DATA_PATH, N_CLASSES):\n",
    "    \n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train_vgg16 = load_data(DATA_PATH+\"data_train.npy\")\n",
    "    print(data_train_vgg16.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # labels obtained from majority voting\n",
    "    labels_train_mv = load_data(DATA_PATH+\"labels_train_mv.npy\")\n",
    "    print(labels_train_mv.shape)\n",
    "\n",
    "#     # labels obtained by using the approach by Dawid and Skene\n",
    "#     labels_train_ds = load_data(DATA_PATH+\"labels_train_DS.npy\")\n",
    "#     print(labels_train_ds.shape)\n",
    "\n",
    "    # data from Amazon Mechanical Turk\n",
    "    print(\"\\nLoading AMT data...\")\n",
    "    answers = load_data(DATA_PATH+\"answers.npy\")\n",
    "    print(answers.shape)\n",
    "    N_ANNOT = answers.shape[1]\n",
    "    print(\"N_CLASSES:\", N_CLASSES)\n",
    "    print(\"N_ANNOT:\", N_ANNOT)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test_vgg16 = load_data(DATA_PATH+\"data_test.npy\")\n",
    "    print(data_test_vgg16.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid_vgg16 = load_data(DATA_PATH+\"data_valid.npy\")\n",
    "    print(data_valid_vgg16.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "    labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "\n",
    "\n",
    "    answers_bin_missings = []\n",
    "    for i in range(len(answers)):\n",
    "        row = []\n",
    "        for r in range(N_ANNOT):\n",
    "            if answers[i,r] == -1:\n",
    "                row.append(-1 * np.ones(N_CLASSES))\n",
    "            else:\n",
    "                row.append(one_hot(answers[i,r], N_CLASSES)[0,:])\n",
    "        answers_bin_missings.append(row)\n",
    "    answers_bin_missings = np.array(answers_bin_missings).swapaxes(1,2)\n",
    "\n",
    "    answers_test_bin_missings = np.zeros((len(labels_test), N_CLASSES))\n",
    "    answers_test_bin_missings[np.arange(len(labels_test)), labels_test] = 1\n",
    "    answers_test_bin_missings = np.repeat(answers_test_bin_missings.reshape([len(labels_test),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "\n",
    "    answers_valid_bin_missings = np.zeros((len(labels_valid), N_CLASSES))\n",
    "    answers_valid_bin_missings[np.arange(len(labels_valid)), labels_valid] = 1\n",
    "    answers_valid_bin_missings = np.repeat(answers_valid_bin_missings.reshape([len(labels_valid),N_CLASSES,1]), N_ANNOT, axis=2)\n",
    "    \n",
    "    x = {'train': data_train_vgg16, 'test': data_test_vgg16, 'val': data_valid_vgg16}\n",
    "    y_gt = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    y_annot = {'train': answers_bin_missings, 'test': answers_test_bin_missings, 'val': answers_valid_bin_missings, 'mv':labels_train_mv_bin}\n",
    "    return x, y_gt, y_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def eval(model,x_test, y_test):\n",
    "    print('Test dataset results: ')\n",
    "    print(dict(zip(model.metrics_names,model.evaluate(x_test,y_test, verbose=False))))\n",
    "\n",
    "\n",
    "def get_trace(model):\n",
    "\n",
    "    channel_matrix = model.get_weights()[-1]\n",
    "    channel_matrix_trace = tf.trace(K.permute_dimensions(channel_matrix, [2, 0, 1]))\n",
    "    channel_matrix_trace_arr = K.eval(channel_matrix_trace)\n",
    "    return channel_matrix_trace_arr\n",
    "\n",
    "\n",
    "def print_single_loss(model):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # list all data in history\n",
    "    print(model.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['baseline_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['baseline_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_history(df, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Make a data frame\n",
    "    df['x'] = range(df.shape[0])\n",
    "\n",
    "    # style\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "\n",
    "    # multiple line plot\n",
    "    num = 0\n",
    "    for column in df.drop('x', axis=1):\n",
    "        num += 1\n",
    "        plt.plot(df['x'], df[column], marker='', color=palette(num), linewidth=1, alpha=0.9, label=column)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=2, ncol=2)\n",
    "\n",
    "    # Add titles\n",
    "    plt.title(title, loc='left', fontsize=12, fontweight=0, color='orange')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(train_data_shape, N_CLASSES):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "    base_model.add(Dense(N_CLASSES))\n",
    "    base_model.add(Activation('softmax'))\n",
    "    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(x, y_gt, y_annot, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_annot['mv'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_gt(x, y_gt, N_CLASSES):\n",
    "    train_data_shape = x['train'].shape\n",
    "    baseline_model = build_base_model(train_data_shape, N_CLASSES)\n",
    "\n",
    "#     eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    history = baseline_model.fit(x['train'], y_gt['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE, verbose=0)\n",
    "    eval(baseline_model, x['test'], y_test=y_gt['test'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace):\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "    \n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "\n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model(x, y_gt, y_annot, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    model = build_base_crowd_model(train_data_shape, N_CLASSES, N_ANNOT, softmax, trace)    \n",
    "    \n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowd_model_pretrain_with_clean_data(x, y_gt, y_annot, x_sample, y_gt_sample, y_annot_sample, N_CLASSES, softmax, trace):\n",
    "    train_data_shape = x['train'].shape\n",
    "    N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "    base_model = Sequential()\n",
    "    base_model.add(Conv2D(32, (3, 3), input_shape=train_data_shape[1:]))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    base_model.add(BatchNormalization(scale=False))\n",
    "\n",
    "    base_model.add(Conv2D(32, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Conv2D(64, (3, 3)))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    base_model.add(Flatten()) \n",
    "    base_model.add(Dense(128))\n",
    "    base_model.add(Activation('relu'))\n",
    "    base_model.add(Dropout(0.5))\n",
    "\n",
    "    train_inputs = Input(shape=(train_data_shape[1:]))\n",
    "    last_hidden = base_model(train_inputs)\n",
    "    baseline_output = Dense(N_CLASSES, activation='softmax', name='baseline')(last_hidden)\n",
    "\n",
    "    if softmax:\n",
    "        channel_layer = CrowdsClassificationSModelChannelMatrix(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer([last_hidden, baseline_output])\n",
    "    else:\n",
    "        channel_layer = CrowdsClassification(N_CLASSES, N_ANNOT)\n",
    "        channeled_output = channel_layer(baseline_output)\n",
    "\n",
    "    baseline_model = Model(inputs=train_inputs, outputs=baseline_output)\n",
    "\n",
    "    # compile model with masked loss and train\n",
    "    baseline_model.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    \n",
    "    history = baseline_model.fit(x_sample['train'], y_gt_sample['train'], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    \n",
    "    model = Model(inputs=train_inputs, outputs=[channeled_output, baseline_output])\n",
    "    \n",
    "    if trace:\n",
    "        loss = MaskedMultiCrossEntropyCurriculumChannelMatrix(model, 1, 1).loss\n",
    "    else:\n",
    "        loss = MaskedMultiCrossEntropy().loss\n",
    "    # compile model with masked loss and train\n",
    "    model.compile(optimizer='adam',\n",
    "                         loss=[loss, 'categorical_crossentropy'],\n",
    "                         loss_weights=[1, 0],\n",
    "                         metrics=['accuracy']\n",
    "                        )\n",
    "    model.set_weights(baseline_model.get_weights()) \n",
    "\n",
    "    history = model.fit(x['train'], [y_annot['train'], y_gt['train']], epochs=N_EPOCHS, shuffle=True,\n",
    "                              batch_size=BATCH_SIZE, verbose=0)\n",
    "    trace_arr = get_trace(model)\n",
    "    eval(model, x['test'], y_test=[y_annot['test'], y_gt['test']])\n",
    "    return history, trace_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(DATA_PATH, N_CLASSES):\n",
    "    print(\"\\nLoading train data...\")\n",
    "    # images processed by VGG16\n",
    "    data_train = load_data(DATA_PATH+\"data_train.npy\")\n",
    "    print(data_train.shape)\n",
    "\n",
    "    # ground truth labels\n",
    "    labels_train = load_data(DATA_PATH+\"labels_train.npy\")\n",
    "    print(labels_train.shape)\n",
    "\n",
    "    # load test data\n",
    "    print(\"\\nLoading test data...\")\n",
    "\n",
    "    # images processed by VGG16\n",
    "    data_test = load_data(DATA_PATH+\"data_test.npy\")\n",
    "    print(data_test.shape)\n",
    "\n",
    "    # test labels\n",
    "    labels_test = load_data(DATA_PATH+\"labels_test.npy\")\n",
    "    print(labels_test.shape)\n",
    "\n",
    "    print(\"\\nLoading validation data...\")\n",
    "    # images processed by VGG16\n",
    "    data_valid = load_data(DATA_PATH+\"data_valid.npy\")\n",
    "    print(data_valid.shape)\n",
    "\n",
    "    # validation labels\n",
    "    labels_valid = load_data(DATA_PATH+\"labels_valid.npy\")\n",
    "    print(labels_valid.shape)\n",
    "\n",
    "    labels_train_bin = one_hot(labels_train, N_CLASSES)\n",
    "#     labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)\n",
    "#     print(labels_train_ds_bin.shape)\n",
    "    labels_test_bin = one_hot(labels_test, N_CLASSES)\n",
    "    labels_valid_bin = one_hot(labels_valid, N_CLASSES)\n",
    "    \n",
    "    x = {'train': data_train, 'test': data_test, 'val': data_valid}\n",
    "    y = {'train': labels_train_bin, 'test': labels_test_bin, 'val': labels_valid_bin}\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train data...\n",
      "(12499, 150, 150, 3)\n",
      "(12499,)\n",
      "(12499,)\n",
      "\n",
      "Loading AMT data...\n",
      "(12499, 200)\n",
      "N_CLASSES: 2\n",
      "N_ANNOT: 200\n",
      "\n",
      "Loading test data...\n",
      "(6250, 150, 150, 3)\n",
      "(6250,)\n",
      "\n",
      "Loading validation data...\n",
      "(6249, 150, 150, 3)\n",
      "(6249,)\n",
      "(125, 150, 150, 3) (125, 2) (12499, 2, 200)\n",
      "\n",
      "Baseline model with 0.01 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.3410710009002687, 'acc': 0.61968}\n",
      "Test dataset results: \n",
      "{'loss': 1.5938798546409607, 'acc': 0.6060799999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.1084623233795166, 'acc': 0.5969599999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.4393676411628724, 'acc': 0.6188799999618531}\n",
      "Test dataset results: \n",
      "{'loss': 1.4205179509353638, 'acc': 0.5745600000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.2270889855957032, 'acc': 0.6103999999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.2108013739013672, 'acc': 0.6145600000381469}\n",
      "Test dataset results: \n",
      "{'loss': 1.7682827521514892, 'acc': 0.611520000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.250554113998413, 'acc': 0.60832}\n",
      "Test dataset results: \n",
      "{'loss': 1.8160422358036041, 'acc': 0.6070400000190734}\n",
      "Test dataset results: \n",
      "{'loss': 1.36280977104187, 'acc': 0.6108799999809266}\n",
      "Test dataset results: \n",
      "{'loss': 1.253510855693817, 'acc': 0.6139199999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.2316024293518066, 'acc': 0.599040000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.6657795809555054, 'acc': 0.6136}\n",
      "Test dataset results: \n",
      "{'loss': 2.028026698627472, 'acc': 0.6072000000190735}\n",
      "Test dataset results: \n",
      "{'loss': 2.153720587100983, 'acc': 0.5875200000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.6279027055358886, 'acc': 0.613600000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.7248836449432372, 'acc': 0.6120000000381469}\n",
      "Test dataset results: \n",
      "{'loss': 1.7130345892524719, 'acc': 0.6083199999809266}\n",
      "Test dataset results: \n",
      "{'loss': 1.5250250605010987, 'acc': 0.5787199999809265}\n",
      "(250, 150, 150, 3) (250, 2) (12499, 2, 200)\n",
      "\n",
      "Baseline model with 0.02 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.4469661504745484, 'acc': 0.6232000000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.3059047704696656, 'acc': 0.658240000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.2906387223052977, 'acc': 0.6196799999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.5960563594055175, 'acc': 0.6196800000095367}\n",
      "Test dataset results: \n",
      "{'loss': 1.3513555128479005, 'acc': 0.65952}\n",
      "Test dataset results: \n",
      "{'loss': 1.4821500659942628, 'acc': 0.639680000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.3209144033050537, 'acc': 0.632960000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.7196744945907594, 'acc': 0.637280000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.3420042987060548, 'acc': 0.611040000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.31615061794281, 'acc': 0.6228800000095367}\n",
      "Test dataset results: \n",
      "{'loss': 1.6320151793670654, 'acc': 0.610080000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.453881142501831, 'acc': 0.6326400000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.4121073668670654, 'acc': 0.629760000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.2912600144195556, 'acc': 0.6419200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.3322108578491212, 'acc': 0.6403200000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.1947642938995362, 'acc': 0.6499200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.265434702835083, 'acc': 0.649920000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.4037772695922852, 'acc': 0.6395200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.205001490020752, 'acc': 0.6510399999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.5747872216033936, 'acc': 0.661120000038147}\n",
      "(375, 150, 150, 3) (375, 2) (12499, 2, 200)\n",
      "\n",
      "Baseline model with 0.03 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.5061620682525634, 'acc': 0.648800000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.4318189844512939, 'acc': 0.63312}\n",
      "Test dataset results: \n",
      "{'loss': 1.3485328060150146, 'acc': 0.65792}\n",
      "Test dataset results: \n",
      "{'loss': 1.4313236771774291, 'acc': 0.628}\n",
      "Test dataset results: \n",
      "{'loss': 1.2069840098190308, 'acc': 0.6544}\n",
      "Test dataset results: \n",
      "{'loss': 1.3843632773208618, 'acc': 0.6467199999809266}\n",
      "Test dataset results: \n",
      "{'loss': 1.6024537477874756, 'acc': 0.643520000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.6949684574699402, 'acc': 0.6353599999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.4927745421600342, 'acc': 0.64048}\n",
      "Test dataset results: \n",
      "{'loss': 1.6073942821502685, 'acc': 0.65744}\n",
      "Test dataset results: \n",
      "{'loss': 1.6100992914581298, 'acc': 0.6275200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.3292982321262359, 'acc': 0.6564799999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.6073309111022949, 'acc': 0.6334400000095367}\n",
      "Test dataset results: \n",
      "{'loss': 1.6432034112167357, 'acc': 0.624960000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.5161330821228027, 'acc': 0.635040000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.5602166074371338, 'acc': 0.6579200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.376194555644989, 'acc': 0.6387199999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.6156809882926941, 'acc': 0.6552000000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.668487348022461, 'acc': 0.6379199999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.5538120135498046, 'acc': 0.6620800000381469}\n",
      "(500, 150, 150, 3) (500, 2) (12499, 2, 200)\n",
      "\n",
      "Baseline model with 0.04 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.5071517372131347, 'acc': 0.660160000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.716358761329651, 'acc': 0.6495999999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.8429420385742188, 'acc': 0.6590400000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.7518170680236815, 'acc': 0.6558400000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.8768748734283447, 'acc': 0.66352}\n",
      "Test dataset results: \n",
      "{'loss': 1.620998890838623, 'acc': 0.632960000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.629899397277832, 'acc': 0.6526400000381469}\n",
      "Test dataset results: \n",
      "{'loss': 1.7735463665008544, 'acc': 0.65872}\n",
      "Test dataset results: \n",
      "{'loss': 1.7527994966125489, 'acc': 0.6649600000381469}\n",
      "Test dataset results: \n",
      "{'loss': 1.395749435119629, 'acc': 0.675360000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.7021562297821045, 'acc': 0.647680000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.6330558256149292, 'acc': 0.6433599999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.7814408659362793, 'acc': 0.6459199999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.295996968383789, 'acc': 0.675680000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.5716195527648926, 'acc': 0.665920000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.7944903959655762, 'acc': 0.648480000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.5087722913360595, 'acc': 0.6534400000095367}\n",
      "Test dataset results: \n",
      "{'loss': 1.5826074645233155, 'acc': 0.6561599999809266}\n",
      "Test dataset results: \n",
      "{'loss': 1.9679947847747803, 'acc': 0.64064}\n",
      "Test dataset results: \n",
      "{'loss': 1.5352012476348877, 'acc': 0.657280000038147}\n",
      "(625, 150, 150, 3) (625, 2) (12499, 2, 200)\n",
      "\n",
      "Baseline model with 0.05 clean data\n",
      "Test dataset results: \n",
      "{'loss': 1.5502810202026367, 'acc': 0.677920000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.987810878829956, 'acc': 0.6331200000095367}\n",
      "Test dataset results: \n",
      "{'loss': 1.5105704705810548, 'acc': 0.668160000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.687434689025879, 'acc': 0.6553600000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.7420584827423096, 'acc': 0.6659200000095368}\n",
      "Test dataset results: \n",
      "{'loss': 1.798925523376465, 'acc': 0.67552}\n",
      "Test dataset results: \n",
      "{'loss': 1.8981754682159424, 'acc': 0.65168}\n",
      "Test dataset results: \n",
      "{'loss': 1.8747838593292236, 'acc': 0.6755200000190735}\n",
      "Test dataset results: \n",
      "{'loss': 1.5579796784210205, 'acc': 0.6521599999809266}\n",
      "Test dataset results: \n",
      "{'loss': 1.8462947859954835, 'acc': 0.6696}\n",
      "Test dataset results: \n",
      "{'loss': 1.8983351422119141, 'acc': 0.68304}\n",
      "Test dataset results: \n",
      "{'loss': 1.8583275686264038, 'acc': 0.6556799999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.5287436935424805, 'acc': 0.66608}\n",
      "Test dataset results: \n",
      "{'loss': 1.541058547592163, 'acc': 0.669920000038147}\n",
      "Test dataset results: \n",
      "{'loss': 1.6674765990447997, 'acc': 0.6675200000047684}\n",
      "Test dataset results: \n",
      "{'loss': 1.54010301612854, 'acc': 0.64912}\n",
      "Test dataset results: \n",
      "{'loss': 1.8966501993560791, 'acc': 0.6686399999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.5595306451034545, 'acc': 0.6905599999809265}\n",
      "Test dataset results: \n",
      "{'loss': 1.545940119857788, 'acc': 0.6577600000381469}\n",
      "Test dataset results: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7245697959899902, 'acc': 0.67232}\n"
     ]
    }
   ],
   "source": [
    "NUM_RUNS = 20\n",
    "N_CLASSES = 2\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 35\n",
    "W = 0\n",
    "\n",
    "DATA_PATH = \"/home/yajingyang/Downloads/PetImages/\"\n",
    "x, y_gt, y_annot = get_data(DATA_PATH, N_CLASSES)\n",
    "N_ANNOT = y_annot['train'].shape[2]\n",
    "\n",
    "for clean_percent in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "    x_sample = x.copy()\n",
    "    x_sample['train'] = x_sample['train'][:round(x_sample['train'].shape[0]*clean_percent)]\n",
    "    y_gt_sample = y_gt.copy()\n",
    "    y_gt_sample['train'] = y_gt_sample['train'][:round(y_gt_sample['train'].shape[0]*clean_percent)]\n",
    "    y_annot_mix = y_annot.copy()\n",
    "    y_annot_mix['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)] \\\n",
    "        = np.reshape(np.repeat(y_gt['train'][:round(y_annot_mix['train'].shape[0]*clean_percent)], N_ANNOT, axis=1)\n",
    "                     , (-1, N_CLASSES, N_ANNOT))\n",
    "    \n",
    "    print(x_sample['train'].shape, y_gt_sample['train'].shape, y_annot_mix['train'].shape)\n",
    "\n",
    "    prefix = 'pct_%.2f_'%clean_percent\n",
    "    loss_csv = prefix + 'loss.csv'\n",
    "    acc_csv = prefix + 'acc.csv'\n",
    "    trace_csv = prefix + 'trace.csv'\n",
    "    acc_df = pd.DataFrame()\n",
    "    loss_df = pd.DataFrame()\n",
    "    trace_df = pd.DataFrame()\n",
    "\n",
    "    print('\\nBaseline model with %.2f clean data' % (clean_percent))\n",
    "    for i in range(NUM_RUNS):\n",
    "        clean_base_acc_df = pd.DataFrame()\n",
    "        clean_base_loss_df = pd.DataFrame()\n",
    "        clean_history = baseline_gt(x_sample, y_gt_sample, N_CLASSES)\n",
    "        clean_base_acc_df.loc[:, i] = clean_history.history['acc']\n",
    "        clean_base_loss_df.loc[:, i] = clean_history.history['loss']\n",
    "            \n",
    "#     print('\\nCrowd noise adaptation model with %.2f clean data' % (clean_percent))\n",
    "#     for i in range(NUM_RUNS):\n",
    "#         acc_df = pd.DataFrame()\n",
    "#         loss_df = pd.DataFrame()\n",
    "#         history, trace_arr = crowd_model(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "#         acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "#         loss_df.loc[:, i] = history.history['baseline_loss']\n",
    "        \n",
    "#     print('\\nCrowd noise adaptation model pretrain with %.2f clean data' % (clean_percent))\n",
    "#     for i in range(NUM_RUNS):\n",
    "#         acc_df = pd.DataFrame()\n",
    "#         loss_df = pd.DataFrame()\n",
    "#         history, trace_arr = crowd_model_pretrain_with_clean_data(x, y_gt, y_annot_mix, N_CLASSES, False, True)\n",
    "#         acc_df.loc[:, i] = history.history['baseline_acc']\n",
    "#         loss_df.loc[:, i] = history.history['baseline_loss']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
